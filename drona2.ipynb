{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "drona2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabirdeb/datascience/blob/main/drona2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project: Automated Question-Answering**"
      ],
      "metadata": {
        "id": "Bp63SZWFbQec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# machine learning\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Text processing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import string\n",
        "\n",
        "# Data reading\n",
        "import pkgutil   # provides binary data\n",
        "from io import StringIO # for binary to high level data conversion"
      ],
      "metadata": {
        "id": "WpBTrM4bPF07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fed2bd4-e29a-496b-c2eb-9bf5c5ae0a9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # required during test time\n",
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "xE1fr5biDRus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# ques_ans_pipeline = pipeline(\"question-answering\")"
      ],
      "metadata": {
        "id": "s1jJUWLEZndu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # reading the data during test time\n",
        "# data_science_df_clean=pd.read_csv('data_science_df_clean.csv')"
      ],
      "metadata": {
        "id": "3yJQN25ubtVC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "bytes_data = pkgutil.get_data(__name__, \"data_science_df_clean.csv\")\n",
        "\n",
        "s=str(bytes_data,'utf-8')\n",
        "data = StringIO(s) \n",
        "data_science_df_clean=pd.read_csv(data)"
      ],
      "metadata": {
        "id": "p5Zpp8v8c0FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing abbreviation processing function\n",
        "def abbreviation_process(text):\n",
        "    abbreviation_dict= {'ml':'machine learning','cnn':'convolutional neural network',\n",
        "                        'rnn':'recurrent neural network','Sequence Models':'recurrent neural network',\n",
        "                        'pca':'principal component analysis','svm':'support vector machine',\n",
        "                        'knn':'k-nearest neighbors','ann':'artificial neural network',\n",
        "                        'nn':'neural network','sgd':'stochastic gradient descent',\n",
        "                        'gd':'gradient descent','nlp':'natural language processing',\n",
        "                        'nlu':'natural language understanding','api': 'application programming interface',\n",
        "                        'gui':'graphical user interface','mlops': 'ml lifecycle',\n",
        "                        'lda':'latent dirichlet allocation','svd':'singular value decomposition',\n",
        "                        'cf':'collaborative filtering','cpu':'central processing unit',\n",
        "                        'anova':'analysis of variance','auc':'area under the curve',\n",
        "                        'cv':'cross validation','dnn':'deep neural network',\n",
        "                        'eda':'exploratory data analysis','gbm':'gradient boosting machine',\n",
        "                        'glm':'generalized linear model','gru':'gated recurrent unit',\n",
        "                        'hmm':'hidden marcov model','ica':'independent component analysis',\n",
        "                        'lstm':'long short term memory','mape':'mean absolute percentage error',\n",
        "                        'mse':'mean squared error','rmse':'root mean squared error',\n",
        "                        'nldr':'non-linear dimensionality reduction','r2':'r-squared',\n",
        "                        'rf':'random forest','roc':'receiver operating characteristic',\n",
        "                        'ai':'artificial intelligence', 'shap': 'shapley additive explanations',\n",
        "                        'lime': 'local interpretable model-agnostic explanations', 'eli5': 'explain like I am 5',\n",
        "                        'xai': 'explainable artificial intelligence', 'opp': 'object oriented programming',\n",
        "                        'idle': 'integrated development and learning environment', 'sql': 'structured query language',\n",
        "                        'rdbms' : 'relational database management system', 'iqr': 'interquartile range',\n",
        "                        'iid': 'independent and indentically distributed', 'clt': 'central limit theorem',\n",
        "                        'ols': 'ordinary least squares', 'vif': 'variance inflation factor',\n",
        "                        'xgboost': 'extreme gradient boosting', 'gmlos': 'geometric mean length of stay',\n",
        "                        'los': 'length of stay', 'smote': 'synthetic minority over-sampling technique',\n",
        "                        'snn': 'standard neural network', 'brnn': 'idirectional recurrent neural network',\n",
        "                        'nlg': 'natural language generation', 'bfs': 'breadth first search',\n",
        "                        'dfs': 'depth first search', 'os': 'operating system',\n",
        "                        'cvcs' : 'central version control system','dvcs': 'distributed version control system',\n",
        "                        'wsgi': 'web server gateway interface', 'asgi': 'asynchronous server gateway interface',\n",
        "                        'mle': 'machine learning engineering', 'gpu': 'graphics processing unit',\n",
        "                        'dag': 'directed acyclic graph', 'rdd': 'resilient distributed dataset'}\n",
        "                        \n",
        "    text = text.lower()    # converting to lowercase\n",
        "    text= text.replace('?','') # removing '?' mark\n",
        "    text = [re.sub('\\s+', ' ', sent) for sent in text] # Removing new line characters\n",
        "    text = [re.sub(\"\\'\", \"\", sent) for sent in text] # Removing distracting single quotes\n",
        "    text=''.join(text)\n",
        "\n",
        "    for k in text.split():   # loop for replacing the abbreviations\n",
        "      for i in abbreviation_dict.keys():\n",
        "        if k==i:\n",
        "          text=text.replace(k,abbreviation_dict.get(i))\n",
        "  \n",
        "    return text"
      ],
      "metadata": {
        "id": "szkiE7YNny86"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing text pre-processing function\n",
        "def text_process(text):\n",
        "    text = text.lower()    # converting to lowercase\n",
        "    text =[char for char in text if char not in string.punctuation] # removing punctuations\n",
        "    text=''.join(text) \n",
        "    text=[word for word in text.split() if word not in stopwords.words('english')]  # removing stopwords\n",
        "    stemmer = SnowballStemmer(\"english\") \n",
        "    text=' '.join(text) \n",
        "    text = [stemmer.stem(word) for word in text.split()] # stemming operation\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "EHKkL-PQqCBD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building of Question-Answering Model**"
      ],
      "metadata": {
        "id": "fjSZLaPE7UGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countvec ngram and Question-Answering Model"
      ],
      "metadata": {
        "id": "8-onFY9P7d8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a function for question answering\n",
        "def tellme(question):\n",
        "  '''\n",
        "  This model gives answer to data science related questions.\n",
        "  '''\n",
        "  global data_science_df_clean\n",
        "  \n",
        "  # Processing the question\n",
        "  question_abbre_processed =abbreviation_process(question)\n",
        "  question_processed = text_process(question_abbre_processed)\n",
        "    \n",
        "  # Appending question in the dataset to match the dimension of question and document vector\n",
        "  data_science_df_clean=data_science_df_clean[(data_science_df_clean.documents!=data_science_df_clean.documents_processed)]\n",
        "  data_science_df_clean.loc[(data_science_df_clean.index.max()+1)] = question_processed\n",
        "\n",
        "  # vectorization of text samples\n",
        "  ngram_model = CountVectorizer(ngram_range=(1,3)) # Downloading pre-trained vectorization model (ngram)\n",
        "  document_term_matrix = ngram_model.fit_transform(data_science_df_clean.documents_processed.values)\n",
        "\n",
        "  # CountVec ngram question-answering model\n",
        "  topic_match=[]\n",
        "  # short_answer_dict={}\n",
        "  # long_answer_dict={}\n",
        "  \n",
        "  vec_1=document_term_matrix[-1:]     # Question vector\n",
        "\n",
        "  for k in range(len(data_science_df_clean.documents[:-1])):\n",
        "    vec_2=document_term_matrix[k:(k+1)]     # Individual document vector\n",
        "    topic_match.append(cosine_similarity(vec_1 , vec_2)[0][0])\n",
        "    \n",
        "  try:\n",
        "    \n",
        "    if max(topic_match)<0.25:  # Deciding the margins through hit and trial for perfect answer\n",
        "      answer=print(\"Sorry ! I have no experience for this question.\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "              \n",
        "    else:\n",
        "      answer=print(f\"{data_science_df_clean.documents[topic_match.index(max(topic_match))]}\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "\n",
        "      # for i in topic_match:\n",
        "      #   if i>0.7*max(topic_match): # Deciding the margins through hit and trial for perfect answer\n",
        "      #     context=data_science_df_clean.first_100_letters[topic_match.index(i)]\n",
        "      #     ans = ques_ans_pipeline(question=question_abbre_processed, context=context)\n",
        "      #     short_answer_dict[ans['score']] =ans['answer']\n",
        "      #     long_answer_dict[ans['score']]=data_science_df_clean.documents[topic_match.index(i)]\n",
        "                  \n",
        "      # if max(list(short_answer_dict.keys()))>0.01: # Deciding the margins through hit and trial for perfect answer\n",
        "      #   answer=print(f\"{long_answer_dict.get(max(list(long_answer_dict.keys())))}\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "                        \n",
        "      # else:\n",
        "      #   answer=print(\"Sorry ! I have no experience for this question.\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "        \n",
        "  except:\n",
        "    answer=print(\"There is an exception\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "    \n",
        "  return answer"
      ],
      "metadata": {
        "id": "r2S54dXV7MD1"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}