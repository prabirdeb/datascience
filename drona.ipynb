{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "drona.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabirdeb/datascience/blob/main/drona.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project: Automated Question-Answering**"
      ],
      "metadata": {
        "id": "Bp63SZWFbQec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# machine learning\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Text processing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import string\n",
        "\n",
        "# Data reading\n",
        "import pkgutil   # provides binary data\n",
        "from io import StringIO # for binary to high level data conversion\n",
        "\n",
        "# For showing image along with answer\n",
        "import urllib.request\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "WpBTrM4bPF07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136eccb6-74a4-4eac-8251-3ba87d438af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # reading the data during test time\n",
        "# data_science_df_clean=pd.read_csv('data_science_df_clean.csv')\n",
        "# data_science_df_pictures=pd.read_csv('data_science_df_pictures.csv')"
      ],
      "metadata": {
        "id": "3yJQN25ubtVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "bytes_data = pkgutil.get_data(__name__, \"data_science_df_clean.csv\")\n",
        "\n",
        "s=str(bytes_data,'utf-8')\n",
        "data = StringIO(s) \n",
        "data_science_df_clean=pd.read_csv(data)"
      ],
      "metadata": {
        "id": "p5Zpp8v8c0FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data for url\n",
        "bytes_data = pkgutil.get_data(__name__, \"data_science_df_pictures.csv\")\n",
        "\n",
        "s=str(bytes_data,'utf-8')\n",
        "data = StringIO(s) \n",
        "data_science_df_pictures=pd.read_csv(data)"
      ],
      "metadata": {
        "id": "txCBKwcbAdTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing abbreviation processing function\n",
        "def abbreviation_process(text):\n",
        "    try:\n",
        "      abbreviation_dict= {'ml':'machine learning','cnn':'convolutional neural network',\n",
        "                          'rnn':'recurrent neural network','Sequence Models':'recurrent neural network',\n",
        "                          'pca':'principal component analysis','svm':'support vector machine',\n",
        "                          'knn':'k-nearest neighbors','ann':'artificial neural network',\n",
        "                          'nn':'neural network','sgd':'stochastic gradient descent',\n",
        "                          'gd':'gradient descent','nlp':'natural language processing',\n",
        "                          'nlu':'natural language understanding','api': 'application programming interface',\n",
        "                          'gui':'graphical user interface','mlops': 'ml lifecycle',\n",
        "                          'lda':'latent dirichlet allocation','svd':'singular value decomposition',\n",
        "                          'cf':'collaborative filtering','cpu':'central processing unit',\n",
        "                          'anova':'analysis of variance','auc':'area under the curve',\n",
        "                          'cv':'cross validation','dnn':'deep neural network',\n",
        "                          'eda':'exploratory data analysis','gbm':'gradient boosting machine',\n",
        "                          'glm':'generalized linear model','gru':'gated recurrent unit',\n",
        "                          'hmm':'hidden marcov model','ica':'independent component analysis',\n",
        "                          'lstm':'long short term memory','mape':'mean absolute percentage error',\n",
        "                          'mse':'mean squared error','rmse':'root mean squared error',\n",
        "                          'nldr':'non-linear dimensionality reduction','r2':'r-squared',\n",
        "                          'rf':'random forest','roc':'receiver operating characteristic',\n",
        "                          'ai':'artificial intelligence', 'shap': 'shapley additive explanations',\n",
        "                          'lime': 'local interpretable model-agnostic explanations', 'eli5': 'explain like I am 5',\n",
        "                          'xai': 'explainable artificial intelligence', 'opp': 'object oriented programming',\n",
        "                          'idle': 'integrated development and learning environment', 'sql': 'structured query language',\n",
        "                          'rdbms' : 'relational database management system', 'iqr': 'interquartile range',\n",
        "                          'iid': 'independent and indentically distributed', 'clt': 'central limit theorem',\n",
        "                          'ols': 'ordinary least squares', 'vif': 'variance inflation factor',\n",
        "                          'xgboost': 'extreme gradient boosting', 'gmlos': 'geometric mean length of stay',\n",
        "                          'los': 'length of stay', 'smote': 'synthetic minority over-sampling technique',\n",
        "                          'snn': 'standard neural network', 'brnn': 'idirectional recurrent neural network',\n",
        "                          'nlg': 'natural language generation', 'bfs': 'breadth first search',\n",
        "                          'dfs': 'depth first search', 'os': 'operating system',\n",
        "                          'cvcs' : 'central version control system','dvcs': 'distributed version control system',\n",
        "                          'wsgi': 'web server gateway interface', 'asgi': 'asynchronous server gateway interface',\n",
        "                          'mle': 'machine learning engineering', 'gpu': 'graphics processing unit',\n",
        "                          'dag': 'directed acyclic graph', 'rdd': 'resilient distributed dataset'}\n",
        "                          \n",
        "      text = text.lower()    # converting to lowercase\n",
        "      text= text.replace('?','') # removing '?' mark\n",
        "      text = [re.sub('\\s+', ' ', sent) for sent in text] # Removing new line characters\n",
        "      text = [re.sub(\"\\'\", \"\", sent) for sent in text] # Removing distracting single quotes\n",
        "      text=''.join(text)\n",
        "\n",
        "      for k in text.split():   # loop for replacing the abbreviations\n",
        "        for i in abbreviation_dict.keys():\n",
        "          if k==i:\n",
        "            text=text.replace(k,abbreviation_dict.get(i))\n",
        "    except:\n",
        "      print(\"Sorry..\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "szkiE7YNny86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing text pre-processing function\n",
        "def text_process(text):\n",
        "    try:\n",
        "      text = text.lower()    # converting to lowercase\n",
        "      text =[char for char in text if char not in string.punctuation] # removing punctuations\n",
        "      text=''.join(text) \n",
        "      text=[word for word in text.split() if word not in stopwords.words('english')]  # removing stopwords\n",
        "      stemmer = SnowballStemmer(\"english\") \n",
        "      text=' '.join(text) \n",
        "      text = [stemmer.stem(word) for word in text.split()] # stemming operation\n",
        "      text = ' '.join(text)\n",
        "\n",
        "    except:\n",
        "      print(\"Please check your statement..\")\n",
        "                      \n",
        "    return text"
      ],
      "metadata": {
        "id": "EHKkL-PQqCBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing function for showing picture\n",
        "def show_image(input):\n",
        "  url=data_science_df_pictures.url[input]\n",
        "  file_name = \".*\"\n",
        "  \n",
        "  try:\n",
        "    if type(url) == str:\n",
        "      urllib.request.urlretrieve(url, file_name)\n",
        "      img = Image.open(file_name)\n",
        "    else:\n",
        "      img = print(\"\")\n",
        "  \n",
        "  except:\n",
        "    img = print(\"\")\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "RgT6wFHw6Vbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building of Question-Answering Model**"
      ],
      "metadata": {
        "id": "fjSZLaPE7UGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execution time has been reduced tremendously from 2000 ms to 50 ms using cosine similarity of whole matrix, in place of for loop or list comprehension in earlier version"
      ],
      "metadata": {
        "id": "Z-j8cs7JDGpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countvec ngram and Question-Answering Model"
      ],
      "metadata": {
        "id": "8-onFY9P7d8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a function for question answering\n",
        "def tellme(question):\n",
        "  '''\n",
        "  This model gives answer to data science related questions.\n",
        "  '''\n",
        "  try:\n",
        "    global data_science_df_clean\n",
        "    \n",
        "    # Appending question in the dataset to match the dimension of question and document vector\n",
        "    data_science_df_clean=data_science_df_clean[(data_science_df_clean.documents!=data_science_df_clean.documents_processed)]\n",
        "    data_science_df_clean.loc[(data_science_df_clean.index.max()+1)] = text_process(abbreviation_process(question))\n",
        "\n",
        "    # vectorization of text samples\n",
        "    document_term_matrix = CountVectorizer(ngram_range=(1,3)).fit_transform(data_science_df_clean.documents_processed.values)\n",
        "\n",
        "    # CountVec ngram question-answering model\n",
        "    # Execution time has been reduced tremendously from 2000 ms to 50 ms using cosine similarity of whole matrix, in place of for loop or list comprehension in earlier version\n",
        "    topic_match=cosine_similarity(document_term_matrix[-1:] , document_term_matrix[:-1])[0] \n",
        "    \n",
        "     \n",
        "    if topic_match.max()<0.25:  # Deciding the margins through hit and trial for perfect answer\n",
        "      answer=print(\"Sorry ! I have no experience for this question.\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "              \n",
        "    else:\n",
        "      answer=print(f\"{data_science_df_clean.documents[np.where(topic_match == topic_match.max())[0][0]]}\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "              \n",
        "  except:\n",
        "    answer=print(\"I can't understand\\n\\n::BEGINNERS MAY TYPE 'HELP LINES'\")\n",
        "    \n",
        "  return show_image(np.where(topic_match == topic_match.max())[0][0])"
      ],
      "metadata": {
        "id": "-INKI2QhpIXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}