,Unnamed: 0,documents,documents_processed,document_tokens,first_100_letters
0,0,"python tokens, Keywords, Identifiers, Literals, Punctuations, Operators 

Like any other computer programming language, python tokens (smallest meaningful part of any statement(expression or command) are:

1. Keywords (basic words of any language)
2. Identifiers (programmer defined words of any program)
3. Literals (data or data structure)
4. Punctuations (, : etc)
5. Operators (+_*/)
",python token keyword identifi liter punctuat oper ,"['python', 'token', 'keyword', 'identifi', 'liter', 'punctuat', 'oper']","python tokens, keywords, identifiers, literals, punctuations, operators   like any other computer pr"
1,1,"Python keywords

There are 32 keywords in C++ and 35 keywords in python. Python keywords:

(False, await, else, import, pass,
None, break, except, in, raise,
True, class, finally, is, return,
and, continue, for, lambda, try,
as, def, from, nonlocal, while,
assert, del, global, not, with,
async, elif, if, or, yield)
",python keyword 32 keyword c 35 keyword python pyth,"['python', 'keyword', '32', 'keyword', 'c', '35', 'keyword', 'python', 'pyth']","python keywords  there are 32 keywords in c++ and 35 keywords in python. python keywords:  (false, a"
2,2,"python data types (Numeric, Boolean, String, Datetime)

Integers or whole numbers (1,2,-5,1000), Floats or real numbers (1.2,-0.5,2e2 or 2E2) and Complex numbers

Boolean Variable (true or false)

Comparison Operators (returns a boolean value) ==, !=, >, <, >=, <=
> 2e2 (called Scientific notation) means 2*10^2",python data type numer boolean string datetim inte,"['python', 'data', 'type', 'numer', 'boolean', 'string', 'datetim', 'inte']","python data types (numeric, boolean, string, datetime)  integers or whole numbers (1,2,-5,1000), flo"
3,3,"Arithmatic Operation

(addition[+], subtraction[-], multiplication[*], division[/], floor division[//], exponentiation[**], remainder modulus[%])

12%4 is 0, 13%4 is 1%2 is 1, 4%5 is 4, 13%0O7 is 6

a += 2 means first add 2 with ""a"" and then save it again in ""a""

same instruction is a = a+2

a /=2 means first divide ""a"" by 2 and then save it again in ""a""

same instruction is a = a/2",arithmat oper addit subtract multipl divis floor d,"['arithmat', 'oper', 'addit', 'subtract', 'multipl', 'divis', 'floor', 'd']","arithmatic operation  (addition[+], subtraction[-], multiplication[*], division[/], floor division[/"
4,4," floor function

In mathematics and computer science, the floor function is the function that takes as input a real number x, and gives as output the greatest integer less than or equal to x, denoted floor or ⌊x⌋.",floor function mathemat comput scienc floor functi,"['floor', 'function', 'mathemat', 'comput', 'scienc', 'floor', 'functi']"," floor function  in mathematics and computer science, the floor function is the function that takes "
5,5,"ceiling function

The ceiling function maps x to the least integer greater than or equal to x, denoted ceil or ⌈x⌉",ceil function ceil function map x least integ grea,"['ceil', 'function', 'ceil', 'function', 'map', 'x', 'least', 'integ', 'grea']","ceiling function  the ceiling function maps x to the least integer greater than or equal to x, denot"
6,6,"Variable

Variable is the storing place in computer memory for a data (Integers, Floats, Booleans, string, datetime) or data structure (list, tuple, set, dictionary,np.array, pd.DataFrame etc.) and the data/data structure can be changed or varied

Variable name (must start with letter or underscore)

Variable name for data=x,y,z etc.

Variable name for data structure=age_of_students, _students_age etc.

case_sensitive (best naming practice), CASE_SENSITIVE, and Case_Sensitive are each a different variable",variabl variabl store place comput memori data int,"['variabl', 'variabl', 'store', 'place', 'comput', 'memori', 'data', 'int']","variable  variable is the storing place in computer memory for a data (integers, floats, booleans, s"
7,7,"commenting multiple lines in colab

Ctrl + /",comment multipl line colab ctrl,"['comment', 'multipl', 'line', 'colab', 'ctrl']",commenting multiple lines in colab  ctrl + /
8,8,"Perfect number

Perfect number is a positive integer that is equal to the sum of its proper divisors. The smallest perfect number is 6, which is the sum of 1, 2, and 3. ",perfect number perfect number posit integ equal su,"['perfect', 'number', 'perfect', 'number', 'posit', 'integ', 'equal', 'su']",perfect number  perfect number is a positive integer that is equal to the sum of its proper divisors
9,9,"To show a float upto two decimal places

print(f'Bananas in the bag is {percentage_bananas:.2f} %')

round(any_float,2)

For Numpy

np.around(my_array,2)",show float upto two decim place printfbanana bag p,"['show', 'float', 'upto', 'two', 'decim', 'place', 'printfbanana', 'bag', 'p']",to show a float upto two decimal places  print(fbananas in the bag is {percentage_bananas:.2f} %)  r
10,10,"To get the absolute value

abs(5.8-7.8) returns 2

",get absolut valu abs5878 return 2,"['get', 'absolut', 'valu', 'abs5878', 'return', '2']",to get the absolute value  abs(5.8-7.8) returns 2  
11,11,"The del keyword in python is primarily used to delete objects in Python

del variable_1, variable_2",del keyword python primarili use delet object pyth,"['del', 'keyword', 'python', 'primarili', 'use', 'delet', 'object', 'pyth']","the del keyword in python is primarily used to delete objects in python  del variable_1, variable_2"
12,12,"order of precedence in python

i) Parentheses
ii) Exponential
iii) Multiplication
iv) Division
v) Addition
vi) Subtraction",order preced python parenthes ii exponenti iii mul,"['order', 'preced', 'python', 'parenthes', 'ii', 'exponenti', 'iii', 'mul']",order of precedence in python  i) parentheses ii) exponential iii) multiplication iv) division v) ad
13,13,int(True) gives result 1 and int(False) gives 0,inttru give result 1 intfals give 0,"['inttru', 'give', 'result', '1', 'intfals', 'give', '0']",int(true) gives result 1 and int(false) gives 0
14,14,"An application or app or software or model is nothing but a soft machine (a machinery products).

Every machine has minimum one function which takes some inputs and gives some outputs.

Products are mainly of two types: machinery products and non-machinery products",applic app softwar model noth soft machin machiner,"['applic', 'app', 'softwar', 'model', 'noth', 'soft', 'machin', 'machiner']",an application or app or software or model is nothing but a soft machine (a machinery products).  ev
15,15,"Four basic answers of any question

yes, no, may be and don't know",four basic answer question yes may dont know,"['four', 'basic', 'answer', 'question', 'yes', 'may', 'dont', 'know']","four basic answers of any question  yes, no, may be and dont know"
16,16,"
logical resoning

Statement: Is buying things on installments profitable to the customer?

Arguments:

I. Yes. He has to pay less.
II. No, paying instalments upsets the family budget.

1. Only I is strong
2. Only II is strong
3. Bother I & II are strong
4. Either I or II is strong (when both are strong but opposite)
5. Neither I nor II is strong",logic reson statement buy thing instal profit cust,"['logic', 'reson', 'statement', 'buy', 'thing', 'instal', 'profit', 'cust']", logical resoning  statement: is buying things on installments profitable to the customer  arguments
17,17,The pprint module provides a capability to “pretty-print” arbitrary Python data structures in a well-formatted and more readable way! ,pprint modul provid capabl “prettyprint” arbitrari,"['pprint', 'modul', 'provid', 'capabl', '“prettyprint”', 'arbitrari']",the pprint module provides a capability to “pretty-print” arbitrary python data structures in a well
18,18,"String is a word, a phrase, a sentence, a paragraph or an entire encyclopedia

> sequence, indexing

> string indexing begins from 0

> 'str' object does not support item assignment means string is immutable

'string' or ""string"" 

double quotes are used when there is any 's in the string",string word phrase sentenc paragraph entir encyclo,"['string', 'word', 'phrase', 'sentenc', 'paragraph', 'entir', 'encyclo']","string is a word, a phrase, a sentence, a paragraph or an entire encyclopedia  > sequence, indexing "
19,19,"to print a new line

print('Use \n to print a new line')",print new line printus n print new line,"['print', 'new', 'line', 'printus', 'n', 'print', 'new', 'line']",to print a new line  print(use \n to print a new line)
20,20,"for color print

from termcolor import colored

print(colored('Hello', 'green', attrs=['bold']))",color print termcolor import color printcoloredhel,"['color', 'print', 'termcolor', 'import', 'color', 'printcoloredhel']","for color print  from termcolor import colored  print(colored(hello, green, attrs=[bold]))"
21,21,"Grabbing the element by index 

print(string_name[3]), 

print(string_name[-2]) 

means (length-2)th index",grab element index printstringname3 printstringnam,"['grab', 'element', 'index', 'printstringname3', 'printstringnam']","grabbing the element by index   print(string_name[3]),   print(string_name[-2])   means (length-2)th"
22,22,"String Slicing, 

[starting index:ending index],  element located at the right index is not included

string[0:13] or string[:13] are same

string[2:18] or string[2:] are same when 18 is the last index

string[:]  If we do not specify the starting and the ending index, it will extract all elements of the string

Reversing a string in python

string_name[::-1]

slicing with step size, string[3:14:2] means forward slicing with step size 2, string[14:3:-2] means backward slicing with step size -2",string slice start indexend index element locat ri,"['string', 'slice', 'start', 'indexend', 'index', 'element', 'locat', 'ri']","string slicing,   [starting index:ending index],  element located at the right index is not included"
23,23,"concatenate strings

print(string1 + string2 )

print(""D"", end = ' ')
print(""C"", end = ' ')

will return D C
",concaten string printstring1 string2 printd end pr,"['concaten', 'string', 'printstring1', 'string2', 'printd', 'end', 'pr']","concatenate strings  print(string1 + string2 )  print(""d"", end =  ) print(""c"", end =  )  will return"
24,24,"String functions

print(), type(), len()

ord('z') -ord('a') returns 25

ord() method converts a character into its Unicode code.",string function print type len ordz orda return 25,"['string', 'function', 'print', 'type', 'len', 'ordz', 'orda', 'return', '25']","string functions  print(), type(), len()  ord(z) -ord(a) returns 25  ord() method converts a charact"
25,25,"String methods 

string.lower(), string.upper(), string.count('n'),
string.index('n'), string.find('n'), string.replace('n','L'), string.split(' '), string.split()-default spliting by space,
string.join([""g"", ""h"", ""o""])",string method stringlow stringupp stringcountn str,"['string', 'method', 'stringlow', 'stringupp', 'stringcountn', 'str']","string methods   string.lower(), string.upper(), string.count(n), string.index(n), string.find(n), s"
26,26," way to embed expressions inside string literals

first_name = 'Rahul'
last_name = 'Modi'

full_name = f'Left plus right makes {first_name}  {last_name}'  

f-strings provide a way to embed expressions inside string literals",way emb express insid string liter firstnam rahul ,"['way', 'emb', 'express', 'insid', 'string', 'liter', 'firstnam', 'rahul']", way to embed expressions inside string literals  first_name = rahul last_name = modi  full_name = f
27,27,"my_string = 'Albert Einstein'   

'Albert' in my_string 

returns True",mystr albert einstein albert mystr return true,"['mystr', 'albert', 'einstein', 'albert', 'mystr', 'return', 'true']",my_string = albert einstein     albert in my_string   returns true
28,28,"String duplication occurs when we multiply string by a number, 

my_name='prabir'
print(my_name*3)",string duplic occur multipli string number mynamep,"['string', 'duplic', 'occur', 'multipli', 'string', 'number', 'mynamep']","string duplication occurs when we multiply string by a number,   my_name=prabir print(my_name*3)"
29,29,"Taking user's input

inputted_number = int(input())
 
or

marks = int(input(""Enter our marks: ""))",take user input inputtednumb intinput mark intinpu,"['take', 'user', 'input', 'inputtednumb', 'intinput', 'mark', 'intinpu']","taking users input  inputted_number = int(input())   or  marks = int(input(""enter our marks: ""))"
30,30,"

A string can be called a safe bridge if it has no gaps in it i.e, no spaces.

",string call safe bridg gap ie space,"['string', 'call', 'safe', 'bridg', 'gap', 'ie', 'space']","  a string can be called a safe bridge if it has no gaps in it i.e, no spaces.  "
31,31,"String matching

import re
 pattern = re.compile(""%s"" % common_term)

      topics=[x for x in list_of_topics if pattern.match(x)]",string match import pattern recompil commonterm to,"['string', 'match', 'import', 'pattern', 'recompil', 'commonterm', 'to']","string matching  import re  pattern = re.compile(""%s"" % common_term)        topics=[x for x in list_"
32,32,"List Basics

List is a data structure to store multiple items in a single variable, sequence, indexing, mutable

['A string',23,100.232,'o',True]

> Grabbing the element by index is same as string

> List Slicing is same as string slicing

> List concatenation is same as string concatenation 

> Duplication method similar to strings

my_string=[0]*26

> List mutability

reassigning a list, my_string= my_string +new_string

> Lists are like arrays but lists are more flexible means they have no size constraint or type constraint",list basic list data structur store multipl item s,"['list', 'basic', 'list', 'data', 'structur', 'store', 'multipl', 'item', 's']","list basics  list is a data structure to store multiple items in a single variable, sequence, indexi"
33,33,"List functions

len(), min(), max(), sum(), sorted(), sorted(my_list,reverse=True) for sorting in reverse order, zip() function  returns a list of n-paired tuples",list function len min max sum sort sortedmylistrev,"['list', 'function', 'len', 'min', 'max', 'sum', 'sort', 'sortedmylistrev']","list functions  len(), min(), max(), sum(), sorted(), sorted(my_list,reverse=true) for sorting in re"
34,34,"List methods

 .append(one_element), .extend(list_of_elements), .pop()  by default removes last index, .remove() means remove by element, .count(), .index(),  .sort(), .reverse()",list method appendoneel extendlistofel pop default,"['list', 'method', 'appendoneel', 'extendlistofel', 'pop', 'default']","list methods   .append(one_element), .extend(list_of_elements), .pop()  by default removes last inde"
35,35,"Nested List

[lst_1,lst_2,lst_3]

Entire table of data can be stored in nested list variable",nest list lst1lst2lst3 entir tabl data store nest ,"['nest', 'list', 'lst1lst2lst3', 'entir', 'tabl', 'data', 'store', 'nest']","nested list  [lst_1,lst_2,lst_3]  entire table of data can be stored in nested list variable"
36,36,"Range function to get a sequence list of numbers. 

for k in range(10):

list(range(10)) or list(range(0,10)) or list(range(0,10,1))

range(0) does not give any output

floats are not allowed in range function",rang function get sequenc list number k range10 li,"['rang', 'function', 'get', 'sequenc', 'list', 'number', 'k', 'range10', 'li']",range function to get a sequence list of numbers.   for k in range(10):  list(range(10)) or list(ran
37,37,"Main Subjects of Data Science Stream

> Coding

> Data Structures

> Mathematics

> Artificial Intelligence

> Business Intelligence

> Computer Engineering",main subject data scienc stream code data structur,"['main', 'subject', 'data', 'scienc', 'stream', 'code', 'data', 'structur']",main subjects of data science stream  > coding  > data structures  > mathematics  > artificial intel
38,38,"Main Modules of Coding

> Python",main modul code python,"['main', 'modul', 'code', 'python']",main modules of coding  > python
39,39,"Main Modules of Data Structures

> Analytics Framework",main modul data structur analyt framework,"['main', 'modul', 'data', 'structur', 'analyt', 'framework']",main modules of data structures  > analytics framework
40,40,"Main Modules of Mathematics

> Mathematics",main modul mathemat mathemat,"['main', 'modul', 'mathemat', 'mathemat']",main modules of mathematics  > mathematics
41,41,"Main Modules of Artificial Intelligence

> Machine Learning

> Deep Learning",main modul artifici intellig machin learn deep lea,"['main', 'modul', 'artifici', 'intellig', 'machin', 'learn', 'deep', 'lea']",main modules of artificial intelligence  > machine learning  > deep learning
42,42,"Main Modules of Business Intelligence

> Industry Insights",main modul busi intellig industri insight,"['main', 'modul', 'busi', 'intellig', 'industri', 'insight']",main modules of business intelligence  > industry insights
43,43,"Main Modules of Computer Engineering

> Data Engineering",main modul comput engin data engin,"['main', 'modul', 'comput', 'engin', 'data', 'engin']",main modules of computer engineering  > data engineering
44,44,"Main Topics of Python

> Integers, Floats and Booleans
> Strings
> Lists
> Tuples, Sets and Dictionaries
> Statements, Indentation and Conditionals
> Loops and Iterations
> List comprehension
> Functions and Methods
> Production Grade Programming
> Competitive Coding
> Numpy
> Pandas
> Data Wrangling
> Data Visualization",main topic python integ float boolean string list ,"['main', 'topic', 'python', 'integ', 'float', 'boolean', 'string', 'list']","main topics of python  > integers, floats and booleans > strings > lists > tuples, sets and dictiona"
45,45,"Main Topics of Analytics Framework

> Excel
> Tableau
> Business KPI
> SQL ",main topic analyt framework excel tableau busi kpi,"['main', 'topic', 'analyt', 'framework', 'excel', 'tableau', 'busi', 'kpi']",main topics of analytics framework  > excel > tableau > business kpi > structured query language 
46,46,"Main Topics of Mathematics

> Calculus
> Vector Algebra
> Matrix Algebra
> Probability Theory
> Summarizing Data
> Random Variables
> Discrete Distributions
> Continuous Distributions
> Joint Distributions
> Sampling & Statistical Inference
> Confidence Intervals
> Hypothesis Testing",main topic mathemat calculus vector algebra matrix,"['main', 'topic', 'mathemat', 'calculus', 'vector', 'algebra', 'matrix']",main topics of mathematics  > calculus > vector algebra > matrix algebra > probability theory > summ
47,47,"Main Topics of Machine Learning

> Linear Regression
> Bias variance tradeoff
> Regularized Linear Regression
> Cross validation and hyperparameter tuning
> Logistic regression
> Decision Trees
> Ensembles of decision trees
> Model Explainability
> k-nearest neighbors
> Naive Bayes Classifier
> Support Vector Machines
> General Modeling Techniques
> K-means clustering
> Hierarchical clustering
> Principal Component Analysis
> Anomaly Detection
> Natural Language Processing 
> Topic modeling
> Recommender Systems
> Time Series Analysis",main topic machin learn linear regress bias varian,"['main', 'topic', 'machin', 'learn', 'linear', 'regress', 'bias', 'varian']",main topics of machine learning  > linear regression > bias variance tradeoff > regularized linear r
48,48,"Main Topics of Deep Learning

> Neural Networks
> Deep Neural Networks
> Improving Deep Neural Networks
> Structuring ML Projects
> Convolutional Neural Networks
> Recurrent Neural Network",main topic deep learn neural network deep neural n,"['main', 'topic', 'deep', 'learn', 'neural', 'network', 'deep', 'neural', 'n']",main topics of deep learning  > neural networks > deep neural networks > improving deep neural netwo
49,49,"Main Topics of Industry Insights

> Guesstimate
> Case Study-ML in Heathcare
> Case Study-ML in Fraud risk analytics
> Case Study-ML in Credit Risk
> Case Study-ML in E-commerce",main topic industri insight guesstim case studyml ,"['main', 'topic', 'industri', 'insight', 'guesstim', 'case', 'studyml']",main topics of industry insights  > guesstimate > case study-ml in heathcare > case study-ml in frau
50,50,"Main Topics of Data Engineering

> Linux basics and terminal commands
> Python modules and project setup
> Version control-Git
> API basics with flask
> Flask application and FastAPI
> Docker
> Microservices-Basics & streamlit
> ML Lifecycle
> Azure Fundamentals
> MLOps-MLFlow
> PySpark
> Airflow",main topic data engin linux basic termin command p,"['main', 'topic', 'data', 'engin', 'linux', 'basic', 'termin', 'command', 'p']",main topics of data engineering  > linux basics and terminal commands > python modules and project s
51,51,"Subtopics of Integers, Floats and Booleans

> python tokens
> Python keywords
> python data types
> Arithmatic Operation 
> floor function
> ceiling function
> Variable 
> order of precedence in python",subtop integ float boolean python token python key,"['subtop', 'integ', 'float', 'boolean', 'python', 'token', 'python', 'key']","subtopics of integers, floats and booleans  > python tokens > python keywords > python data types > "
52,52,"Tuples basics

Tuples, Sets and Dictionaries are data structures to store multiple items in a single variable

Tuples are similar to list but immutable

('A string',23,100.232,'o',True) or 'A string',23,100.232,'o',True

> Tuple indexing same as list indexing

> Tuple slicing same as list slicing

> Tuple functions are same as list

> Tuples methods are less than list, .index(), .count()

> Tuples are used rarely only when immutability is a must",tupl basic tupl set dictionari data structur store,"['tupl', 'basic', 'tupl', 'set', 'dictionari', 'data', 'structur', 'store']","tuples basics  tuples, sets and dictionaries are data structures to store multiple items in a single"
53,53,"Set basics

Sets are an unordered collection of unique elements

set() function creates an emty set. {1,6,4,'abc'} is a non empty set. Sets are mutable like list

A set cannot have a mutable item like list within.

We can not convert a set into list

> Set A-B is not equal to set B-A",set basic set unord collect uniqu element set func,"['set', 'basic', 'set', 'unord', 'collect', 'uniqu', 'element', 'set', 'func']",set basics  sets are an unordered collection of unique elements  set() function creates an emty set.
54,54,"Set methods

.add()

 .update([2,3,4]) helps to add multiple elements to a set

.remove(element)
 
.union(another list, tuple or set)

 .intersection(another list, tuple or set)

set.intersection(set1,set2,set3)
set.intersection(*set_list)

 .difference(another list, tuple or set)

.symmetric_difference(another set) returns excluding intersection",set method add update234 help add multipl element ,"['set', 'method', 'add', 'update234', 'help', 'add', 'multipl', 'element']","set methods  .add()   .update([2,3,4]) helps to add multiple elements to a set  .remove(element)   ."
55,55,"Dictionary Basics

Dictionaries are hash tables or hash maps that can map keys to values

{} makes an emty dictionary. Non emty dictionary {key1:value1,key2:value2,key3:value3}

We can call values by their key, 

my_dict[key_name] or can use .get(key_name) method. 

We can also call an element of any data structure values and can apply method on that element.",dictionari basic dictionari hash tabl hash map map,"['dictionari', 'basic', 'dictionari', 'hash', 'tabl', 'hash', 'map', 'map']",dictionary basics  dictionaries are hash tables or hash maps that can map keys to values  {} makes a
56,56,"Dictionary methods

.keys(), .values(), .items(), .pop(key) 

.items() returns a list of tuples with key and values",dictionari method key valu item popkey item return,"['dictionari', 'method', 'key', 'valu', 'item', 'popkey', 'item', 'return']","dictionary methods  .keys(), .values(), .items(), .pop(key)   .items() returns a list of tuples with"
57,57,"Dictionary operations

Can add a new key-value pair

 my_dict['Design'] ='Sr Data Scientist' 

We can also do this by using .update({'Design':'Sr Data Scientist'}) method. 

.update() method can also be used to update exsisting key-values

Can delete a key-value pair

del my_dict[key]

dict() function converts list of paired elements into dictionary",dictionari oper add new keyvalu pair mydictdesign ,"['dictionari', 'oper', 'add', 'new', 'keyvalu', 'pair', 'mydictdesign']",dictionary operations  can add a new key-value pair   my_dict[design] =sr data scientist   we can al
58,58,"Subtopics of Strings

> Grabbing the element by index 
> String Slicing
> concatenate strings
> String functions
> String methods 
>  way to embed expressions inside string literals
> String duplication
> safe bridge ",subtop string grab element index string slice conc,"['subtop', 'string', 'grab', 'element', 'index', 'string', 'slice', 'conc']",subtopics of strings  > grabbing the element by index  > string slicing > concatenate strings > stri
59,59,"Subtopics of  Lists

> List Basics
> List functions
> List methods
> Nested List",subtop list list basic list function list method n,"['subtop', 'list', 'list', 'basic', 'list', 'function', 'list', 'method', 'n']",subtopics of  lists  > list basics > list functions > list methods > nested list
60,60,"Subtopics of Tuples, Sets and Dictionaries

> Tuples basics
> Set basics
> Set methods
> Dictionary Basics
> Dictionary methods
> Dictionary operations",subtop tupl set dictionari tupl basic set basic se,"['subtop', 'tupl', 'set', 'dictionari', 'tupl', 'basic', 'set', 'basic', 'se']","subtopics of tuples, sets and dictionaries  > tuples basics > set basics > set methods > dictionary "
61,61,"Subtopics of Statements, Indentation and Conditionals

> Assignment statement, conditional statement
> Expression
> Multi-line statements
> Comments
> Auto indentation",subtop statement indent condit assign statement co,"['subtop', 'statement', 'indent', 'condit', 'assign', 'statement', 'co']","subtopics of statements, indentation and conditionals  > assignment statement, conditional statement"
62,62,"Subtopics of Loops and Iterations

> Loop basics
> Two main loops in python
> Enumerate function
> break, continue, and pass statements ",subtop loop iter loop basic two main loop python e,"['subtop', 'loop', 'iter', 'loop', 'basic', 'two', 'main', 'loop', 'python', 'e']",subtopics of loops and iterations  > loop basics > two main loops in python > enumerate function > b
63,63,"Subtopics of List comprehension

> List comprehension basics
> Set Comprehension 
> Dictionary comprehension",subtop list comprehens list comprehens basic set c,"['subtop', 'list', 'comprehens', 'list', 'comprehens', 'basic', 'set', 'c']",subtopics of list comprehension  > list comprehension basics > set comprehension  > dictionary compr
64,64,"Subtopics of Functions and Methods

> Functions, Model and  Environment 
> Defining a function 
> Functions details 
> Function and method 
> Global variable",subtop function method function model environ defi,"['subtop', 'function', 'method', 'function', 'model', 'environ', 'defi']","subtopics of functions and methods  > functions, model and  environment  > defining a function  > fu"
65,65,"Subtopics of Production Grade Programming

> Production Grade Programming Basics
> Object Oriented Programming
>  Attribute and Object
> Defining a class
> Polymorphism
> Exception handling",subtop product grade program product grade program,"['subtop', 'product', 'grade', 'program', 'product', 'grade', 'program']",subtopics of production grade programming  > production grade programming basics > object oriented p
66,66,"Subtopics of Competitive Coding

> Competitive coding  basics ",subtop competit code competit code basic,"['subtop', 'competit', 'code', 'competit', 'code', 'basic']",subtopics of competitive coding  > competitive coding  basics 
67,67,"Assignment statement, conditional statement

Instructions that a Python interpreter can execute are called statements. A statement may or may not return a value. 

Assignment statement (assign some data or data structure to a variable) & conditional statement (if, elif (used for nested if), else, while, for and import)",assign statement condit statement instruct python ,"['assign', 'statement', 'condit', 'statement', 'instruct', 'python']","assignment statement, conditional statement  instructions that a python interpreter can execute are "
68,68,"Expression

Expression needs to be executed and evaluated and returns a value. For example 2+3 returns 5 in python.  All functions and methods in python returns a value and hence they are expressions

Thus we can conclude that all expressions are statements but all statements are not expressions",express express need execut evalu return valu exam,"['express', 'express', 'need', 'execut', 'evalu', 'return', 'valu', 'exam']",expression  expression needs to be executed and evaluated and returns a value. for example 2+3 retur
69,69,"Multi-line statements 

line continuation by () or [] or {}. Large string can be broken in multi line by \

We can also put multiple statements in a single line using semicolons. a = 1 ; b = 2 ; c = 3 or a,b,c=1,2,3",multilin statement line continu larg string broken,"['multilin', 'statement', 'line', 'continu', 'larg', 'string', 'broken']",multi-line statements   line continuation by () or [] or {}. large string can be broken in multi lin
70,70,"Comments

Comments (single line or multi line) are for programmers to better understand a program. Do comments by #. Another way of doing this is to use triple quotes, either ''' or """"""",comment comment singl line multi line programm bet,"['comment', 'comment', 'singl', 'line', 'multi', 'line', 'programm', 'bet']",comments  comments (single line or multi line) are for programmers to better understand a program. d
71,71,"Auto indentation

Auto indentation is done with: + enter or manually with tab to maintain is proper structure of the code",auto indent auto indent done enter manual tab main,"['auto', 'indent', 'auto', 'indent', 'done', 'enter', 'manual', 'tab', 'main']",auto indentation  auto indentation is done with: + enter or manually with tab to maintain is proper 
72,72,"We use ""truthy"" and ""falsy"" to differentiate from the boolean values True and False.",use truthi falsi differenti boolean valu true fals,"['use', 'truthi', 'falsi', 'differenti', 'boolean', 'valu', 'true', 'fals']","we use ""truthy"" and ""falsy"" to differentiate from the boolean values true and false."
73,73,"Subtopics of Numpy

> Library, Package
> Module
> Modular programming
> N-dimensional array
> Creating a two-dimensional array
> attributes of numpy array
> Advantages of numpy array
> Functions of Numpy
> Defining Array size
> Conversion of list and tuple to ndarray
> Array Manipulation
> Important functions in numpy
> Difference between view and copy in numpy
> Basic Operations & Functions in numpy
> Difference between list and numpy array",subtop numpi librari packag modul modular program ,"['subtop', 'numpi', 'librari', 'packag', 'modul', 'modular', 'program']","subtopics of numpy  > library, package > module > modular programming > n-dimensional array > creati"
74,74,"Subtopics of Pandas

> Basics of Pandas Dataframe
> Rows and columns of pandas dataframe
> Pandas series
> Convertion of numpy array to pandas dataframe
> Methods and attributes of pandas DataFrame 
> Connecting google drive to colab notebook
> For uploading any file from local computer to the colab session
> Converting a csv file to pandas DataFrame
> Editing in pandas df and then save them to csv/excel
> Slicing operation on Pandas DataFrame
> Slicing operation on a particular column  in Pandas DataFrame
> Conditional Slicing/Filtering in  Pandas DataFrame
> Adding new column in existing df in  Pandas DataFrame
> Removing one or multiple columns in  Pandas DataFrame
> Setting a column as row index in  Pandas DataFrame
> Methods for particular column in  Pandas DataFrame
> Sort values in  Pandas DataFrame
> Creating a new column with lambda function
> Pandas data display options
> Another way to convert dict to df
> Converting pandas df to numpy array
> String to numeric in  Pandas DataFrame
> Slicing operation on Pandas DataFrame
> Slicing operation on a particular column  in Pandas DataFrame
> Conditional Slicing/Filtering in  Pandas DataFrame
> Adding new column in existing df in  Pandas DataFrame
> Removing one or multiple columns in  Pandas DataFrame
> Setting a column as row index in  Pandas DataFrame
> Methods for particular column in  Pandas DataFrame
> Sort values in  Pandas DataFrame
> Creating a new column with lambda function
> Pandas data display options
> Another way to convert dict to df
> Converting pandas df to numpy array
> String to numeric in  Pandas DataFrame
> Numpy array or pandas df to matrix convertion",subtop panda basic panda datafram row column panda,"['subtop', 'panda', 'basic', 'panda', 'datafram', 'row', 'column', 'panda']",subtopics of pandas  > basics of pandas dataframe > rows and columns of pandas dataframe > pandas se
75,75,"Subtopics of Data Wrangling

> Basics of Data wrangling
> Concatenating pandas DataFrame
> DataFrame merging operation through joins
> Groupby operation on pandas dataframe for data analysis
> Detailed EDA
> Quick EDA
> Pandas profiling in colab notebook
> EDA practice
> Use of ast library
> Use of explode function
",subtop data wrangl basic data wrangl concaten pand,"['subtop', 'data', 'wrangl', 'basic', 'data', 'wrangl', 'concaten', 'pand']",subtopics of data wrangling  > basics of data wrangling > concatenating pandas dataframe > dataframe
76,76,"Subtopics of Data Visualization

> libraries for data visualization
> Matplotlib library
> Matplotlib pyplot function
> Matplotlib Line Plot
> Matplotlib Horizontal Bar Plot
> Matplotlib Box plot and Scatter Plot
> drawing a trend line in the scatter plot
> drawing horizontal or vertial lines
> Seaborn library
> seaborn Line plot
>  Scatter plot, Distribution/Density Plot
> seaborn Joint Distribution Plot, Heatmap, Bar Plot
> seaborn Histogram, Factor Plot, Box plot
> Seaborn Pairplot
> Ploting directly from pandas dataframe
> Editing the plot area with matplotlib and seaborn
> Combining two plots in a single graph
> plot functions
> saving the plot as png in google drive",subtop data visual librari data visual matplotlib ,"['subtop', 'data', 'visual', 'librari', 'data', 'visual', 'matplotlib']",subtopics of data visualization  > libraries for data visualization > matplotlib library > matplotli
77,77,"Subtopics of Excel

> Excel basics
> Data Handling in excel
> sections in excel file (top to bottom)
> Menu Bar Tabs in excel
> circular reference
> Data types in excel
> To view the groupby statistics like pandas df in excel
> Filter in excel
> Conditional Formatting 
 in excel
> Sorting in excel
> Removing Duplicates in excel
> Pivot and Slicers in excel
> Refreshing all pivot tables
> Charts in excel
> Dashboard in excel
> Waterfall or birdge graph analysis
> Excel functions
> Concatenation of text in excel",subtop excel excel basic data handl excel section ,"['subtop', 'excel', 'excel', 'basic', 'data', 'handl', 'excel', 'section']",subtopics of excel  > excel basics > data handling in excel > sections in excel file (top to bottom)
78,78,"Subtopics of Tableau

> Tableau basics
> Best practices for visualization
> Panes of Tableau
> Trend line models in Tableau
> Creating Calculated field in Tableau
> Chart area of Tableau
> Chart types in Tableau Tableau Pills
> TO PUBLISH THE WORKBOOK IN THE SERVER TO BE USED BY OTHERS
>  Tableau Desktop applications
> Components of a Dashboard
> Reference line and Reference band
> File extensions in Tableau
> Filters in Tableau
> Data blending
> Data Types in Tableau",subtop tableau tableau basic best practic visual p,"['subtop', 'tableau', 'tableau', 'basic', 'best', 'practic', 'visual', 'p']",subtopics of tableau  > tableau basics > best practices for visualization > panes of tableau > trend
79,79,"Subtopics of Business KPI

> KPI Basics
> KPI vs Metric
> Need of KPI for the company
> Types of Indicators
> Effectiveness and Efficiency
> Ways to develop KPI
> Three steps to a stronger KPI strategy
> Key Performance Indicators in practice
> Executive Dashboard
> Possible dangers of industrial performance indicators ",subtop busi kpi kpi basic kpi vs metric need kpi c,"['subtop', 'busi', 'kpi', 'kpi', 'basic', 'kpi', 'vs', 'metric', 'need', 'kpi', 'c']",subtopics of business kpi  > kpi basics > kpi vs metric > need of kpi for the company > types of ind
80,80,"Subtopics of SQL

> SQL Basics
> List of Relational database
> Difference between SQL and Python
> Basics of Relational Database (RDBMS)
> Basics of Non-Relational Database
> Parts of SQL
> SQL Statements
> Practicing sql queries
> To view all the table names
>  To view all the column names along with datatype of a table
> To check the shape of the table
> To see the head of the table
> SQL Query/ Statements/ commands
> SELECT and SELECT DISTINT statements
> Conditional operators
> Reading SQLite Database file as pandas df
> SQLite
> File extensions
> LIKE and ILIKE
> ADVANCED SQL COMMANDS
> TIMESTAMPS
> Extracts YEAR/ MONTH/ DAY/ WEEK/ QUARTER
> SUB-QUERY
> DATA/ DATA STRUCTURE TYPES IN SQL
> Primary Key
> Foreign Key
> CONSTRAINTS IN SQL
> CREATING DATABASES & TABLES
> CONDITIONAL EXPRESSIONS & PROCEDURES
> Simple View and Complex View
> Syntax for the CREATE VIEW
> SQL queries in Python
> SQL aliases
> Different Types of SQL JOINs
> CREATE DATABASE in SQL
> CREATE TABLE in SQL, DELETE in SQL
> SQL TRUNCATE TABLE
> SQL ALTER TABLE
> SQL string datatypes
> INSERT INTO in SQL
> UPDATE in SQL
> SQL CHECK
> UNION in SQL
> SQL TIMESTAMP and DATETIME
> SQL NOT IN
> Operation with Null values in SQL
> SQL CONCAT
> SQL and String
> Temporary table and Heap table
> Sequence in SQL
> Primary Key, Super Key and Candidate Key",subtop structur queri languag structur queri langu,"['subtop', 'structur', 'queri', 'languag', 'structur', 'queri', 'langu']",subtopics of structured query language  > structured query language basics > list of relational data
81,81,"Subtopics of Calculus

> Main branches of pure mathematics
> Application of mathematics in machine learning
> Basics of Calculus
> Machine Learning Use Cases of Calculus
> Basics of derivative
> The chain rule
> Point of maxima and point of minima
> Partial Derivatives
> Ways to find the slope of f(x,y)
> Jacobian Matrix
> Derivatives Formulas
>  Definite Integrals
> Formulas for integration
> Basics of Limit
> Solution method of limit of indeterminate form",subtop calculus main branch pure mathemat applic m,"['subtop', 'calculus', 'main', 'branch', 'pure', 'mathemat', 'applic', 'm']",subtopics of calculus  > main branches of pure mathematics > application of mathematics in machine l
82,82,"Subtopics of Vector Algebra

> Sub-branches  of Algebra
> Linear algebra
> Scaler, Vector, Matrix and Tensor
> Vector and Array
> Dimension and Direction
> position vector/ location vector/ radius vector
> Applications of Linear Algebra in Data Science
> Vector operations
> Vector Dot Product
> Vector Projection
> Vector Cross product
> L1 norm
> Representing multivariable linear equation in vector space
> Multivariable linear equations
> Visualization of an experience",subtop vector algebra subbranch algebra linear alg,"['subtop', 'vector', 'algebra', 'subbranch', 'algebra', 'linear', 'alg']","subtopics of vector algebra  > sub-branches  of algebra > linear algebra > scaler, vector, matrix an"
83,83,"Subtopics of Matrix Algebra

> Basics Matrix Algebra
> Types of Matrices
> Matrix operations
> Matrix multiplication
> Equation in matrix form
> Identity matrix
> Determinant of a matrix
> Transpose of a matrix
> Adjoint of a matrix
> cofactor matrix
> Inverse of a matrix
> Eigenvalues and Eigenvectors of a square matrix
> Use of eigen values and eigen vectors
> Nilpotent matrix ",subtop matrix algebra basic matrix algebra type ma,"['subtop', 'matrix', 'algebra', 'basic', 'matrix', 'algebra', 'type', 'ma']",subtopics of matrix algebra  > basics matrix algebra > types of matrices > matrix operations > matri
84,84,"Subtopics of Probability Theory

> Probability theory
> Set theory in Probability Theory
> Experiment, sample space, observation and experience
> Set Theory
> Venn Diagrams
> Mutually exclusive sets
> Permutation & Combination
> Basic Probability
> Probability Axiom#1
> Probability Axiom#2
> Probability Axiom#3 or Special Addition Rule
> Addition Rule
> Conditional Probability
> Multiplication Rule
> Bayes' Theorem
> Solving any set or probability problem
> Difference between mutually exclusive and independent events",subtop probabl theori probabl theori set theori pr,"['subtop', 'probabl', 'theori', 'probabl', 'theori', 'set', 'theori', 'pr']",subtopics of probability theory  > probability theory > set theory in probability theory > experimen
85,85,"Loop basics

Loops help us to execute a block of code repeatedly

When a statement  repeatedly execute a single statement or group of statements as long as the condition is true, then it is called a 'loop'",loop basic loop help us execut block code repeat s,"['loop', 'basic', 'loop', 'help', 'us', 'execut', 'block', 'code', 'repeat', 's']",loop basics  loops help us to execute a block of code repeatedly  when a statement  repeatedly execu
86,86,"Two main loops in python

for and while (similar to an if statement but continues to execute the code repeatedly as long as the condition is True)

A while loop in Python is used for 
indefinite type of iteration

for loops are used to loop through an iterable object (string, list, tuple, set and dict) and perform the same action on each element

for i in iterable_object:
last_element=i

Here, i is the element or key of the data_structure

for x, y in list_of_tuples:
   print(x,y) 

A word of caution however! It is possible to create an infinitely running loop with while statements",two main loop python similar statement continu exe,"['two', 'main', 'loop', 'python', 'similar', 'statement', 'continu', 'exe']",two main loops in python  for and while (similar to an if statement but continues to execute the cod
87,87,"Iterate means utter repeatedly.

for house in got_houses_list[::]:
  print(f""House {house}""). 

Here we are assigning the variable house as the elements of sliced list",iter mean utter repeat hous gothouseslist printfho,"['iter', 'mean', 'utter', 'repeat', 'hous', 'gothouseslist', 'printfho']","iterate means utter repeatedly.  for house in got_houses_list[::]:   print(f""house {house}"").   here"
88,88,"Enumerate function 

Enumerate() function adds a counter to an iterable(string, list, tuple, set and dict) and returns it in a form of enumerate object. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() method",enumer function enumer function add counter iterab,"['enumer', 'function', 'enumer', 'function', 'add', 'counter', 'iterab']","enumerate function   enumerate() function adds a counter to an iterable(string, list, tuple, set and"
89,89,"break, continue, and pass statements in our loops add additional functionality

break: Breaks out of the current closest enclosing loop.
continue: Goes to the top of the closest enclosing loop.
pass: Does nothing at all.",break continu pass statement loop add addit functi,"['break', 'continu', 'pass', 'statement', 'loop', 'add', 'addit', 'functi']","break, continue, and pass statements in our loops add additional functionality  break: breaks out of"
90,90,"Subtopics of Summarizing Data

> Basics of Summarizing Data
> Numerical, Categorical, Dichotomous, 
 and Ordinal Data
> Measure of central tendency
> Understanding Mean
> Understanding Median
> Understanding Mode
> Measure of spread
> Understanding Range
> Understanding Variance
> Understanding Standard Deviation
> Understanding Interquartile Range
> Steps to Calculate IQR
> Outliers with respect to IQR
> Measures of Symmetry
> Understanding skewness
> Libraries for Summarizing Data
> Skewness and Kurtosis measurements",subtop summar data basic summar data numer categor,"['subtop', 'summar', 'data', 'basic', 'summar', 'data', 'numer', 'categor']","subtopics of summarizing data  > basics of summarizing data > numerical, categorical, dichotomous,  "
91,91,"Subtopics of Random Variables

> Understanding random variable
> Python Code Random Variables
> Random seed
> Types of Random Variables
> Continuous and Discrete Random variables using numpy array
> Discrete Random Variables
> Continuous Random Variables
> Mean of Random Variables
> Variance of Random Variables
> Point probability, cumulative probability
> Formulas for expectation and variance",subtop random variabl understand random variabl py,"['subtop', 'random', 'variabl', 'understand', 'random', 'variabl', 'py']",subtopics of random variables  > understanding random variable > python code random variables > rand
92,92,"Subtopics of Discrete Distributions

> Basics of Probability distribution
> Types of Discrete Probability Distribution
> Uniform Distribution
> Bernoulli Distribution
> Binomial Distribution
> Geometric Distribution
> Poisson Distribution
> Moments of probability distribution
> Moments in Physics",subtop discret distribut basic probabl distribut t,"['subtop', 'discret', 'distribut', 'basic', 'probabl', 'distribut', 't']",subtopics of discrete distributions  > basics of probability distribution > types of discrete probab
93,93,"Subtopics of Continuous Distributions

> Basics of continuous distribution
> Types of continuous probability distribution
> Uniform Distribution
> Normal Distribution
> Standard Normal Distribution
> Exponential Distribution
> Gamma Function
> Gamma Distribution
> Chi-square Distribution
>  t-Distribution
> F-Distribution
> log-normal distribution
> Exponential expressions
> Euler's number
> Python Code for Continuous Distributions
> Continuous distributions in ML
> Most frequent types of distribution for data scientist",subtop continu distribut basic continu distribut t,"['subtop', 'continu', 'distribut', 'basic', 'continu', 'distribut', 't']",subtopics of continuous distributions  > basics of continuous distribution > types of continuous pro
94,94,"Subtopics of Joint Distributions

> Basics of Joint Distribution
> Two dimensional random vector
> Types of Joint 
Probability Distributions
> Independent Random Variables
> Probability mass function
> Degree of association
> Understanding of Covariance
> Understanding of Correlation",subtop joint distribut basic joint distribut two d,"['subtop', 'joint', 'distribut', 'basic', 'joint', 'distribut', 'two', 'd']",subtopics of joint distributions  > basics of joint distribution > two dimensional random vector > t
95,95,"Subtopics of Sampling & Statistical Inference

> Basics of Sampling & Statistical Inference
> Random sample
> Understanding of Statistics
> Statistical Inference
> Sample Mean and Sample Variance
> Independent and indentically distributed random variables
> Sampling with Replacement
> Central Limit Theorem (CLT) or z-results or z-score
> t Result or t Score
> F result or F Score
> Point Estimation
> Mean and Expectation",subtop sampl statist infer basic sampl statist inf,"['subtop', 'sampl', 'statist', 'infer', 'basic', 'sampl', 'statist', 'inf']",subtopics of sampling & statistical inference  > basics of sampling & statistical inference > random
96,96,"Subtopics of Confidence Intervals

> Basics of Confidence interval
> Calculating population parameters for one sample
> Population proportion
> Calculating population parameters for two samples from difference population
> Pooled variance
> Confidence limits
> Sampling schemes from best to worst",subtop confid interv basic confid interv calcul po,"['subtop', 'confid', 'interv', 'basic', 'confid', 'interv', 'calcul', 'po']",subtopics of confidence intervals  > basics of confidence interval > calculating population paramete
97,97,"Subtopics of Hypothesis Testing

> Basics of Hypothesis Testing
> Rare Event Rule for Inferential Statistics
> Components of a formal hypothesis test
> Null Hypothesis
> Alternative Hypothesis
> Identifying the null and alternative hypothesis
> Test Statistic
> Significance Level
> Critical Region
> Critical Value
> Two-tailed, Right-tailed, Left-tailed Tests
> P-value or probability value
> Conclusions in Hypothesis Testing based on P-value
> Type - I error
> Type - II error
> Power of a hypothesis test",subtop hypothesi test basic hypothesi test rare ev,"['subtop', 'hypothesi', 'test', 'basic', 'hypothesi', 'test', 'rare', 'ev']",subtopics of hypothesis testing  > basics of hypothesis testing > rare event rule for inferential st
98,98,"Subtopics of Linear Regression

> Understanding computer science
> Understanding convensional programming
>  Types of Computer languages
> Programming language  vs scripting language
> AI technique
> Basics of machine learning
> Basics of deep learning
> Basics of data science
> Role of a data scientist
> Difference between Business Analyst and Data Scientist
> Predictive ML model
> Meaning of heuristic technique
> Comparison between Heuristic technique and ML technique
> Types of learning models
> Application of regression model
> Application of classification model
> Application of Clustering model
> Components of reinforcement learning
> Application of Reinforcement learning model
> Supervised, parametric, regression algorithm
> Basics of linear regression
> Error or residuals
> Loss function or cost function
> Types of loss function
> OLS method for finding out the model parameters
> Gradient Descent Fundamentals
> Assumptions of regression
> Multicollinearity issue
> Heteroscedasticity issue
> Properties of regression line
> Advantages of linear regression
> Limitations of linear regression
> Data preparation for linear regression
> Omission of relevant variable from a regression equation
> Visualizing Linear Regression
> Understanding of Feature scaling
> Difference bwteen Matrix and metric
> Libraries for linear regression
> Importance of csv file
> Implementation Steps of Linear Regression
> transform and fit_transform",subtop linear regress understand comput scienc und,"['subtop', 'linear', 'regress', 'understand', 'comput', 'scienc', 'und']",subtopics of linear regression  > understanding computer science > understanding convensional progra
99,99,"Subtopics of Bias variance tradeoff

> Optimal Model
> Underfit Model
> Overfit Model
> Understanding Estimator
> Bias and Variance of an Estimator or ML Model
> Polynomial model
> Conversion of Categorical column to numerical
> Multi Label columns to Binary
> Number-String to numerical value
> Noise, Underfittiing, Overfitting and Overgeneralizing",subtop bias varianc tradeoff optim model underfit ,"['subtop', 'bias', 'varianc', 'tradeoff', 'optim', 'model', 'underfit']",subtopics of bias variance tradeoff  > optimal model > underfit model > overfit model > understandin
100,100,"Subtopics of Regularized Linear Regression

> Basics of Regularized Linear Regression
> Types of Regularization
> Ridge Regression (L2 Regularization)
> Lasso Regression (L1 Regularization)
> Libraries for Regularized Linear Regression
> Alpha value
> Variance Inflation Factor (VIF)",subtop regular linear regress basic regular linear,"['subtop', 'regular', 'linear', 'regress', 'basic', 'regular', 'linear']",subtopics of regularized linear regression  > basics of regularized linear regression > types of reg
101,101,"list comprehension basics

A list comprehension is a syntax for creating a list based on an existing list

For loop which returns a list/tuple can be written as list comprehension. Tuple function may be used to convert list comprehension into tuple.

[output_expression for  variable in input_sequence (string, list, tuple, set or dict) conditionals] 

[number**2 for number in list_of_numbers if number%2!=0]

[1 for act,pred in zipped_list if act==pred]",list comprehens basic list comprehens syntax creat,"['list', 'comprehens', 'basic', 'list', 'comprehens', 'syntax', 'creat']",list comprehension basics  a list comprehension is a syntax for creating a list based on an existing
102,102,"Set Comprehension

Structure of Set Comprehension is same as list comprehension.",set comprehens structur set comprehens list compre,"['set', 'comprehens', 'structur', 'set', 'comprehens', 'list', 'compre']",set comprehension  structure of set comprehension is same as list comprehension.
103,103,"Dictionary comprehension

Structure of Dictionary comprehension is same as list comprehension.",dictionari comprehens structur dictionari comprehe,"['dictionari', 'comprehens', 'structur', 'dictionari', 'comprehe']",dictionary comprehension  structure of dictionary comprehension is same as list comprehension.
104,104,"Function range and domain

Range is defined as all the possible values which a function  f(x)  can take.

Domain is defined as all the possible values which  x  can take.",function rang domain rang defin possibl valu funct,"['function', 'rang', 'domain', 'rang', 'defin', 'possibl', 'valu', 'funct']",function range and domain  range is defined as all the possible values which a function  f(x)  can t
105,105,"Subtopics of Cross validation and hyperparameter tuning

>  Cross validation Basics
> Simple Validation vs Cross Validation
> k-fold CV
> Python coding for CV
> yellowbrick CVScores
> Fundamentals of hyperparameters
> Hyperparameters tuning
> Coding for Hyperparameters tuning
> Steps of ML modelling
> Understanding Warnings
> Data preparation or data preprocessing
> Linear Transformation or Scaling
> Non-linear Transformations",subtop cross valid hyperparamet tune cross valid b,"['subtop', 'cross', 'valid', 'hyperparamet', 'tune', 'cross', 'valid', 'b']",subtopics of cross validation and hyperparameter tuning  >  cross validation basics > simple validat
106,106,"Subtopics of Logistic regression

> Basics of Logistic regression
> Meaning of odds and logit function in probability
> Logistic function or sigmoid function
> Working of parametric model
> Benefits of Parametric ML models
> Limitations of Parametric ML Models
> Classification model and probability
> Generative models  and Discriminative models
> Libraries for Logistic regression
> Training Logistic regression model
> Evaluating the performance of Logistic regression model
> Confusion Matrix
> Evaluation through confusion matrix
> Accuracy, Precision, Recall and F1-Score
> Importance of F1 Score
> Checking Cross-validation scores",subtop logist regress basic logist regress mean od,"['subtop', 'logist', 'regress', 'basic', 'logist', 'regress', 'mean', 'od']",subtopics of logistic regression  > basics of logistic regression > meaning of odds and logit functi
107,107,"Subtopics of Decision Trees

> Types of ML models
> Important Terminology in Decision Tree ML Model
> Steps of Decision tree algorithm
> Methods to measure the similarity of child nodes
> Fitting Decision tree classifier
> Fitting Decision tree regressor
> Coding for Visualizing Decision Tree
> Advantages of Decision tree
> Disadvantages of Decision tree",subtop decis tree type machin learn model import t,"['subtop', 'decis', 'tree', 'type', 'machin', 'learn', 'model', 'import', 't']",subtopics of decision trees  > types of machine learning models > important terminology in decision 
108,108,"Subtopics of Ensembles of decision trees

> Basics of Ensembles of decision trees
> Ensemble techniques
> Bagging technique
> Boosting technique
> Extreme Gradient Boosting
> Stacking technique
> Python coding for Ensembles of decision trees
> Finding feature importance
> Classification report
> Ways to improve random forest accuracy",subtop ensembl decis tree basic ensembl decis tree,"['subtop', 'ensembl', 'decis', 'tree', 'basic', 'ensembl', 'decis', 'tree']",subtopics of ensembles of decision trees  > basics of ensembles of decision trees > ensemble techniq
109,109,"Subtopics of Model Explainability

> Basics of Model Explainability
> Black Box Model vs. White Box Model
> Explainable AI
>  Importance of explainability
>  Scope of explainability
>  Approach of explainability
> Techniques or Libraries for Explainability in ML
> Local Interpretable Model-Agnostic Explanations
> Shapley Additive Explanations
> Implementing SHAP
> Explain Like I'm 5
> Other techniques for Explainability in ML",subtop model explartifici intelligencen basic mode,"['subtop', 'model', 'explartifici', 'intelligencen', 'basic', 'mode']",subtopics of model explartificial intelligencenability  > basics of model explartificial intelligenc
110,110,"Subtopics of k-nearest neighbors

> Basics of KNN
> Euclidean Distance
> Working of kNN
> Ways to select the value of k in the kNN Algorithm
> Disadvantages of kNN Algorithm
> Python coding for KNN
> Receiver operating characteristic and AUC
> Knn for recommender system
> Sparse matrix and Dense matrix",subtop knearest neighbor basic knearest neighbor e,"['subtop', 'knearest', 'neighbor', 'basic', 'knearest', 'neighbor', 'e']",subtopics of k-nearest neighbors  > basics of k-nearest neighbors > euclidean distance > working of 
111,111,"Subtopics of Naive Bayes Classifier

> Basics of Naive Bayes
> Bayes theorem
> Understanding of Naive Bayes
> Generative model vs Discriminative model
> Text Pre-processing
> Steps of Text Pre-processing
> One hot encoding
> Vectorization techniques
> Python coding for Naive Bayes",subtop naiv bay classifi basic naiv bay bay theore,"['subtop', 'naiv', 'bay', 'classifi', 'basic', 'naiv', 'bay', 'bay', 'theore']",subtopics of naive bayes classifier  > basics of naive bayes > bayes theorem > understanding of naiv
112,112,"Subtopics of Support Vector Machines

> Basics of SVM
> Kernel function
> Hinge loss function, hypersurface and hyperplane
> Python coding for SVM
> Tuning the SVM
> Advantages of SVM
> Disadvantages of SVM",subtop support vector machin basic support vector ,"['subtop', 'support', 'vector', 'machin', 'basic', 'support', 'vector']",subtopics of support vector machines  > basics of support vector machine > kernel function > hinge l
113,113,"Subtopics of General Modeling Techniques

> Understanding Feature engineering
> Importance of Feature Engineering
> Basic EDA
> Understanding Outliers
> Time Complexity and  Space Complexity",subtop general model techniqu understand featur en,"['subtop', 'general', 'model', 'techniqu', 'understand', 'featur', 'en']",subtopics of general modeling techniques  > understanding feature engineering > importance of featur
114,114,"Subtopics of K-means clustering

> Understanding Clustering Algorithm
> Python coding for Kmeans
> Few issues of Kmeans
> Silhouette score for finding best no. of clusters
> Elbow method for finding best no. of clusters
> Plotting Clusters
> Features of KMeans model
> Other Clustering models",subtop kmean cluster understand cluster algorithm ,"['subtop', 'kmean', 'cluster', 'understand', 'cluster', 'algorithm']",subtopics of k-means clustering  > understanding clustering algorithm > python coding for kmeans > f
115,115,"Subtopics of Hierarchical clustering

> Basics of Hierarchical clustering
> Types of hierarchical clustering
> Proximity matrix
>  Understanding Linkage
> Finding out no. of clusters from visualization
> Python coding for Hierarchical clustering",subtop hierarch cluster basic hierarch cluster typ,"['subtop', 'hierarch', 'cluster', 'basic', 'hierarch', 'cluster', 'typ']",subtopics of hierarchical clustering  > basics of hierarchical clustering > types of hierarchical cl
116,116,"Subtopics of Principal Component Analysis

> Basics of PCA
> Drawback of PCA
> Python Coding of PCA
> Understanding the important features in PCA
> Deeper Understanding of PCA
> Math behind PCA
> TruncatedSVD
> PCA Visualization
> Trace of matrix",subtop princip compon analysi basic princip compon,"['subtop', 'princip', 'compon', 'analysi', 'basic', 'princip', 'compon']",subtopics of principal component analysis  > basics of principal component analysis > drawback of pr
117,117,"Subtopics of Anomaly Detection

> Basics of Anomaly detection
> Assumptions in Anomaly detection
> Finding global outliers-Isolation Forest
> Univariate Anomaly Detection
> Python coding for Isolation forest
> Multivariate Anomaly Detection
> Finding local outliers-LOF
> Visual representation of univariate anomalies
> Deeper Understanding of Anomalies
> Data types of anomaly detection",subtop anomali detect basic anomali detect assumpt,"['subtop', 'anomali', 'detect', 'basic', 'anomali', 'detect', 'assumpt']",subtopics of anomaly detection  > basics of anomaly detection > assumptions in anomaly detection > f
118,118,"Subtopics of Natural Language Processing 

> Natural Language Understanding
> Important Applications of NLP
> Feature engineering in NLP
> n-gram in NLP
> Understanding TF-IDF
> RNN or LSTM
> Libraries for NLP
> Right order for a text classification
> Measuring the complexity of a sentence",subtop natur languag process natur languag underst,"['subtop', 'natur', 'languag', 'process', 'natur', 'languag', 'underst']",subtopics of natural language processing   > natural language understanding > important applications
119,119,"Subtopics of Topic modeling

> Basics of Topic modeling
> Basic assumptions of all topic models
> Libraries for Topic Modeling
> Understanding LDA
> Plate Notation
> Python coding of LDA model
> Visualization of LDA
> Cosine Similarity",subtop topic model basic topic model basic assumpt,"['subtop', 'topic', 'model', 'basic', 'topic', 'model', 'basic', 'assumpt']",subtopics of topic modeling  > basics of topic modeling > basic assumptions of all topic models > li
120,120,"Subtopics of Recommender Systems

> Basics of Recommender System
> Popular Recommender Systems
> Collaborative Filtering
> Content-Based Filtering
> Hybrid Approach
> Implementation strategies of Collaborative Filtering
> Latent factor models
> Matrix Factorization
> Implementation of SVD
> Metrices used for evaluation of Recommender systems
> Shortcoming of content-based recommender systems",subtop recommend system basic recommend system pop,"['subtop', 'recommend', 'system', 'basic', 'recommend', 'system', 'pop']",subtopics of recommender systems  > basics of recommender system > popular recommender systems > col
121,121,"Functions, Model and  Environment

Functions are to view the properties of a data or data structure and methods are to perform some actions on data or data structure. 

e.g. functions and methods can be applied on string, list, tuple, set, dict

In natural language, function means assigning a memory location for taking some input and giving some output. Model is a combination of multiple functions. Environment is a collection of multiple models.

Functions is one of the most basic levels of reusing code. It groups together a set of statements so they can be run more than once.",function model environ function view properti data,"['function', 'model', 'environ', 'function', 'view', 'properti', 'data']","functions, model and  environment  functions are to view the properties of a data or data structure "
122,122,"Defining a function

There are many inbuilt functions associated with data or data structure, however we can define our own functions as and when required.

def example_function(argument):
  '''
  This functions returns the …...
  '''
  return desired_result

> argument means the input

> function without return statement, 

def change(one):
   print(one*3)",defin function mani inbuilt function associ data d,"['defin', 'function', 'mani', 'inbuilt', 'function', 'associ', 'data', 'd']","defining a function  there are many inbuilt functions associated with data or data structure, howeve"
123,123,"Functions details

We can pass 'n' number of arguments in a function

def change(first, *second):
 
change(1,2,3,4)- Here the first input is 1 and the second input as tupe (2,3,4)

Function can have multiple arguments like example_function(arg1,arg2,..etc) or multiple returns like return x,y,…etc

Function can have multiple return statement like if ….return x else return y",function detail pass n number argument function de,"['function', 'detail', 'pass', 'n', 'number', 'argument', 'function', 'de']","functions details  we can pass n number of arguments in a function  def change(first, *second):   ch"
124,124,"Function and method

Function and method both look similar as they perform in an almost similar way. A method is called by its name but it is associated with an object (dependent).

Methods are of the form:

object.method(arg1,arg2,etc...)",function method function method look similar perfo,"['function', 'method', 'function', 'method', 'look', 'similar', 'perfo']",function and method  function and method both look similar as they perform in an almost similar way.
125,125,"Global variable

Iside a function, we can call global variable (defined outside function)

b=10 

def change():
global b
return b

Then, change() will return 10",global variabl isid function call global variabl d,"['global', 'variabl', 'isid', 'function', 'call', 'global', 'variabl', 'd']","global variable  iside a function, we can call global variable (defined outside function)  b=10   de"
126,126,"Subtopics of Time Series Analysis

> Basics of Time Series
> Components of Time Series
> Multiplicative Model
> Ways to approach a Time Series Prediction Problem
> Naive Forecast
> Moving Average
> Weighted average
> Exponential smoothing
> Double exponential smoothing
> Statistical technique in Time Series
> Econometric approach
> Understanding ARIMA
> Understanding SARIMA
> Libraries for Time Series Analysis
> Machine learning approach in Time series
> Downside of SARIMA
> Understanding Leakage
> Prophet or Facebook Prophet
> Cross Validation in Time Series
> Demand pattern classification for choosing the right model in time series
> Product forecastability
",subtop time seri analysi basic time seri compon ti,"['subtop', 'time', 'seri', 'analysi', 'basic', 'time', 'seri', 'compon', 'ti']",subtopics of time series analysis  > basics of time series > components of time series > multiplicat
127,127,"Subtopics of Neural Networks

> Basics of Artificial Neural Network
> Perceptron, neuron, node, unit
> Activation function
> Model coefficients
> The components of neural network
> Backward propagation
> Understanding epochs
> Understanding of batch size
> Optimal accuracy in Neural Network
> Applications of Neural Networks
> Steps of ANN
> Libraries for ANN model
> Define, Create and Compile ANN model
> fit the ANN model on the dataset
> evaluate the ANN model
> Plotting roc_curve
> Plotting precision_recall_curve",subtop neural network basic artifici neural networ,"['subtop', 'neural', 'network', 'basic', 'artifici', 'neural', 'networ']","subtopics of neural networks  > basics of artificial neural network > perceptron, neuron, node, unit"
128,128,"Subtopics of Deep Neural Networks

> Basic of Deep Neural Networks
> SNN, CNN and RNN
> Channels of RGB image
> Deeper understanding of deep neural network
> Loss function and Cost Function in DNN
> Coputation graph
> Calculation behind gradient descent
> Understanding vectorization
> Vectorized implementation of forward propagation for layer l
> Notations in DNN
>  Vector dot product
> Element-wise matrix multiplication
> Notation for multiple layers
> Activation functions
> Linear and nonlinear activation function
> Keras and TensorFlow
> Problem of zero initialization
> Initializing parameters for the model
>  Deep Learning capabilities
> Mathematics behind 4L neural network for multiple observation
> Layer dimension notation in Neural Network
> Weight notation in Neural Network
> Bias notation in vectorized form in Neural Network
> Network notation for i th experience
> Forward propagation and backpropagation
> shallow neural network",subtop deep neural network basic deep neural netwo,"['subtop', 'deep', 'neural', 'network', 'basic', 'deep', 'neural', 'netwo']","subtopics of deep neural networks  > basic of deep neural networks > snn, convolutional neural netwo"
129,129,"Subtopics of Improving Deep Neural Networks

> Basics of Improving Deep Neural Networks
> Basic 'recipe' for all machine learning models
> DNN model complexity
> Dropout regularization
> Implementing dropout
> Data Augmentation
> Early stopping technique to stop overfitting
> Importance of normalized inputs
> Vanishing/exploding gradients
> Batch size and iteration
> Optimization Algorithms
> Mini-Batch gradient descent
> Notation of Mini-batch
> Choosing our mini-batch size
> Gradient Descent with momentum
> Exponentially weighted averages
> RMSprop
> Adam Optimization Algorithm
> Learning rate decay
> Problem of local optima
> Hyperparameters tuning in DNN
> Batch normalization
> Multi-class classification
> Deep learning programming framework
> Python coding for DNN
> Different vector operations in tensor flow
> Cross entropy loss function
> Computational graph",subtop improv deep neural network basic improv dee,"['subtop', 'improv', 'deep', 'neural', 'network', 'basic', 'improv', 'dee']",subtopics of improving deep neural networks  > basics of improving deep neural networks > basic reci
130,130,"Subtopics of Structuring ML Projects

> Basics Structuring ML project
> Orthogonalization Basics
> Fundamental assumptions of supervised learning
> Setting up our goal
> Train/dev/test distribution
> Changing dev/test sets and metrics
> Bias/Variance Analysis
> Human level error and avoidable bias
> Error Analysis
> Mislabeled data
> Mismatched training and dev/test data
> Learning from multiple tasks
> Transfer learning
> Multi-task learning
> End to end deep learning
> Pros and cons of end to end deep learning
> Difference between Multi-class and multi-task learning
> Testing the model for the entire dataset",subtop structur machin learn project basic structu,"['subtop', 'structur', 'machin', 'learn', 'project', 'basic', 'structu']",subtopics of structuring machine learning projects  > basics structuring machine learning project > 
131,131,"Subtopics of Convolutional Neural Networks

> Perceptual task
> Basics of Pixel
> Convolution on Black-and-white Image
> Convolution operation
> Padding in CNN
> Strided Convolution
> Convolution on RGB image
> Types of layer in a convolutional network
> Pooling Layer
> Necessity of Convolution
> Calulating the size of output volume for convolution or pooling
> Notation for multiple CONV layers
> Common behavior of all the CNN architecture
> LeNet-5 architecture
> AlexNet architecture
> VGG-16 architecture
> ResNet and Inception architecture
> Network in Network
> Using open-source implementation
> Common Augmentation methods
> Sources of data for any ML model
> Object detection
> Tips for winning competitions
> Difference between image classification and object detection
> Image Classification with Localization
> Landmark detection
> Object detection
> Object detection algorithm
> YOLO algorithm
> Steps in YOLO algorithm
> Intersection over union  algorithm
> Non-max supression
> anchor box algorithm
> Face Verification
> Face Recognition
> Siamese network
> Training set for calculating Triplet loss
> Neural Style Transfer
> Finding generated image
> Style and style matrix ",subtop convolut neural network perceptu task basic,"['subtop', 'convolut', 'neural', 'network', 'perceptu', 'task', 'basic']",subtopics of convolutional neural networks  > perceptual task > basics of pixel > convolution on bla
132,132,"Subtopics of Recurrent Neural Network

> Examples of Models with sequence data
> Notation in RNN
> Vectorization of words
> Problem of a standard neural network
> Summary of RNN
> Types of RNN
> Language Modelling
> Sampling or creating sequence
> Character-level language model
> Exploding and Vanishing gradient
> Gated Recurrent Unit
> Long short-term memory
> Bidirectional RNN
> Deep RNN
> Natural language generation
> Huggingface Transformer
> NLP and Word Embedding
> Featurized representation
> Visualizing word embeddings
> Embedding matrix
> Transfer learning and word embeddings
> Learning word embeddings
> Context/target pairs
> Word2Vec model
> Problem with softmax classification
> Negative Sampling
> GloVe model
> The problem of bias in word embeddings
> Use of Pre-trained model for getting word embeddings
> Machine translation
> Finding the most likely translation
> Refinements to beam search
> BFS and DFS
> Error analysis on beam search
> Attention model
> Blue Score
> Speech recognition
> Basic Rule of CTC based technique
> Trigger word detection",subtop recurr neural network exampl model sequenc ,"['subtop', 'recurr', 'neural', 'network', 'exampl', 'model', 'sequenc']",subtopics of recurrent neural network  > examples of models with sequence data > notation in recurre
133,133,"Subtopics of Guesstimate

> Basics of case interview
> Categorization of case studies
> Guesstimate (Problem Solving Approach)",subtop guesstim basic case interview categor case ,"['subtop', 'guesstim', 'basic', 'case', 'interview', 'categor', 'case']",subtopics of guesstimate  > basics of case interview > categorization of case studies > guesstimate 
134,134,"Subtopics of Case Study-ML in Heathcare

> Meaning of Diagnosis
> ML in Healthcare
> Geometric Mean Length of Stay
> Features for the prediction of LOS",subtop case studymachin learn heathcar mean diagno,"['subtop', 'case', 'studymachin', 'learn', 'heathcar', 'mean', 'diagno']",subtopics of case study-machine learning in heathcare  > meaning of diagnosis > machine learning in 
135,135,"Subtopics of Case Study-ML in Fraud risk analytics

> Understanding Fraud
> WAYS TO CAPTURE FRAUDULENT BEHAVIOURS
> Social Engineering Scams
> Synthetic Minority Over-sampling Technique",subtop case studyml fraud risk analyt understand f,"['subtop', 'case', 'studyml', 'fraud', 'risk', 'analyt', 'understand', 'f']",subtopics of case study-ml in fraud risk analytics  > understanding fraud > ways to capture fraudule
136,136,"Subtopics of Case Study-ML in Credit Risk

> Curious Case of Customer Credit
> Overall Objective of Credit Risk
> Predictive Analytics of Credit Risk
> Prescriptive Analytics
> Metrics that help in evaluating our model’s accuracy
> Model Health using KS Scores
> Decision Making using Risk Bins",subtop case studyml credit risk curious case custo,"['subtop', 'case', 'studyml', 'credit', 'risk', 'curious', 'case', 'custo']",subtopics of case study-ml in credit risk  > curious case of customer credit > overall objective of 
137,137,"Subtopics of Case Study-ML in E-commerce

> Curious Case of Customer Contacts
> Important Features in E-commerce
>  Predictive Analytics in  E-commerce",subtop case studyml ecommerc curious case custom c,"['subtop', 'case', 'studyml', 'ecommerc', 'curious', 'case', 'custom', 'c']",subtopics of case study-ml in e-commerce  > curious case of customer contacts > important features i
138,138,"Subtopics of Linux basics and terminal commands

> Understanding UNIX Operating System
> Kernel and shell of UNIX
> Understanding LINUX Operating System
> Ways to connect to an EC2 instance
> Security of Remote Computer
> Git Bash Understanding
> Introduction to Repository
> Steps to connect to AWS EC2 instance with git bash
> Understanding IP address
> Basic Bash or Linux commands",subtop linux basic termin command understand unix ,"['subtop', 'linux', 'basic', 'termin', 'command', 'understand', 'unix']",subtopics of linux basics and terminal commands  > understanding unix operating system > kernel and 
139,139,"Subtopics of Python modules and project setup

> Few UNIX commands during project setup
> File Permission Handling for security
> UNIX command in colab notebook
> Creating a Simple Module
> Basics of modular programming
> One-Off Script Layout
> Installable Package Layout
> App with Internal Packages Layout
> Data Science Project Layout
> Using cookiecutter for packaging
> stdin, stdout and stderr
> Reading a static data file from inside a Python package
> Code for Creating python library
> Testing during packaging of our own python library",subtop python modul project setup unix command pro,"['subtop', 'python', 'modul', 'project', 'setup', 'unix', 'command', 'pro']",subtopics of python modules and project setup  > few unix commands during project setup > file permi
140,140,"Production Grade Programming Basics

Production environment is the setting where  products are actually put into operation for their intended uses by end users.

1. Object Oriented Programming (OOP)
2. Handling Errors and Exceptions

Production grade code is where all chances of error are taken care with try block and new classes are created suitably",product grade program basic product environ set pr,"['product', 'grade', 'program', 'basic', 'product', 'environ', 'set', 'pr']",production grade programming basics  production environment is the setting where  products are actua
141,141,"Object Oriented Programming 

OOP is a programming language model organized around object and class (a group of objects) rather than actions and lada logic

> Object is an identifiable entity with some characteristic and behavior. Thus object means identifiers (a name used to identify a variable, function, class, module or other object)

> Instantiation in OOP means creating an instance of class",object orient program oop program languag model or,"['object', 'orient', 'program', 'oop', 'program', 'languag', 'model', 'or']",object oriented programming   oop is a programming language model organized around object and class 
142,142,"Attribute and Object

An attribute is a characteristic of an object. A method is an operation we can perform with the object.",attribut object attribut characterist object metho,"['attribut', 'object', 'attribut', 'characterist', 'object', 'metho']",attribute and object  an attribute is a characteristic of an object. a method is an operation we can
143,143,"Defining a class

Apart from these standard classes, we can create new class as and when required. We can define different attributes and methods inside the class with proper indendation

By convention we give classes a name that starts with a capital letter.

> Similar functions are grouped together in a class

class Cylinder:
  
  def __init__(self, radius=1, height=1):
    self.radius=radius
    self.height=height
    
  def volume(self):
    return 3.14 * ((self.radius)**2)* self.height

  def surface_area(self):
    return 2 * 3.14 * self.radius* self.height

_init_ is used to define the attribute

radius=1, height=1 are the default values of the variables to avoid null error.",defin class apart standard class creat new class r,"['defin', 'class', 'apart', 'standard', 'class', 'creat', 'new', 'class', 'r']","defining a class  apart from these standard classes, we can create new class as and when required. w"
144,144,"Polymorphism

polymorphism refers to the way in which different object classes can share the same method name",polymorph polymorph refer way differ object class ,"['polymorph', 'polymorph', 'refer', 'way', 'differ', 'object', 'class']",polymorphism  polymorphism refers to the way in which different object classes can share the same me
145,145,"Exception handling   

Errors detected during execution are called exceptions. There is full list of built in python exceptions-

https://docs.python.org/3/library/exceptions.html

try:
   we do our operations here...
   ...
except:
   If there is an exception, then execute this block.

else:
   If there is no exception then execute this block. 

There can be more than zero except statements in a try-except block

try:
   Code block here
   ...
   Due to any exception, this code may be skipped!

finally:
   This code block would always be executed.

> Finally can also be used with try and except block",except handl error detect execut call except full ,"['except', 'handl', 'error', 'detect', 'execut', 'call', 'except', 'full']",exception handling     errors detected during execution are called exceptions. there is full list of
146,146,Single Underscore in the variable name(var_ ): Sometimes used as a name for temporary or insignificant variables,singl underscor variabl namevar sometim use name t,"['singl', 'underscor', 'variabl', 'namevar', 'sometim', 'use', 'name', 't']",single underscore in the variable name(var_ ): sometimes used as a name for temporary or insignifica
147,147,"create, read, update and delete

CRUD Meaning: CRUD is an acronym that comes from the world of computer programming and refers to the four functions that are considered necessary to implement a persistent storage application: create, read, update and delete.",creat read updat delet crud mean crud acronym come,"['creat', 'read', 'updat', 'delet', 'crud', 'mean', 'crud', 'acronym', 'come']","create, read, update and delete  crud meaning: crud is an acronym that comes from the world of compu"
148,148,"Double Leading and Trailing Underscore in the variable name( __var__ ): Indicates special methods defined by the Python language. 

",doubl lead trail underscor variabl name var indic ,"['doubl', 'lead', 'trail', 'underscor', 'variabl', 'name', 'var', 'indic']",double leading and trailing underscore in the variable name( __var__ ): indicates special methods de
149,149,"Subtopics of Version control-Git

> CVCS and DVCS
> Introduction to GitHub
> GitHub workflow
> Steps of creating local git repository
> Concept of Branch
> Synchronising local git repository with Github
> Merge Conflict
> Continuous Integration/ Continuous Deployment or Delivery",subtop version controlgit central version control ,"['subtop', 'version', 'controlgit', 'central', 'version', 'control']",subtopics of version control-git  > central version control system and distributed version control s
150,150,"Subtopics of API basics with flask

> Understanding API
> Types of web pages
> Machine Learning Services
> Types of API
> Basics of Flask
> Benefits of using the Flask framework
> Popular HTTP Requests
> Use of Virtual Environment
> Creating a virtual environment
> Understanding pip install
> Creating a Web App in Flask
> Jinja techniques
> Ways to a edit script code in Colab notebook
> Flaskr as a basic blog application
> Hyper Text Markup Language",subtop applic program interfac basic flask underst,"['subtop', 'applic', 'program', 'interfac', 'basic', 'flask', 'underst']",subtopics of application programming interface basics with flask  > understanding application progra
151,151,"Subtopics of Flask application and FastAPI

> Understanding FastAPI
> WSGI vs ASGI
> Features of FastAPI
> Pydantic Data Model
> FastAPI Working with SQL",subtop flask applic fastapi understand fastapi web,"['subtop', 'flask', 'applic', 'fastapi', 'understand', 'fastapi', 'web']",subtopics of flask application and fastapi  > understanding fastapi > web server gateway interface v
152,152,"Subtopics of Docker

> Docker Basics
> Image or Docker Image
> Container or Docker Container
> Introduction to dockerhub
> Ways to install docker in EC2 instance
> Basic Docker Commands
> Docker or Container volume
> Docker Bridge Networking and Port Mapping
> Docker Compose
> Docker swarm",subtop docker docker basic imag docker imag contai,"['subtop', 'docker', 'docker', 'basic', 'imag', 'docker', 'imag', 'contai']",subtopics of docker  > docker basics > image or docker image > container or docker container > intro
153,153,"Subtopics of Microservices-Basics & streamlit

> Understanding Microservices
> API Gateway
> Development of Microservices
> Creating a Streamlit file
> Creating a file for streamlit inside the session",subtop microservicesbas streamlit understand micro,"['subtop', 'microservicesbas', 'streamlit', 'understand', 'micro']",subtopics of microservices-basics & streamlit  > understanding microservices > application programmi
154,154,"Subtopics of ML Lifecycle

> Machine learning engineering
> 4 Phases to ML Lifecycle
> Challenges with ML during development
> Challenges with ML in production
> Data drift and Model drift",subtop machin learn lifecycl machin learn engin 4 ,"['subtop', 'machin', 'learn', 'lifecycl', 'machin', 'learn', 'engin', '4']",subtopics of machine learning lifecycle  > machine learning engineering > 4 phases to machine learni
155,155,"Competitive coding  basics 

Competitive coding is important for interview to test coding aptitude

30-40% companies keep this competitive coding round in their interview process

> not allowed to run the code on python interpreter or editor. 

> Pen and paper test to check how we run the code on our head.",competit code basic competit code import interview,"['competit', 'code', 'basic', 'competit', 'code', 'import', 'interview']",competitive coding  basics   competitive coding is important for interview to test coding aptitude  
156,156,"google colab notebook

To run a python code on cloud we can use google colab notebook (is the Jupyter notebook that run in the cloud and are highly integrated with Google Drive).",googl colab notebook run python code cloud use goo,"['googl', 'colab', 'notebook', 'run', 'python', 'code', 'cloud', 'use', 'goo']",google colab notebook  to run a python code on cloud we can use google colab notebook (is the jupyte
157,157,"Code editor tend to go for a broader approach and able to edit all types of files, instead of specializing in a particular type or language. Example of Code Editors are VS Code, PyCharm",code editor tend go broader approach abl edit type,"['code', 'editor', 'tend', 'go', 'broader', 'approach', 'abl', 'edit', 'type']","code editor tend to go for a broader approach and able to edit all types of files, instead of specia"
158,158,Compiler scans the entire program and translates the whole of it into machine code at once. ,compil scan entir program translat whole machin co,"['compil', 'scan', 'entir', 'program', 'translat', 'whole', 'machin', 'co']",compiler scans the entire program and translates the whole of it into machine code at once. 
159,159,"Interpreter translates just one statement of the program at a time into machine code and takes very less time to analyse the code. 

cmd.exe is the default command-line interpreter for the OS/2, eComStation, ArcaOS, Microsoft Windows, and ReactOS operating systems. The name refers to its executable filename. It is also commonly referred to as cmd or the Command Prompt, referring to the default window title on Windows

",interpret translat one statement program time mach,"['interpret', 'translat', 'one', 'statement', 'program', 'time', 'mach']",interpreter translates just one statement of the program at a time into machine code and takes very 
160,160,"IDLE is Integrated Development and Learning Environment for Python. Example of IDE's are Spyder, Jupyter notebook

",integr develop learn environ integr develop learn ,"['integr', 'develop', 'learn', 'environ', 'integr', 'develop', 'learn']",integrated development and learning environment is integrated development and learning environment f
161,161,"Python language was created in 1991

Van Rossum thought he needed a name that was short, unique, and slightly mysterious, so he decided to call the language Python. Python is a high level language.",python languag creat 1991 van rossum thought need ,"['python', 'languag', 'creat', '1991', 'van', 'rossum', 'thought', 'need']","python language was created in 1991  van rossum thought he needed a name that was short, unique, and"
162,162,"Anaconda and python

Anaconda is the heaviest and the biggest snake in the world (can weight upto 550 pound or 250kg or more and length 25 feets or more). 

On the other hand, the python is no doubt the longest snake in the world(can be of length 33 feet or more).

To run a python code in local machine, we need to install anaconda which installs python with many more libraries like numpy, pandas, matplotlib etc. and we can use jupyter notebook. ",anaconda python anaconda heaviest biggest snake wo,"['anaconda', 'python', 'anaconda', 'heaviest', 'biggest', 'snake', 'wo']",anaconda and python  anaconda is the heaviest and the biggest snake in the world (can weight upto 55
163,163,"Cloud computing or Cluster computing, Cloud Storage

Cloud (or Cluster) computing means the use of remote computer having remote storage (called cloud storage), OS and application softwares.

Cloud Storage examples: Google Drive, Amazon Drive (AWS S3) and Azure Storage. Google Drive offers 15 GB for free including emails & Google Photos and Amazon Drive (S3) gives we 5 GB for free.
> This remote computer is also called cloud server. Server (local or remote) is a computer connected to multiple users.",cloud comput cluster comput cloud storag cloud clu,"['cloud', 'comput', 'cluster', 'comput', 'cloud', 'storag', 'cloud', 'clu']","cloud computing or cluster computing, cloud storage  cloud (or cluster) computing means the use of r"
164,164,"Types of computer languages

High-Level Language (programmer-friendly, requires a compiler/interpreter to be translated into machine code) and Low-level language (machine-friendly,requires an assembler that would translate instructions)",type comput languag highlevel languag programmerfr,"['type', 'comput', 'languag', 'highlevel', 'languag', 'programmerfr']","types of computer languages  high-level language (programmer-friendly, requires a compiler/interpret"
165,165,"Jupyter notebook

Jupyter is a very popular application used for data analysis. It's an IPython notebook (""interactive python""). We can run each block of code separately. Python has two basic modes: script and interactive. The script mode is the mode where the scripted and finished .py files are run in the Python interpreter. Interactive mode is a command line shell which gives immediate feedback for each statement

Jupyter, comes from the core supported programming languages that it supports: Julia, Python, and R. There are many more languages that it supports. 
",jupyt notebook jupyt popular applic use data analy,"['jupyt', 'notebook', 'jupyt', 'popular', 'applic', 'use', 'data', 'analy']",jupyter notebook  jupyter is a very popular application used for data analysis. its an ipython noteb
166,166,"
Notebook documents

Notebook documents are both human-readable documents containing the analysis description and the results (figures, tables, etc..) as well as executable documents which can be run to perform data analysis.",notebook document notebook document humanread docu,"['notebook', 'document', 'notebook', 'document', 'humanread', 'docu']", notebook documents  notebook documents are both human-readable documents containing the analysis de
167,167,"collaboration site

A collaboration site is used to support project teams, research groups, and other collaborative work. Teams, committees, and student groups may make announcements, engage in online discussions, and share resources within their collaboration sites.",collabor site collabor site use support project te,"['collabor', 'site', 'collabor', 'site', 'use', 'support', 'project', 'te']","collaboration site  a collaboration site is used to support project teams, research groups, and othe"
168,168,"Understanding of language

>> Basic data, data-structure (int, float, boolean, str, list, tuple, set, dict) and variables can be thought of basic words of python language
>> Syntax is grammer for the language
>> expression, conditional and statements are the sentences of the language
>> Loops, functions, methods and classes are some set of sentences to perform an operation on the data (experience stored in a computer)

>> Subjects like Social Science or Humanities, Science, Commerce and Technical are nothing but different features/ dimensions/ parameters/streams for understanding life experience logically. Every subject is again an experience for human being and thus have some special words or features or concepts

>> Libraries are subject specific. Thus every libraries have some special words, functions and methods and class.

>> Data scientists are mainly concerned with statistics related libraries to understand any experience (data) from logical dimension

>> The improvement of human intelligence is also based on understanding an experience logically. The first experience of human being is social science or humanites",understand languag basic data datastructur int flo,"['understand', 'languag', 'basic', 'data', 'datastructur', 'int', 'flo']","understanding of language  >> basic data, data-structure (int, float, boolean, str, list, tuple, set"
169,169,"Subtopics of Azure Fundamentals

> Cloud computing advantages
> Cloud service models
> Runtime system
> CPU vs GPU
> Azure machine learning platform
> Azure ML Workspace and Azure ML piplines
> Connecting to azure ml
> Different cloud deployment models",subtop azur fundament cloud comput advantag cloud ,"['subtop', 'azur', 'fundament', 'cloud', 'comput', 'advantag', 'cloud']",subtopics of azure fundamentals  > cloud computing advantages > cloud service models > runtime syste
170,170,"Subtopics of MLOps-MLFlow

> ML Lifecycle management
> MLOps and DevOps
> Agile software development
> Key components of MLOps
> Advantages of MLOps
> Key outcomes of MLOps
> Model registry and Metadata
> MLFLow Basics",subtop machin learningopsmachin learningflow machi,"['subtop', 'machin', 'learningopsmachin', 'learningflow', 'machi']",subtopics of machine learningops-machine learningflow  > machine learning lifecycle management > mac
171,171,"Subtopics of PySpark

> Apache Spark (PySpark)
> Advantages of Spark
> Spark Modules
> RDD operations
> Anatomy of Spark Application
> Spark application coding
> Directed Acyclic Graph
> Azure Databricks
> Spark application deploy modes",subtop pyspark apach spark pyspark advantag spark ,"['subtop', 'pyspark', 'apach', 'spark', 'pyspark', 'advantag', 'spark']",subtopics of pyspark  > apache spark (pyspark) > advantages of spark > spark modules > resilient dis
172,172,"Subtopics of Airflow

> Apache Airflow
> Introduction to Jenkins
> Use of Apache Airflow
> Airflow Components
> Web Server and Scheduler
> Executor, Worker and Operator
> Metadata Database
> Applications of Airflow
> Understanding Celery",subtop airflow apach airflow introduct jenkin use ,"['subtop', 'airflow', 'apach', 'airflow', 'introduct', 'jenkin', 'use']",subtopics of airflow  > apache airflow > introduction to jenkins > use of apache airflow > airflow c
173,173,"Library

A library is a collection of python packages with multiple programming blocks
Library means main folder
Top 10 Python Libraries for Data Scientists : 
Numpy (stands for Numerical Python, released in 2005), 
Pandas (in 2008), 
SciPy (in 2001), 
Matplotlib (in 2003),
Seaborn (in 2018), 
Scikit-Learn (in 2007), TensorFlow (in 2015), 
Keras (in 2015), 
Theano, 
PyTorch,  
sys, 
time",librari librari collect python packag multipl prog,"['librari', 'librari', 'collect', 'python', 'packag', 'multipl', 'prog']",library  a library is a collection of python packages with multiple programming blocks library means
174,174,"Package

A package is a collection of Python modules

Package means sub folder

We may import the entire library (the main folder) or a particular package (a sub folder) or a particular module (a file) with dot(.) operator",packag packag collect python modul packag mean sub,"['packag', 'packag', 'collect', 'python', 'modul', 'packag', 'mean', 'sub']",package  a package is a collection of python modules  package means sub folder  we may import the en
175,175,"Module

 A module is a single Python file that consists of classes, functions, attributes and methods

Module means .py files

The term Library, Package or module are often used interchangeably, especially since many libraries only consist of a single module

Modules provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers. 

Modules also provide standardized solutions for many problems that occur in everyday programming.

some popular categories of Python modules are: 
1. operating system
2. math
3. time
4. data visualization ",modul modul singl python file consist class functi,"['modul', 'modul', 'singl', 'python', 'file', 'consist', 'class', 'functi']","module   a module is a single python file that consists of classes, functions, attributes and method"
176,176,"Modular programming 

Modular programming refers to the process of breaking a large, unwieldy programming task into separate, smaller, more manageable subtasks or modules. 

Advantages are Simplicity, Maintainability, Reusability",modular program modular program refer process brea,"['modular', 'program', 'modular', 'program', 'refer', 'process', 'brea']","modular programming   modular programming refers to the process of breaking a large, unwieldy progra"
177,177,"N-dimensional array

The most important object defined in NumPy is an N-dimensional array type called ndarray.

> One dimensional array with 22 elements: size=(22)

> Two dimensional array with 4 lists each having 22 elements: size=(4,22)- similar like table with 4 rows and 22 columns

> Three dimensional array with 5 lists (dimension 1) each having 4 lists (dimension 2) each having 22 elements (dimension 3): size(5,4,22)",ndimension array import object defin numpi ndimens,"['ndimension', 'array', 'import', 'object', 'defin', 'numpi', 'ndimens']",n-dimensional array  the most important object defined in numpy is an n-dimensional array type calle
178,178,"Creating a two-dimensional array 

another_array = np.array([[1,2,5],[3,4,7]]) shape of this ndarray is 2,3. A full experience table can be stored in a 2D Numpy array. It may be of shape 1000k(rows),1k(column). The same may be done with nested list means a list having 1000k lists and each list haviing 1k elements",creat twodimension array anotherarray nparray12534,"['creat', 'twodimension', 'array', 'anotherarray', 'nparray12534']","creating a two-dimensional array   another_array = np.array([[1,2,5],[3,4,7]]) shape of this ndarray"
179,179,"attributes of numpy array

my_array.shape
my_array.ndim
my_array.size",attribut numpi array myarrayshap myarrayndim myarr,"['attribut', 'numpi', 'array', 'myarrayshap', 'myarrayndim', 'myarr']",attributes of numpy array  my_array.shape my_array.ndim my_array.size
180,180,"Advantages of numpy array 

1. Numpy array takes very less space in memory as compared to list
2. Execution time for any operation with Numpy array is also very less as compared to list 
3. Functionality wise Numpy array is better than list ",advantag numpi array 1 numpi array take less space,"['advantag', 'numpi', 'array', '1', 'numpi', 'array', 'take', 'less', 'space']",advantages of numpy array   1. numpy array takes very less space in memory as compared to list 2. ex
181,181,"Functions of Numpy

>> create ndarray 
1. np.empty((5,3),dtype=int)
2. np.zeros((3,3))
3. np.ones((3,3))
4. np.random.random(size=(4,3))
5. np.random.randint(0,5,size=(2,3,4))

np.random.randn(2, 3) # Return a sample (or samples) from the “standard normal” distribution with shape (2,3)

6. np.full((3,3),30)- creates a (3,3) array full of 30's
>> create 1d array
np.arange(10,20,2)- 
np.linspace(1,50,10)",function numpi creat ndarray 1 npempty53dtypeint 2,"['function', 'numpi', 'creat', 'ndarray', '1', 'npempty53dtypeint', '2']","functions of numpy  >> create ndarray  1. np.empty((5,3),dtype=int) 2. np.zeros((3,3)) 3. np.ones((3"
182,182,"Defining Array size

Size=(2) means an array or list with 2 elements

size=(2,3) means a list of 2 lists each having 3 elements

size=(2,3,4) means a list of 2 lists each having 3 lists each having 4 elements

array([[[1, 4, 3, 2],
        [1, 0, 2, 0],
        [0, 1, 3, 4]],

       [[4, 1, 2, 3],
        [0, 2, 0, 2],
        [2, 2, 4, 0]]])

(2,3,4) is actually the array shape and the size is 2*3*4 means 24, the total no. of elements",defin array size size2 mean array list 2 element s,"['defin', 'array', 'size', 'size2', 'mean', 'array', 'list', '2', 'element', 's']","defining array size  size=(2) means an array or list with 2 elements  size=(2,3) means a list of 2 l"
183,183,"Conversion of list and tuple to ndarray

np.array(my_list or my_tuple)

 np.array(my_list, ndmin = 2)
> Indexing and Slicing ndarray is same as list
slicing of 2D array
my_array[2:5,0:3]

or 
my_array[2:5,3]

or
my_array[2:5]
Grabbing element by index of 2D array
my_array[0,9] or my_array[0][9]

Grabbing element with .flat
my_array.flat[5]",convers list tupl ndarray nparraymylist mytupl npa,"['convers', 'list', 'tupl', 'ndarray', 'nparraymylist', 'mytupl', 'npa']","conversion of list and tuple to ndarray  np.array(my_list or my_tuple)   np.array(my_list, ndmin = 2"
184,184,"Array Manipulation

np.reshape(my_array,new_shape)
my_array.reshape(new_shape)
np.resize(my_array,shape_with_new_size) my_array.resize(shape_with_new_size)
np.transpose(my_array)
my_array.transpose()
my_array.T
my_array.flatten()
np.stack((array_1, array_2), axis=0)
np.fill_diagonal(my_matrix,5)

my_array.dtype=""float64""

>> Array shape is defined as size in the syntax",array manipul npreshapemyarraynewshap myarrayresha,"['array', 'manipul', 'npreshapemyarraynewshap', 'myarrayresha']","array manipulation  np.reshape(my_array,new_shape) my_array.reshape(new_shape) np.resize(my_array,sh"
185,185,"Important functions in numpy

1. np.insert(my_array, location_index, insert_value, axis=0),
2. np.append(my_array, array_to_append, axis=1)- array_to_append should match no. of elements in all  dimension except axis=1
3. np.delete(my_array, location_index, axis=2),
4. np.unique(my_array, return_index=True, return_counts=True)--returns a tuple of arrays
>> axis=0 is the 1st dim or minor axis and signifies rows, axis=1 is the 2nd dim or major axis and signifies columns",import function numpi 1 npinsertmyarray locationin,"['import', 'function', 'numpi', '1', 'npinsertmyarray', 'locationin']","important functions in numpy  1. np.insert(my_array, location_index, insert_value, axis=0), 2. np.ap"
186,186,"Difference between view and copy in numpy

a = np.array([2,34,12])
b = a
b[0] = -999 
print(b)
print(a)

[-999   34   12]
[-999   34   12]

copy

a = np.array([2,34,12])
b = a.copy()
b[0] = -999
print(b)
print(a)

[-999   34   12]
[ 2 34 12]

",differ view copi numpi nparray23412 b b0 999 print,"['differ', 'view', 'copi', 'numpi', 'nparray23412', 'b', 'b0', '999', 'print']","difference between view and copy in numpy  a = np.array([2,34,12]) b = a b[0] = -999  print(b) print"
187,187,"Basic Operations & Functions in numpy

a=array_1
b=array_2
np.add(a,b),
np.subtract(a,b),
np.multiply(a,b),
np.divide(a,b),
np.sum(a),

np.min(a,axis=2),
np.max(a,axis=1),
np.mean(a,axis=0),
np.median(a,axis=0)

np.var(a,axis=0),
np.sort(a,axis=0),

my_array.astype('float64')

np.argsort(a,axis=0)-returns the indices of sorted array,
np.argmin(a,axis=0)-returns the index of the minimum element 
np.where(conditional_statement)
my_array.tolist()- list(my_array) gives list of array which is of no use
np.percentile(my_df['column'],50)--returns the value for 50th percentile or median
",basic oper function numpi aarray1 barray2 npaddab ,"['basic', 'oper', 'function', 'numpi', 'aarray1', 'barray2', 'npaddab']","basic operations & functions in numpy  a=array_1 b=array_2 np.add(a,b), np.subtract(a,b), np.multipl"
188,188,"Basics of Pandas Dataframe

The name is derived from the term “panel data”. It is for doing practical, real world data analysis in Python

Pandas is designed to make it easier to work with structured data i.e. organised experience set.

The DataFrame object in pandas is ""a two-dimensional tabular, column-oriented data structure with both row and column labels."" (excel is a similar datastructure)

A DataFrame is like a fixed-size dict in that we can get and set values by index label

Due to its inherent tabular structure, pandas dataframes also allow for cells to have null values (i.e. no data value such as blank space, NaN(Not a Number), -999, etc).",basic panda datafram name deriv term “panel data” ,"['basic', 'panda', 'datafram', 'name', 'deriv', 'term', '“panel', 'data”']",basics of pandas dataframe  the name is derived from the term “panel data”. it is for doing practica
189,189,"Difference between list and numpy array

Python lists are flexible and can store data items of various types (e.g. integers, floats, text strings). List can be converted to numpy array.

However, numpy arrays generally deals with numerical data. Thus, numpy arrays can provide more functionality for running calculations such as element-by-element arithmetic operations",differ list numpi array python list flexibl store ,"['differ', 'list', 'numpi', 'array', 'python', 'list', 'flexibl', 'store']",difference between list and numpy array  python lists are flexible and can store data items of vario
190,190,"Rows and columns of pandas dataframe

The columns in pandas DataFrames can be different types (e.g. the first column containing integers and the second column containing text strings). Each value in pandas dataframe is referred to as a cell that has a specific row index and column index within the tabular structure.",row column panda datafram column panda datafram di,"['row', 'column', 'panda', 'datafram', 'column', 'panda', 'datafram', 'di']",rows and columns of pandas dataframe  the columns in pandas dataframes can be different types (e.g. 
191,191,"Pandas series

We can creare pandas series.

number_series = pd.Series([2, 3, 5, 6, 8])

Pandas series is nothing but a 1D numpy array",panda seri crear panda seri numberseri pdseries2 3,"['panda', 'seri', 'crear', 'panda', 'seri', 'numberseri', 'pdseries2', '3']","pandas series  we can creare pandas series.  number_series = pd.series([2, 3, 5, 6, 8])  pandas seri"
192,192,"Convertion of numpy array to pandas dataframe

We can convert a 2D numpy array, list of lists, dictionary, list of dicts, list of tuples into pandas DataFrame.

1. pd.DataFrame(some_2D_array , columns=['Akshay','Devashish','Divy'])
2. pd.DataFrame(list_of_lists, columns = ['Name', 'Age'])
3. pd.DataFrame([employee_dict])
4. pd.DataFrame(list_of_dicts)
5. pd.DataFrame(list_of_tuples, columns = ['Name', 'Age'])",convert numpi array panda datafram convert 2d nump,"['convert', 'numpi', 'array', 'panda', 'datafram', 'convert', '2d', 'nump']","convertion of numpy array to pandas dataframe  we can convert a 2d numpy array, list of lists, dicti"
193,193,"For uploading any file from local computer to the colab session

from google.colab import files
upload=files.upload()",upload file local comput colab session googlecolab,"['upload', 'file', 'local', 'comput', 'colab', 'session', 'googlecolab']",for uploading any file from local computer to the colab session  from google.colab import files uplo
194,194,"Methods and attributes of pandas DataFrame 

1. my_df.index
2. my_df.columns
3. my_df.head()
4. my_df.tail()
5. my_df.info()
6. my_df.rename(columns = {'Name':'Actor Name','Age' :'Actor Age'}, inplace=True)
or,
my_df.columns = list_of_new column_names
7. my_df.shape
8. my_df.describe() or my_df.groupby('column_name').describe()
9. my_df.values
10. my_df.keys() or  my_df.columns 

my_df.replace(r'?', np.NaN)  

my_df['col_name'].replace([np.inf, -np.inf], 0, inplace=True)

my_df['col_name'].replace(np.nan, my_df['col_name'].median(), inplace=True)

>> r string stands for 'raw string'

>> u string stands for 'unicode string'

>> np.inf means numpy infinite number

my_data.dropna(inplace=True) is called for removing the rows which contains the null values",method attribut panda datafram 1 mydfindex 2 mydfc,"['method', 'attribut', 'panda', 'datafram', '1', 'mydfindex', '2', 'mydfc']",methods and attributes of pandas dataframe   1. my_df.index 2. my_df.columns 3. my_df.head() 4. my_d
195,195,"Connecting google drive to colab notebook

from google.colab import drive
drive.mount('/content/drive')

-After mounting the drive in colab, we can  read a file/create a file/write in a file/append in a file/edit a file(csv/excel) 

",connect googl drive colab notebook googlecolab imp,"['connect', 'googl', 'drive', 'colab', 'notebook', 'googlecolab', 'imp']",connecting google drive to colab notebook  from google.colab import drive drive.mount(/content/drive
196,196,"Converting a csv file to pandas DataFrame

my_df=pd.read_csv('file_path')

or 
data = pd.read_csv(open_source_csv_url)

> url is also a file_path

> When data is in latin language,

my_df=pd.read_csv('file_path', encoding='latin-1')

For creating a duplicate copy of the dataframe

new_df = my_df.copy()
",convert csv file panda datafram mydfpdreadcsvfilep,"['convert', 'csv', 'file', 'panda', 'datafram', 'mydfpdreadcsvfilep']",converting a csv file to pandas dataframe  my_df=pd.read_csv(file_path)  or  data = pd.read_csv(open
197,197,"Creating an empty csv file on a specific folder

my_df = pd.DataFrame(list())

my_df.to_csv('folder_path/name_of_empty_csv.csv')
",creat empti csv file specif folder mydf pddatafram,"['creat', 'empti', 'csv', 'file', 'specif', 'folder', 'mydf', 'pddatafram']",creating an empty csv file on a specific folder  my_df = pd.dataframe(list())  my_df.to_csv(folder_p
198,198,"Writing data in a csv file

1. Create the csv writer

import csv

writer = csv.writer(open('file_path', 'w'))

2. Define variables

header=['Name','Weight','Height']

data=[['Ram',600,10],['Sham',400,20],['Jadu',200,10]]

3. Write multiple rows to the csv file

writer.writerow(header)

writer.writerows(data)",write data csv file 1 creat csv writer import csv ,"['write', 'data', 'csv', 'file', '1', 'creat', 'csv', 'writer', 'import', 'csv']",writing data in a csv file  1. create the csv writer  import csv  writer = csv.writer(open(file_path
199,199,"Appending new row to an exsisting csv file

import csv

writer = csv.writer(open('file_path', 'a'))

data=['Madhu',800,90]

writer.writerow(data)",append new row exsist csv file import csv writer c,"['append', 'new', 'row', 'exsist', 'csv', 'file', 'import', 'csv', 'writer', 'c']","appending new row to an exsisting csv file  import csv  writer = csv.writer(open(file_path, a))  dat"
200,200,"Editing a csv file

> Read the csv as pandas df and then do all the editing and then save it to csv

>> Reading a json file as pandas df

my_df=pd.read_json('file_path')",edit csv file read csv panda df edit save csv read,"['edit', 'csv', 'file', 'read', 'csv', 'panda', 'df', 'edit', 'save', 'csv', 'read']",editing a csv file  > read the csv as pandas df and then do all the editing and then save it to csv 
201,201,"Excel file with multiple worksheets

xls = pd.ExcelFile('file_path')

xls.sheet_names

df1 = pd.read_excel(xls, 'sheet_name')

Reading an excel file first sheet directly

df = pd.read_excel ('file_path')",excel file multipl worksheet xls pdexcelfilefilepa,"['excel', 'file', 'multipl', 'worksheet', 'xls', 'pdexcelfilefilepa']",excel file with multiple worksheets  xls = pd.excelfile(file_path)  xls.sheet_names  df1 = pd.read_e
202,202,"Writing in a new excel file

import xlwt
  
my_workbook = xlwt.Workbook() 

sheet1 = my_workbook.add_sheet(""sheet_name"")
  
font_style = xlwt.easyxf('font: bold 1,color red;')
  
sheet1.write(0, 0, 'JADU', font_style)

my_workbook.save(""file_path/file_name.xls"")",write new excel file import xlwt myworkbook xlwtwo,"['write', 'new', 'excel', 'file', 'import', 'xlwt', 'myworkbook', 'xlwtwo']",writing in a new excel file  import xlwt    my_workbook = xlwt.workbook()   sheet1 = my_workbook.add
203,203,"Editing in pandas df and then save them to csv/excel

my_df = pd.DataFrame([['Ram',600,10],['Sham',400,20],['Jadu',200,10]], columns=['Name','Weight','Height'])

my_df.to_csv('/content/drive/MyDrive/my_data.csv')

or,

my_df.to_excel('/content/drive/MyDrive/my_data.xlsx')",edit panda df save csvexcel mydf pddataframeram600,"['edit', 'panda', 'df', 'save', 'csvexcel', 'mydf', 'pddataframeram600']","editing in pandas df and then save them to csv/excel  my_df = pd.dataframe([[ram,600,10],[sham,400,2"
204,204,"Slicing operation on Pandas DataFrame

my_df.iloc[:,-4:] or my_df.iloc[:, 7]  slicing operation both on rows and columns by index

my_df.loc[list_of_index_name] -slicing by index name
 my_df.loc[:,['budget','genres','runtime']] -slicing operation on rows by index and on columns by column names

my_df[0:10] -slicing operation only on rows

my_df[['original_language','release_date','cast']] -slicing operation only on columns",slice oper panda datafram mydfiloc4 mydfiloc 7 sli,"['slice', 'oper', 'panda', 'datafram', 'mydfiloc4', 'mydfiloc', '7', 'sli']","slicing operation on pandas dataframe  my_df.iloc[:,-4:] or my_df.iloc[:, 7]  slicing operation both"
205,205,"Slicing operation on a particular column  in Pandas DataFrame

my_df['release_date'][0:5]

type(my_df['release_date']) is pandas series

Writing any value at a particular cell

my_df['column_name'][row_index]=value 

(This is similar like cell reference in excel)",slice oper particular column panda datafram mydfre,"['slice', 'oper', 'particular', 'column', 'panda', 'datafram', 'mydfre']",slicing operation on a particular column  in pandas dataframe  my_df[release_date][0:5]  type(my_df[
206,206,"Conditional Slicing/Filtering in  Pandas DataFrame

>> Normal Filtering with single selection on single or multiple columns

my_df.loc[(my_df['new_cast']=='Robert De Niro')]

my_df.loc[(my_df['new_cast']=='Robert De Niro') & (my_df['original_language']=='en')]

>> Normal Filtering with multiple selection on single column

my_df.loc[(my_df['original_language']=='en') | (my_df['original_language']=='fr')]

>> Conditional Filtering on single or multiple columns in  Pandas DataFrame

long_movies = my_df[my_df['runtime'] > 120]

long_english_movies = my_df[(my_df['original_language']=='en') & (df['runtime'] > 120)]",condit slicingfilt panda datafram normal filter si,"['condit', 'slicingfilt', 'panda', 'datafram', 'normal', 'filter', 'si']",conditional slicing/filtering in  pandas dataframe  >> normal filtering with single selection on sin
207,208,"Adding new column in existing df in  Pandas DataFrame

my_df['half_runtime'] = 250 
or 
my_df.half_runtime = [list_with_same_len]

my_df['half_runtime'] = my_df['runtime'] * 0.5

my_df['movie_profit'] = my_df['revenue'] - df['budget']
>> We can add rows/columns by concatination

Adding new row in existing df

my_df.loc[(my_df.index.max()+1)] =list_of_values_for columns ",ad new column exist df panda datafram mydfhalfrunt,"['ad', 'new', 'column', 'exist', 'df', 'panda', 'datafram', 'mydfhalfrunt']",adding new column in existing df in  pandas dataframe  my_df[half_runtime] = 250  or  my_df.half_run
208,209,"Removing one or multiple columns in  Pandas DataFrame

my_df.drop(list_of_columns, axis = 1, inplace=True)

Removing one or multiple rows

my_df=my_df.drop(my_df.index[list_of_index]).reset_index()

>> We can remove rows/columns by slicing",remov one multipl column panda datafram mydfdropli,"['remov', 'one', 'multipl', 'column', 'panda', 'datafram', 'mydfdropli']","removing one or multiple columns in  pandas dataframe  my_df.drop(list_of_columns, axis = 1, inplace"
209,210,"Setting a column as row index in  Pandas DataFrame

my_df.set_index('imdb_id', inplace=True)

inplace=True for parmanent change
",set column row index panda datafram mydfsetindexim,"['set', 'column', 'row', 'index', 'panda', 'datafram', 'mydfsetindexim']","setting a column as row index in  pandas dataframe  my_df.set_index(imdb_id, inplace=true)  inplace="
210,211,"Methods for particular column in  Pandas DataFrame

1. my_df['column_name'].apply(any_function)
2.
list(my_df['status'].unique())
3. my_df.original_language.nunique()
4. df['original_language'].value_counts()
5. df['homepage'].isnull()
6. my_df['homepage'].fillna(0)- works when there is np.NaN values
>> .isna() and .notna() functions are also useful
>>  Checking zero values
if 0 in my_df.values returns T/F",method particular column panda datafram 1 mydfcolu,"['method', 'particular', 'column', 'panda', 'datafram', '1', 'mydfcolu']",methods for particular column in  pandas dataframe  1. my_df[column_name].apply(any_function) 2. lis
211,212,"Sort values in  Pandas DataFrame

my_df.sort_values('runtime', ascending=False)

my_df.sort_values(['status','new_profit'], ascending=[False,True])

Sorting by index
my_df.sort_index(inplace=True)

To get the total null values in the entire dataset

print(my_df.isnull().sum())",sort valu panda datafram mydfsortvaluesruntim asce,"['sort', 'valu', 'panda', 'datafram', 'mydfsortvaluesruntim', 'asce']","sort values in  pandas dataframe  my_df.sort_values(runtime, ascending=false)  my_df.sort_values([st"
212,213,"Datetime operations

from datetime import datetime

from datetime import date

from datetime import timedelta

strptime means string parser, this will convert a string format to datetime.

strftime means string formatter, this will convert a datetime to string format.",datetim oper datetim import datetim datetim import,"['datetim', 'oper', 'datetim', 'import', 'datetim', 'datetim', 'import']",datetime operations  from datetime import datetime  from datetime import date  from datetime import 
213,214,"Creating a new column with lambda function (applied on pandas series)

my_df['new_column_name']=my_df['exsisting_column_name'].apply(lambda x : datetime.strptime(x,'%m/%d/%Y'))

my_df['class'] = my_df['Churn'].apply(lambda x : 1 if x == ""Yes"" else 0)

A lambda function is just like any normal python function, except that it has no name when defining it. lambda input_variables : output expression

add_lambda = lambda a, b: a + b

> To extract the year

pd.DatetimeIndex(my_df['datetime_col']).year

>>
df['new_cast'] = df['new_cast'].apply(lambda x:[i['name'] for i in x] if isinstance(x,list) else [])

>> isinstance() function returns True if the specified object is of the specified type, otherwise False ",creat new column lambda function appli panda seri ,"['creat', 'new', 'column', 'lambda', 'function', 'appli', 'panda', 'seri']",creating a new column with lambda function (applied on pandas series)  my_df[new_column_name]=my_df[
214,215,"Pandas data display options

pd.options.display.float_format = '{:.4f}'.format",panda data display option pdoptionsdisplayfloatfor,"['panda', 'data', 'display', 'option', 'pdoptionsdisplayfloatfor']",pandas data display options  pd.options.display.float_format = {:.4f}.format
215,216,"Difference among and & |

If a==5 and b==10:
print('mango') - This and operator is used along with if statement and returns boolean value

(column_a==5 & coumn_b==10) - returns the result when both the conditions are true

(column_a==5 | coumn_b==10) - returns the result when first condition is true and then returns the result when second condition is true",differ among a5 b10 printmango oper use along stat,"['differ', 'among', 'a5', 'b10', 'printmango', 'oper', 'use', 'along', 'stat']",difference among and & |  if a==5 and b==10: print(mango) - this and operator is used along with if 
216,217,"Use of Unary operator, ~

Unary means involving a single component

~ reverses an integer or boolean value",use unari oper unari mean involv singl compon reve,"['use', 'unari', 'oper', 'unari', 'mean', 'involv', 'singl', 'compon', 'reve']","use of unary operator, ~  unary means involving a single component  ~ reverses an integer or boolean"
217,218,"Another way to convert dict to df

pd.DataFrame.from_dict(dictionary_object, orient='index')",anoth way convert dict df pddataframefromdictdicti,"['anoth', 'way', 'convert', 'dict', 'df', 'pddataframefromdictdicti']","another way to convert dict to df  pd.dataframe.from_dict(dictionary_object, orient=index)"
218,219,"Converting pandas df to numpy array

my_df.to_numpy()",convert panda df numpi array mydftonumpi,"['convert', 'panda', 'df', 'numpi', 'array', 'mydftonumpi']",converting pandas df to numpy array  my_df.to_numpy()
219,220,"String to numeric in  Pandas DataFrame

my_df[list_of columns]=my_df[list_of columns].astype('float64')

or

pd.to_numeric(my_df[list_of columns])",string numer panda datafram mydflistof columnsmydf,"['string', 'numer', 'panda', 'datafram', 'mydflistof', 'columnsmydf']",string to numeric in  pandas dataframe  my_df[list_of columns]=my_df[list_of columns].astype(float64
220,221,"Numpy array or pandas df to matrix convertion

my_matrix = np.asmatrix(my_array or my_df)",numpi array panda df matrix convert mymatrix npasm,"['numpi', 'array', 'panda', 'df', 'matrix', 'convert', 'mymatrix', 'npasm']",numpy array or pandas df to matrix convertion  my_matrix = np.asmatrix(my_array or my_df)
221,222,"Basics of Data wrangling

Data wrangling is the process of cleaning and unifying messy and complex data sets for easy access and analysis.",basic data wrangl data wrangl process clean unifi ,"['basic', 'data', 'wrangl', 'data', 'wrangl', 'process', 'clean', 'unifi']",basics of data wrangling  data wrangling is the process of cleaning and unifying messy and complex d
222,223,"Concatenating pandas DataFrame

pd.concat(list_of_df)-concats both axis

pd.concat(list_of_df, axis=1)-Concatenate along axis 1- means columnwise",concaten panda datafram pdconcatlistofdfconcat axi,"['concaten', 'panda', 'datafram', 'pdconcatlistofdfconcat', 'axi']","concatenating pandas dataframe  pd.concat(list_of_df)-concats both axis  pd.concat(list_of_df, axis="
223,224,"DataFrame merging operation through joins

1. Inner join
2. Left join
3. Right join
4. Outer join

pd.merge(english_movies, long_movies, how='left',left_on='imdb_id',right_on='imdb_id')

> default join is inner",datafram merg oper join 1 inner join 2 left join 3,"['datafram', 'merg', 'oper', 'join', '1', 'inner', 'join', '2', 'left', 'join', '3']",dataframe merging operation through joins  1. inner join 2. left join 3. right join 4. outer join  p
224,225,"Groupby operation on pandas dataframe for data analysis

my_df.groupby('Year_of_release')['runtime'].max().reset_index()

my_df.groupby('Year_of_release')['imdb_id'].count().median()

grouped_actors = new_df.groupby('new_cast').agg({'new_genre':'sum','profit':'mean'}).reset_index()

agg() function is used to pass a function or list of functions to be applied

df.groupby([list_of_columns])['column_name'].median()",groupbi oper panda datafram data analysi mydfgroup,"['groupbi', 'oper', 'panda', 'datafram', 'data', 'analysi', 'mydfgroup']",groupby operation on pandas dataframe for data analysis  my_df.groupby(year_of_release)[runtime].max
225,226,"Detailed EDA

1. We can not see the whole of an extreamly large dataset. 
2. Therefore to understand the dataset we see the head(),tail() and shape of the dataset.
3. Then we try to understand the dataset from info() (missing values or np.NaN values, others strings like '?', '-', 'Not available' etc. and also check the type of values, wheather list, tuple, set or dict is available as string) and describe() methods
> Deleting the column with missing data
updated_df = df.dropna(axis=1, how='all')
> Deleting the row with missing data
updated_df = newdf.dropna(axis=0)

> Filling the Missing Values – Imputation
updated_df['Age']=updated_df['Age'].fillna(updated_df['Age'].mean())
>> imputation with mode of categorical
4. Then we perform data cleaning and simplify the data (converting others strings like '?', '-', 'Not available' etc. to np.NaN) and may do concatinating or merging operation in case of multiple datasets

5. Searching for the columns on which the dataset can be filtered. If not found then creating/extracting the columns/features on which the dataset can be filtered 

6. Then we do filtering on different conditions (extracting subset of data) and groupby operation (classfying the whole dataset on all possible groups) for further analysis

7. Then we plot the  data on different aspects to analyse it more deeply

8. We can plot the observed values for one feature with respect to observation numbers or we can plot relation between two features

> To check duplication of data
len(my_df[my_df.duplicated()]) shall be 0",detail exploratori data analysi 1 see whole extrea,"['detail', 'exploratori', 'data', 'analysi', '1', 'see', 'whole', 'extrea']",detailed exploratory data analysis  1. we can not see the whole of an extreamly large dataset.  2. t
226,227,"Quick EDA

For serious exploratory data analysis without writing code for different statistics and visualization, we can import  
pandas_profiling  for quick data analysis.",quick exploratori data analysi serious exploratori,"['quick', 'exploratori', 'data', 'analysi', 'serious', 'exploratori']",quick exploratory data analysis  for serious exploratory data analysis without writing code for diff
227,228,"Pandas profiling in colab notebook

1. Run the following code to install the new version of pandas profiling in colab

pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip 

2. Restart the kernel (connecting again to the hosted runtime)

3. Re-import the libraries

import pandas as pd
import numpy as np
from pandas_profiling import ProfileReport

4. Reread our data set as pandas dataframe

5. Generate profile report
 data_profile=ProfileReport(my_dataframe,title='file_name',html={'style':{'full_width':True}})

6. Viewing profile report in colabnotebook
 data_profile.to_notebook_iframe()

7. Saving the report as html

data_profile.to_file(output_file='folder_path/file_name.html')",panda profil colab notebook 1 run follow code inst,"['panda', 'profil', 'colab', 'notebook', '1', 'run', 'follow', 'code', 'inst']",pandas profiling in colab notebook  1. run the following code to install the new version of pandas p
228,229,"EDA practice

sklearn is a machinelearning library that provides 
1. Toy datasets (Boston house prices,Iris plants,Diabetis,hand writen digits,Breast Cancer etc.), 

2. Real world datasets, 

3. Generated datasets and 

4. Image Data for practice

from sklearn.datasets import load_diabetes

my_data=load_diabetes()",exploratori data analysi practic sklearn machinele,"['exploratori', 'data', 'analysi', 'practic', 'sklearn', 'machinele']",exploratory data analysis practice  sklearn is a machinelearning library that provides  1. toy datas
229,230,"Use of ast library

import ast

ast stands for Abstract Syntax Trees

ast.literal_eval(): A function that  evaluates a string containing a Python literal (basic words) as list, tuple, set or dictionaries

or,
df['genres'] = df.apply(lambda row: eval(row['genres']), axis=1)",use ast librari import ast ast stand abstract synt,"['use', 'ast', 'librari', 'import', 'ast', 'ast', 'stand', 'abstract', 'synt']",use of ast library  import ast  ast stands for abstract syntax trees  ast.literal_eval(): a function
230,231,"Use of explode function

new_df = df.explode('new_cast')

explode() function is used to transform each element of a list-like to a row, replicating the index values",use explod function newdf dfexplodenewcast explod ,"['use', 'explod', 'function', 'newdf', 'dfexplodenewcast', 'explod']",use of explode function  new_df = df.explode(new_cast)  explode() function is used to transform each
231,232,"libraries for data visualization

A Picture is worth a thousand words!

matplotlib and seaborn",librari data visual pictur worth thousand word mat,"['librari', 'data', 'visual', 'pictur', 'worth', 'thousand', 'word', 'mat']",libraries for data visualization  a picture is worth a thousand words!  matplotlib and seaborn
232,233,"Matplotlib library

MATLAB is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks.

Matplotlib work like MATLAB.

One of the core aspects of Matplotlib is matplotlib.pyplot.

Different plots are: 
1. line plot
2.bar plot, 
3. box plot,
4. scatter plot,
5. step plot,
6. histogram, 
7. Time Series, 
8. Fill Between ",matplotlib librari matlab proprietari multiparadig,"['matplotlib', 'librari', 'matlab', 'proprietari', 'multiparadig']",matplotlib library  matlab is a proprietary multi-paradigm programming language and numeric computin
233,234,"Matplotlib  pyplot function

Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.

import matplotlib.pyplot as plt

%matplotlib inline

from matplotlib import rcParams

%matplotlib inline sets the backend of matplotlib to the 'inline' backend. The resulting plots will then also be stored in the notebook document. This is line oriented magic function (Cell oriented magic functions are prefixed with a double %%)",matplotlib pyplot function pyplot function make ch,"['matplotlib', 'pyplot', 'function', 'pyplot', 'function', 'make', 'ch']","matplotlib  pyplot function  each pyplot function makes some change to a figure: e.g., creates a fig"
234,235,"Matplotlib Line Plot

plt.plot(pandas_series_for_x_label, pandas_series_for_y_label,'bo--',linewidth=2, markersize=8)

> 'bo-' means blue line, 0 as marker, -- is line style",matplotlib line plot pltplotpandasseriesforxlabel ,"['matplotlib', 'line', 'plot', 'pltplotpandasseriesforxlabel']","matplotlib line plot  plt.plot(pandas_series_for_x_label, pandas_series_for_y_label,bo--,linewidth=2"
235,236,"Matplotlib Horizontal Bar Plot

plt.barh(y,x,width=0.4,color='rosybrown')",matplotlib horizont bar plot pltbarhyxwidth04color,"['matplotlib', 'horizont', 'bar', 'plot', 'pltbarhyxwidth04color']","matplotlib horizontal bar plot  plt.barh(y,x,width=0.4,color=rosybrown)"
236,237,"Matplotlib Box plot and Scatter Plot

plt.boxplot(my_new_df['column_name'])

Scatter Plot
plt.scatter(x, y,s=80)

> s=80 is the marker size

> A histogram is a bar graph-like representation of data that buckets(or bins) a range of outcomes into columns along the x-axis. The y-axis represents the number count or percentage of occurrences in the data. Alternate is sns.distplot

",matplotlib box plot scatter plot pltboxplotmynewdf,"['matplotlib', 'box', 'plot', 'scatter', 'plot', 'pltboxplotmynewdf']",matplotlib box plot and scatter plot  plt.boxplot(my_new_df[column_name])  scatter plot plt.scatter(
237,238,"drawing a trend line in the scatter plot

fit = np.polyfit(x, y, deg=1) 

p = np.poly1d(fit) 

plt.plot(x,p(x),""r--"") 
",draw trend line scatter plot fit nppolyfitx deg1 p,"['draw', 'trend', 'line', 'scatter', 'plot', 'fit', 'nppolyfitx', 'deg1', 'p']","drawing a trend line in the scatter plot  fit = np.polyfit(x, y, deg=1)   p = np.poly1d(fit)   plt.p"
238,239,"drawing horizontal or vertial lines

plt.axvline(x=1.8, ymin=0, ymax=1, color ='red', linestyle =""--"",  linewidth = 1)

plt.axhline(y=2.2, xmin=0, xmax=1, color ='red', linestyle =""--"", linewidth = 1)

>> plt.gcf().autofmt_xdate() -to show x-axis labels inclined

> A time series graph is a line graph of repeated measurements taken over regular time intervals. Time is always shown on the horizontal axis.",draw horizont vertial line pltaxvlinex18 ymin0 yma,"['draw', 'horizont', 'vertial', 'line', 'pltaxvlinex18', 'ymin0', 'yma']","drawing horizontal or vertial lines  plt.axvline(x=1.8, ymin=0, ymax=1, color =red, linestyle =""--"","
239,240,"Seaborn library

Seaborn is a library for making statistical graphics in Python. It is built on top of matplotlib.

import seaborn as sns

Based on matplotlib but many special features are available in these plots",seaborn librari seaborn librari make statist graph,"['seaborn', 'librari', 'seaborn', 'librari', 'make', 'statist', 'graph']",seaborn library  seaborn is a library for making statistical graphics in python. it is built on top 
240,241,"seaborn Line plot,  Scatter plot, Distribution/Density Plot

1. Line plot
sns.lineplot(x=""index"",y=i1,data=eps,label=i1)
2. Scatter plot with regression line
sns.lmplot(x='feature_name', y='feature_name', data=my_df,height=10, aspect=1)
> lmplot is advance than sns.regplot(x='feature_name', y='feature_name', data=my_df)
3. Distribution/Density Plot
rcParams['figure.figsize'] = 7,7
sns.distplot(my_df['column_name'], hist=True)
> From the distplot we can find wheather the variables are normally distributed
",seaborn line plot scatter plot distributiondens pl,"['seaborn', 'line', 'plot', 'scatter', 'plot', 'distributiondens', 'pl']","seaborn line plot,  scatter plot, distribution/density plot  1. line plot sns.lineplot(x=""index"",y=i"
241,242,"seaborn Joint Distribution Plot, Heatmap, Bar Plot

4. Joint Distribution Plot

sns.jointplot(x='', y='', data=my_df,height=10)

5. Heatmap (for finding correlation between variables)

sns.heatmap(my_df.corr(), vmin=-1, cmap='Greens', annot=True)- visually pleasing
6. Bar Plot
plt.figure(figsize=(7,7))
 or
plt.rcParams['figure.figsize'] = (7,7)

sns.barplot(x='feature name', y='feature name', data=my_df)

",seaborn joint distribut plot heatmap bar plot 4 jo,"['seaborn', 'joint', 'distribut', 'plot', 'heatmap', 'bar', 'plot', '4', 'jo']","seaborn joint distribution plot, heatmap, bar plot  4. joint distribution plot  sns.jointplot(x=, y="
242,243,"seaborn Histogram, Factor Plot, Box plot

7. Histogram (plot of data group vs. frequency)
rcParams['figure.figsize'] = 10,5

sns.histplot(my_df['column_name'], kde=True)

8. Factor Plot
sns.factorplot(x='sample' ,y= 'value',data=my_df,height=10)

9. Box plot

rcParams['figure.figsize'] = 7,7

sns.boxplot(x='lsample', y='value', data=my_df)

sns.boxplot(new_df['RM'])
> We can check both distribution and outliers by boxplot",seaborn histogram factor plot box plot 7 histogram,"['seaborn', 'histogram', 'factor', 'plot', 'box', 'plot', '7', 'histogram']","seaborn histogram, factor plot, box plot  7. histogram (plot of data group vs. frequency) rcparams[f"
243,244,"10. Seaborn Pairplot

It is a very useful tool to carry out univariate, bivariate and multivariate analysis in a single plot

sns.pairplot(data=my_df, kind='scatter', diag_kind='kde')
plt.show()
>> Kind of diagonal graphs are Kernel Density Estimates(kde) and kind of non-diagonal graphs are scatter
>> Diagonal graphs are for univariate analysis
>> If dependent variable is the first column of dataset, then the first row graphs are for bivariate analysis
> In pairgrid plot, we can pass various parameter for more flexibility

",10 seaborn pairplot use tool carri univari bivari ,"['10', 'seaborn', 'pairplot', 'use', 'tool', 'carri', 'univari', 'bivari']","10. seaborn pairplot  it is a very useful tool to carry out univariate, bivariate and multivariate a"
244,245,"Ploting directly from pandas dataframe

# set the required column as index(parameter to be plotted in x axis)

my_df=my_df.set_index('column_name')

It is very useful to compare multiple time series. Then 'Time' shall be set as index and then my_df.plot.line(figsize=(7,7))

my_df.groupby(['iyear'])['nkill', 'nwound'].sum().plot.line(figsize=(10,8))
>> requirement of scaler transformation may be investigated

# We can plot one parameter vs one parameter

my_df.plot.line(x='MS Flow through Turbine',y='Load\n', figsize=(15,10))

> We can also plot bar, scatter, box etc. 

df.groupby(['month_number'])[['facewash','facecream']].mean().plot.bar()

my_df.hist(bins=10, figsize=(7,7))

> The plot method on Series and DataFrame is just a simple wrapper around plt.plot()",plote direct panda datafram set requir column inde,"['plote', 'direct', 'panda', 'datafram', 'set', 'requir', 'column', 'inde']",ploting directly from pandas dataframe  # set the required column as index(parameter to be plotted i
245,246,"Editing the plot area with matplotlib and seaborn

plt.title('my_chart_title')
plt.ylabel('chart_y_label')
plt.xlabel('chart_x_label')
plt.grid()

plt.rcParams['figure.figsize'] = (7,7) 

> size=7 inches/ 7 inches

sns.set_style(""darkgrid"")

>  pandas data structures, matplotlib and seaborn are closely integrated",edit plot area matplotlib seaborn plttitlemychartt,"['edit', 'plot', 'area', 'matplotlib', 'seaborn', 'plttitlemychartt']",editing the plot area with matplotlib and seaborn  plt.title(my_chart_title) plt.ylabel(chart_y_labe
246,247,"Combining two plots in a single graph

plt.figure(figsize=(7,7))
sns.distplot(np.log10(my_df['feature_name']),color=""y"")

-combination of bar plot and distribution plot
Create multiple plots
 1. using for loop

2. Using subplot
plt.suptitle(""Title for whole figure"", fontsize=16)
plt.subplot(total_subplot_rows,total_subplot_columns, subplot_number)

sns.countplot(my_df['feature_name'])",combin two plot singl graph pltfigurefigsize77 sns,"['combin', 'two', 'plot', 'singl', 'graph', 'pltfigurefigsize77', 'sns']","combining two plots in a single graph  plt.figure(figsize=(7,7)) sns.distplot(np.log10(my_df[feature"
247,248,"plot functions

plt.show()

 If we are not using matplotlib in interactive mode at all, figures will only appear if we invoke plt.show().

plt.axis('equal')

is used to show multiple plots under same axis

plt.legend(list_of_features)

Shows the legend

sns.color_palette()

To see the available colors",plot function pltshow use matplotlib interact mode,"['plot', 'function', 'pltshow', 'use', 'matplotlib', 'interact', 'mode']","plot functions  plt.show()   if we are not using matplotlib in interactive mode at all, figures will"
248,249,"saving the plot as png in google drive

plt.savefig('folder_path/my_fig.png')

Saving any print output as word file in google drive

with open(""folder_path/file_name.docx"", ""a"") as f:
    print(""Hello StackOverflow!"", file=f)
    print(""I have a question."", file=f)",save plot png googl drive pltsavefigfolderpathmyfi,"['save', 'plot', 'png', 'googl', 'drive', 'pltsavefigfolderpathmyfi']",saving the plot as png in google drive  plt.savefig(folder_path/my_fig.png)  saving any print output
249,250,"Excel basics

Excel is a datastructure with columns and rows. It is also a programming platform
>> In excel, cell reference (a cell or a range of cells) like A5,H6 etc. are feature names or variable names or identifiers
>> Thus, excel stores all the values (input value or output of any function) mentioned in different cell in different variables (A5,H6 etc.) 
>> Thus we can call the variables inside any function (=A5*H6) as and when required and it is stored in another variable (say, K1)
For many organisation presentation in excel is needed",excel basic excel datastructur column row also pro,"['excel', 'basic', 'excel', 'datastructur', 'column', 'row', 'also', 'pro']",excel basics  excel is a datastructure with columns and rows. it is also a programming platform >> i
250,251,"Data Handling in excel

> Paste special option may be used whenever only one feature of a cell or a group of cells are required to paste.

We can also do arithmetic operations and transpose with paste special. 

> Present the workbook with multiple worksheets

> Arithmatic operation with a particular cell: use $ (F4 key does this freezing of cell)",data handl excel past special option may use whene,"['data', 'handl', 'excel', 'past', 'special', 'option', 'may', 'use', 'whene']",data handling in excel  > paste special option may be used whenever only one feature of a cell or a 
251,252,"sections in excel file (top to bottom)

1. Menu bar

2. Standard toolbar

3. Formula bar (including Name box)

4. Headings (A,B,C…)

5. Spreadsheet

6. Sheet tabs

7. Status bar",section excel file top bottom 1 menu bar 2 standar,"['section', 'excel', 'file', 'top', 'bottom', '1', 'menu', 'bar', '2', 'standar']",sections in excel file (top to bottom)  1. menu bar  2. standard toolbar  3. formula bar (including 
252,253,"Menu Bar Tabs in excel

1. Home tab 
i. Font alignment, type, size, colour
ii. Cell colour
iii. Merge Cells
iv. Hide columns and rows (cell> format)
v. Adjusting row height and column width  (cell> format)
vi. Clear format
2. View tab
i. Freeze panes
ii. Gridlines
3. Data tab 
i. Grouping Rows or Columns
4. Insert tab 
i. Pictures or logo
ii. Hyperlinks (mainly used for multiple worksheets)

5. Formula (Functions) tab
i. Logical (AND, OR, IF, NOT etc.)

ii. Date & Time

iii. Text (Left, Right, Concatenate(&), Find, Text to Column)

iv. Lookups (VLOOKUP& HLOOKUP)

-VLOOKUP returns value after a search from data range that is set up vertically.
VLOOKUP(search_by_value_as _row_index, A1:B4, required_value_column_index, range_lookup)

v. Math

vi. More Functions (Financial, Statistical, Engineering and more)

vii. Formula Auditing> Error Checking> circular ref, 
Data>edit links, 
IFERROR
evaluate formula, 
trace dependent etc.",menu bar tab excel 1 home tab font align type size,"['menu', 'bar', 'tab', 'excel', '1', 'home', 'tab', 'font', 'align', 'type', 'size']","menu bar tabs in excel  1. home tab  i. font alignment, type, size, colour ii. cell colour iii. merg"
253,254,"

> A circular reference refers to a formula, that visits its own or another cell more than once in its chain of calculations",circular refer refer formula visit anoth cell chai,"['circular', 'refer', 'refer', 'formula', 'visit', 'anoth', 'cell', 'chai']","  > a circular reference refers to a formula, that visits its own or another cell more than once in "
254,255,"Data types in excel

It is very important to check the data types in excel to avoid mistakes

General,

Date,

Number,

Time,

Text,

Percentage
",data type excel import check data type excel avoid,"['data', 'type', 'excel', 'import', 'check', 'data', 'type', 'excel', 'avoid']",data types in excel  it is very important to check the data types in excel to avoid mistakes  genera
255,256,"To view the groupby statistics like pandas df in excel

> First hide the not necessary columns or group the columns 

> Then sort the values of the coulmns on which groupby operation to be performed

> Then click on Data tab> Outline> Subtotal and use functions like sum, average, count, min, max etc. in the required column

> Mode statistic in pivot table is available in powerpivot of office 2013. But mode can be calculated through mode() function in lower version

> Then copy different  subtotal statistics and paste in separate worksheet (Select the required rows and columns and then press Alt+; for selecting the visible cells only) for creating the dashboard
 
> PivotTable is better option than Subtotal for creating the dashboard",view groupbi statist like panda df excel first hid,"['view', 'groupbi', 'statist', 'like', 'panda', 'df', 'excel', 'first', 'hid']",to view the groupby statistics like pandas df in excel  > first hide the not necessary columns or gr
256,257,"Filter in excel
(Home/Data tab)

1. Filter
> The most important function for data analysis and experiment
> We can filter data by custom filter
> We can filter data by font colour or cell colour

> Shortcut for selecting the filter: Alt+down arrow
> When doing any arithmatic operation with the filtered cells, we need to select the cells and then press ALT+;
> selecting entire column: Ctrl+space
> Protect sheet: Alt+T+P+P
> Sheet change: Ctrl+Pgdn
> Hiding entire row: Ctrl+9
> Unhide row: Ctrl + Shift + 9
> Hiding column: Ctrl+0
> Getting to cell A1: Ctrl+home
> To copy cell contents using drag and drop press the Ctrl key
",filter excel homedata tab 1 filter import function,"['filter', 'excel', 'homedata', 'tab', '1', 'filter', 'import', 'function']",filter in excel (home/data tab)  1. filter > the most important function for data analysis and exper
257,258,"Conditional Formatting 
 in excel
(Home tab)

2. Conditional Formatting 

> This is for automatic colour coding, bar coding or icon coding of the data 

> It helps in data analysis by  highlighting cells with certain rule like certain text or duplicate values",condit format excel home tab 2 condit format autom,"['condit', 'format', 'excel', 'home', 'tab', '2', 'condit', 'format', 'autom']",conditional formatting   in excel (home tab)  2. conditional formatting   > this is for automatic co
258,259,"Sorting in excel
(Home/ Data tab)

3. Sorting
> Like filter, sorting can also be done by both values and colour (text or cell)

> Sorting can also be done for multiple levels (go to add level option in the sort dialog box)

> Sorting may be done for rows means for column headings",sort excel home data tab 3 sort like filter sort a,"['sort', 'excel', 'home', 'data', 'tab', '3', 'sort', 'like', 'filter', 'sort', 'a']","sorting in excel (home/ data tab)  3. sorting > like filter, sorting can also be done by both values"
259,260,"Removing Duplicates in excel
(Data tab)

4. Remove Dups

> Directly removes duplicate observation with one column or multiple columns",remov duplic excel data tab 4 remov dup direct rem,"['remov', 'duplic', 'excel', 'data', 'tab', '4', 'remov', 'dup', 'direct', 'rem']",removing duplicates in excel (data tab)  4. remove dups  > directly removes duplicate observation wi
260,261,"Pivot and Slicers in excel
(Insert tab)

5. Pivot and Slicers
> Pivot is a way to summarize data

> It helps us to create a summary table playing with Row labels, Column labels, Filters and Summarizing Values

> We can copy-paste the  summary table multiple times and create different combinations of results

> By right click on values we can change the summarizing data type (sum,count,avg,max,min,etc.)

> For pivot table, excel shows option tab in which we can do more arrangement of the summary table

> We can show additional calculated column (option tab> formulas> calculated field) if required

> If we double click on the summarized value, excel shows the exact data for that summarization on different worksheet

> We can also do groupby or other operations from other tabs apart from pivot table option tab if required

Pivot Slicer

> In pivot table option tab, we have slicer (a type of filter which operates from outside the table). We can connect multiple pivot tables in a single slicer.

Slicers is not available in Excel 2007.",pivot slicer excel insert tab 5 pivot slicer pivot,"['pivot', 'slicer', 'excel', 'insert', 'tab', '5', 'pivot', 'slicer', 'pivot']",pivot and slicers in excel (insert tab)  5. pivot and slicers > pivot is a way to summarize data  > 
261,262,"Refreshing all pivot tables

How do we refresh data in all pivot tables or entire worksheet at a time

> Pivot table  options> Refresh all and pivot chart analyze> Refresh all",refresh pivot tabl refresh data pivot tabl entir w,"['refresh', 'pivot', 'tabl', 'refresh', 'data', 'pivot', 'tabl', 'entir', 'w']",refreshing all pivot tables  how do we refresh data in all pivot tables or entire worksheet at a tim
262,263,"Charts in excel
(Insert tab)

6. Charts
> After selecting the table, we can insert chart of our choice
> On getting the chart, we can select the chart and will get design, layout and format tabs for the chart
> In layout tab> analysis> Trendline
- we have More trendline options which allows us to adjust the trendline and see the equation of the trend line on the chart
>> For comparing values over categories Column Chart (vertical bar graph) is useful
>> To track the progress of the stock market on a daily basis or to track or compare performance of employees in a year, Line chart is useful",chart excel insert tab 6 chart select tabl insert ,"['chart', 'excel', 'insert', 'tab', '6', 'chart', 'select', 'tabl', 'insert']","charts in excel (insert tab)  6. charts > after selecting the table, we can insert chart of our choi"
263,264,"Dashboard in excel

> Dashboard is the panel facing the driver of a vehicle or the pilot of an aircraft, containing instruments and controls

> A data dashboard is an information management tool that visually tracks, analyzes and displays key performance indicators (KPI), metrics and key data points to monitor the health of a business, department or specific process

> Data dashboard is created with pivot table, slicer, additional tables and charts, title, legend

> At first we need to do all data crunching in python and create a clean csv or excel. 

> If management want the visualization (dashboard) in excel, then we do all  onward excel operations.

> If the management want the dashboard in Tableau (or Power BI), then we can connect the clean excel file in Tableau (or Power BI) for onward operations.",dashboard excel dashboard panel face driver vehicl,"['dashboard', 'excel', 'dashboard', 'panel', 'face', 'driver', 'vehicl']",dashboard in excel  > dashboard is the panel facing the driver of a vehicle or the pilot of an aircr
264,265,"Waterfall or birdge graph analysis

> This means, how things have changed between two periods(or two states) 

> Insert a stacked column chart and adjust the invisible colour",waterfal birdg graph analysi mean thing chang two ,"['waterfal', 'birdg', 'graph', 'analysi', 'mean', 'thing', 'chang', 'two']","waterfall or birdge graph analysis  > this means, how things have changed between two periods(or two"
265,266,"Excel functions

COUNT(B2:C8) function calculates the number of cells that have numeric entries

The COUNTA(H2:O16) Function is categorized under Excel Statistical functions. It will calculate the number of cells that have numeric or text entries (i.e not blank)

LEN(H2) returns the string lenth of the cell

IF(LEN(H2)>50,1,0) # returns 1 if string length is greater than 50 elase 0
INDEX(A1:B5,2,2) function returns the value at a given location in a range or array (table ) by index (here row index and column index starts from 1)-// This actually returns value in B2

MATCH(41,B2:B5,0) function returns the index after exact matching of the value with the given range (vertical or horizontal)

-MATCH function is similar like VLOOKUP or HLOOKUP but match function returns index and VLOOKUP or HLOOKUP returns value

=A1=B1 returns True or False

ROUND(235.415, -1) returns 240

ROUND(235.415, -2) ruturns 200

ROUND(235.415, 2) returns 235.42

ROUND(235.415, 0) or ROUNDDOWN(235.415, 0) returns 235

> Positive number (1,2) indicates digits after the decimal point and negative number (-1,-2) indicates digits before 0 point (unit place or one's place of the number)

0,1,2,-1,-2 etc indicates the rounding location in ROUND function",excel function countb2c8 function calcul number ce,"['excel', 'function', 'countb2c8', 'function', 'calcul', 'number', 'ce']",excel functions  count(b2:c8) function calculates the number of cells that have numeric entries  the
266,267,"Concatenation of text in excel

using Ampersand (&)

= A5&A6

or using CONCATENATE(A5,A6)

Function To insert current date and time
now()",concaten text excel use ampersand a5a6 use concate,"['concaten', 'text', 'excel', 'use', 'ampersand', 'a5a6', 'use', 'concate']","concatenation of text in excel  using ampersand (&)  = a5&a6  or using concatenate(a5,a6)  function "
267,268,"A numeric value can be treated as label value if Apostrophe (‘) precedes it.

Microsoft Excel uses the table function to calculate the results in the data table.",numer valu treat label valu apostroph ‘ preced mic,"['numer', 'valu', 'treat', 'label', 'valu', 'apostroph', '‘', 'preced', 'mic']",a numeric value can be treated as label value if apostrophe (‘) precedes it.  microsoft excel uses t
268,269,"Tableau basics

Tableau is a data visualization software and also a business intelligence (BI) software (may also be used for data crunching)

By definition, Tableau displays measures over time as a Line.

Alternative is Power BI (by microsoft)

Connect Tableau with static data ((excel, text etc.) or dynamic data (SQL server )

Like worksheets in excel, Tableau has sheets and Dashboard. In one sheet there can be one visualization. In Dashboard we can have multiple visualization.  

Tableau operation is similar like pivot chart operation in excel",tableau basic tableau data visual softwar also bus,"['tableau', 'basic', 'tableau', 'data', 'visual', 'softwar', 'also', 'bus']",tableau basics  tableau is a data visualization software and also a business intelligence (bi) softw
269,270,"Best practices for visualization

> Keep things simple and digestible (more important than beauty)
> There must be a meaningful association with colour
> In Bar graphs, size is more iterpretable than circular graphs
> Attention map for visualization: center is the most emphasized one, top left is the second emphasized part, bottom right is least emphasized one

In visualization we can play with following parameters

> Colour
>Size
> labels
> Tooltip
> Details",best practic visual keep thing simpl digest import,"['best', 'practic', 'visual', 'keep', 'thing', 'simpl', 'digest', 'import']",best practices for visualization  > keep things simple and digestible (more important than beauty) >
270,271,"Panes of Tableau

In Tableau, There are two panes in the left side:

1. Data
2. Analysis

under data pane there are:
i. Dimensions (column or variable names)
ii. Measures
iii. Calculated fields
iv. Sets (Sets are custom fields based on existing dimensions and criteria that we specify)
v. Parameters
>> Data pane fields are as per sql schema when connected to sql server

under analysis pane there are:
i. Summarize (constant line, avg. line, median and quartile)
ii. Model (Trend line, forecast)

> Maximum of 32 tables can be join in tableau",pane tableau tableau two pane left side 1 data 2 a,"['pane', 'tableau', 'tableau', 'two', 'pane', 'left', 'side', '1', 'data', '2', 'a']","panes of tableau  in tableau, there are two panes in the left side:  1. data 2. analysis  under data"
271,272,"Trend line models in Tableau

Tableau provides users with five trend line models: 

Linear, 
Logarithmic, 
Exponential, 
Polynomial and 
Power.",trend line model tableau tableau provid user five ,"['trend', 'line', 'model', 'tableau', 'tableau', 'provid', 'user', 'five']","trend line models in tableau  tableau provides users with five trend line models:   linear,  logarit"
272,273,"Creating Calculated field in Tableau

> Select “Analysis” present in the menu bar.
> Select “Create Calculated Field” from the list.
> Then calculated Field window will open. Name it.
> Type the estimated value of the measure.

For creating variable size bins, we use Calculated fields (In the Data pane, right-click a measure and select Create > Bins)",creat calcul field tableau select “analysis” prese,"['creat', 'calcul', 'field', 'tableau', 'select', '“analysis”', 'prese']",creating calculated field in tableau  > select “analysis” present in the menu bar. > select “create 
273,274,"Chart area of Tableau

> In Tableau, the chart area is divided into grid (helps to show multiple dimensions and multiple measures in a single chart). The columns of the grid shows the dimensions and the rows of the grid shows the measures. (can be reversed)

On dragging dimension to columns and measure to rows, Tableau automatically creates a bar graph (vertical).

Click on Show Me button (top right corner) to change the visualization


As an alternative to Dashboard, we can create tableau stories

We can drag and drop a variable in the Detail Mark or any of the marks, whenever detailing on visualization is required.

We can disable or enable the highlighting action for the workbook or sheets from the toolbar.

Disaggregation returns all records in the underlying data source.",chart area tableau tableau chart area divid grid h,"['chart', 'area', 'tableau', 'tableau', 'chart', 'area', 'divid', 'grid', 'h']","chart area of tableau  > in tableau, the chart area is divided into grid (helps to show multiple dim"
274,275,"Chart types in Tableau

1. Area Chart

2. Bar Chart

3. Box and Whisker Plots

4. Bullet Chart (like horizontal bar garph, also called projectile chart)-This is to compare multiple measures for a single category

5. Scatter Plot
6. Pie Chart
7. Bubble Chart
8. Line Chart

> The chart wizard term data series refers to a collection of chart data markers",chart type tableau 1 area chart 2 bar chart 3 box ,"['chart', 'type', 'tableau', '1', 'area', 'chart', '2', 'bar', 'chart', '3', 'box']",chart types in tableau  1. area chart  2. bar chart  3. box and whisker plots  4. bullet chart (like
275,276,"Tableau Pills

The fields drag and dropped in the columns and rows look like pills and so they are generally called pills

Pill colors

Most dimensions we use will be blue and most measures will be green. 

Blue pills however in fact correspond to discrete data and green pills to continuous data. If we see a red pill at any point, it means there is an error of some kind – a dimension/measure is missing or the calculation we did hasn't been recognised.

e.g. 

Discrete Dimensions: Product Name

Continuous Dimensions: Year(order date)

Discrete Measures: Sum(discrete profit)

Continuous Measures:  Sum(continuous profit)",tableau pill field drag drop column row look like ,"['tableau', 'pill', 'field', 'drag', 'drop', 'column', 'row', 'look', 'like']",tableau pills  the fields drag and dropped in the columns and rows look like pills and so they are g
276,277,"TO PUBLISH THE WORKBOOK IN THE SERVER TO BE USED BY OTHERS

The symbol related with the field that has been assembled is a 
Paper Clasp (An orange check mark indicates that the data source is the secondary data source in the workbook)

Cell Size Option In Format Menu is TO CUSTOMIZE THE SIZE OF THE CELLS DISPLAYING THE DATA",publish workbook server use other symbol relat fie,"['publish', 'workbook', 'server', 'use', 'other', 'symbol', 'relat', 'fie']",to publish the workbook in the server to be used by others  the symbol related with the field that h
277,278,"Tableau Desktop applications

Tableau Desktop. 
Tableau Prep
Tableau Server
 Tableau Online. 
Tableau Reader
Tableau Public
Tableau Viewer
Tableau Explorer",tableau desktop applic tableau desktop tableau pre,"['tableau', 'desktop', 'applic', 'tableau', 'desktop', 'tableau', 'pre']",tableau desktop applications  tableau desktop.  tableau prep tableau server  tableau online.  tablea
278,279,"Components of a Dashboard

1. Horizontal (bar graph)
2. Vertical (bar graph)
3. Image Extract",compon dashboard 1 horizont bar graph 2 vertic bar,"['compon', 'dashboard', '1', 'horizont', 'bar', 'graph', '2', 'vertic', 'bar']",components of a dashboard  1. horizontal (bar graph) 2. vertical (bar graph) 3. image extract
279,280,COUNTD function is used for displaying a distinct or unique value of the dimension. It will count the distinct value of the number of items in a group and will display it. It will ignore NULL values.,countd function use display distinct uniqu valu di,"['countd', 'function', 'use', 'display', 'distinct', 'uniqu', 'valu', 'di']",countd function is used for displaying a distinct or unique value of the dimension. it will count th
280,281,"Reference line and Reference band

Say we are analyzing the monthly sales for several products, we can include a reference line at the average sales mark so we can see how each product performed against the average.

A Reference Band can be based on two fixed points.",refer line refer band say analyz month sale sever ,"['refer', 'line', 'refer', 'band', 'say', 'analyz', 'month', 'sale', 'sever']","reference line and reference band  say we are analyzing the monthly sales for several products, we c"
281,282,"File extensions in Tableau

Tableau Workbook (.twb)
Tableau Packaged Workbook (.twbx)
Tableau Data Source(.tds)",file extens tableau tableau workbook twb tableau p,"['file', 'extens', 'tableau', 'tableau', 'workbook', 'twb', 'tableau', 'p']",file extensions in tableau  tableau workbook (.twb) tableau packaged workbook (.twbx) tableau data s
282,283,"Filters in Tableau

There are several different kinds of filters in Tableau and they get executed in the following order from top to bottom

1. Extract filters
2. Data Source filters
3. Context filters
4. Dimension filters
5. Measure filters
6. Table Calc filters",filter tableau sever differ kind filter tableau ge,"['filter', 'tableau', 'sever', 'differ', 'kind', 'filter', 'tableau', 'ge']",filters in tableau  there are several different kinds of filters in tableau and they get executed in
283,284,"Data blending 

Data blending is a method for combining data from multiple sources.

Blends, unlike relationships or joins, never truly combine the data. Instead, blends query each data source independently, the results are aggregated to the appropriate level, then the results are presented visually together in the view.

> We can perform all kinds of joins using data blending

> Blending uses Left join by default",data blend data blend method combin data multipl s,"['data', 'blend', 'data', 'blend', 'method', 'combin', 'data', 'multipl', 's']","data blending   data blending is a method for combining data from multiple sources.  blends, unlike "
284,285,"Data Types in Tableau

String
Numeric 
Date and Time
Boolean 
Geographic
Cluster or Mixed ",data type tableau string numer date time boolean g,"['data', 'type', 'tableau', 'string', 'numer', 'date', 'time', 'boolean', 'g']",data types in tableau  string numeric  date and time boolean  geographic cluster or mixed 
285,286,"
> Sum is the default aggregation used for the tree map

",sum default aggreg use tree map,"['sum', 'default', 'aggreg', 'use', 'tree', 'map']", > sum is the default aggregation used for the tree map  
286,287,"KPI Basics

Don’t just Measure. Measure what matters.

A popular saying in organizations is “what gets measured gets managed“. -by management guru Peter Drucker

KPI stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. 

Some Important KPIs

There is KPIs specific to marketing, development, and support etc.",kpi basic don't measur measur matter popular say o,"['kpi', 'basic', ""don't"", 'measur', 'measur', 'matter', 'popular', 'say', 'o']",kpi basics  don’t just measure. measure what matters.  a popular saying in organizations is “what ge
287,288,"KPI vs Metric

While key performance indicators and metrics are related, they’re not the same. 

Metrics are not the most critical measures. Some examples include “monthly store visits” or “white paper downloads”.

To build business intelligence, we should track both strategic key performance indicators and tactical metrics.  ",kpi vs metric key perform indic metric relat they',"['kpi', 'vs', 'metric', 'key', 'perform', 'indic', 'metric', 'relat', ""they'""]","kpi vs metric  while key performance indicators and metrics are related, they’re not the same.   met"
288,289,"Need of KPI for the company

1. we Can Measure our Targets
2. Create an Atmosphere of Learning
3. Receive Important Information
4. Encourage Accountability
5. Boost Morale
6. Keep our teams aligned
7. Provide a health check
8. Make adjustments
9. Hold our teams accountable",need kpi compani 1 measur target 2 creat atmospher,"['need', 'kpi', 'compani', '1', 'measur', 'target', '2', 'creat', 'atmospher']",need of kpi for the company  1. we can measure our targets 2. create an atmosphere of learning 3. re
289,290,"Types of Indicators

1. Quantitative indicators
2. Qualitative indicators
3. Leading indicators
4. Lagging indicators
5. Input indicators
6. Process indicators
7. Output indicators
8. Practical indicators
9. Directional indicators
10. Actionable indicators
11. Financial indicators",type indic 1 quantit indic 2 qualit indic 3 lead i,"['type', 'indic', '1', 'quantit', 'indic', '2', 'qualit', 'indic', '3', 'lead', 'i']",types of indicators  1. quantitative indicators 2. qualitative indicators 3. leading indicators 4. l
290,291,"Effectiveness and Efficiency

Effectiveness = atual result/ expected result

It is the comparison between actual output and expected output

> If we maximize the expectation equal to the input, then effectiveness increases and becomes equal to efficiency (e.g. we may expect to score 100 out of 100 in any exam)

Efficiency = expected cost (of product, output)/ actual cost (of resource consumed, input)

It is the comparison between  actual output and actual input",effect effici effect atual result expect result co,"['effect', 'effici', 'effect', 'atual', 'result', 'expect', 'result', 'co']",effectiveness and efficiency  effectiveness = atual result/ expected result  it is the comparison be
291,292,"Ways to develop KPI

1. Define how KPIs will be used
2. Tie them to strategic goals
3. Write SMART KPIs
4. Keep them clear-cut
5. Plan to iterate
6. Avoid KPI overload",way develop kpi 1 defin kpis use 2 tie strateg goa,"['way', 'develop', 'kpi', '1', 'defin', 'kpis', 'use', '2', 'tie', 'strateg', 'goa']",ways to develop kpi  1. define how kpis will be used 2. tie them to strategic goals 3. write smart k
292,293,"Three steps to a stronger KPI strategy

1. Select KPIs that matter most
2. Create a KPI-driven culture
3. Iterate
",three step stronger kpi strategi 1 select kpis mat,"['three', 'step', 'stronger', 'kpi', 'strategi', '1', 'select', 'kpis', 'mat']",three steps to a stronger kpi strategy  1. select kpis that matter most 2. create a kpi-driven cultu
293,294,"Key Performance Indicators in practice

1. Result indicators – what have we done?
2. Key result indicators – how well have we done?

3. Performance indicators – what shall be done?
4. Key Performance indicators – what needs to be done

> Performance Measures  is an indicator of the measurement of success in any organization

> On-Time To Promise is a KPI measure (the percentage of time we're successful at delivering the product when we promised our customers we would deliver it.)

> In Industrial performance, resource means Material resources, Human resources and Financial resources",key perform indic practic 1 result indic – done 2 ,"['key', 'perform', 'indic', 'practic', '1', 'result', 'indic', '–', 'done', '2']",key performance indicators in practice  1. result indicators – what have we done 2. key result indic
294,295,"Executive Dashboard 

An Executive Dashboard is a reporting tool that provides a visual display of organizational KPIs, metrics, and data. The objective of executive dashboards is to give CEOs an at-a-glance visibility into business performance across all units and projects.",execut dashboard execut dashboard report tool prov,"['execut', 'dashboard', 'execut', 'dashboard', 'report', 'tool', 'prov']",executive dashboard   an executive dashboard is a reporting tool that provides a visual display of o
295,297,"Possible dangers of industrial performance indicators

1. Do not balance all the goals
2. Perform unnecessary indicators
3. Difficulty of defining objectives by service",possibl danger industri perform indic 1 balanc goa,"['possibl', 'danger', 'industri', 'perform', 'indic', '1', 'balanc', 'goa']",possible dangers of industrial performance indicators  1. do not balance all the goals 2. perform un
296,298,"SQL Basics

SQL means Structured Query Language

> In csv file there can be only one worksheet means one table

> In excel file there can be multiple worksheet means multiple tables

> Similar like excel, in .db or .tar files (SQL) there are  multiple inter-related tables but used for very large amount of data

> Sql needs a server because it is dynamic database

> SQL statement is also called clause

> SQL is case insensitive",structur queri languag basic structur queri langua,"['structur', 'queri', 'languag', 'basic', 'structur', 'queri', 'langua']",structured query language basics  structured query language means structured query language  > in cs
297,299,"List of Relational database

-Oracle Database 12c

-MySQL (Oracle)

-Microsoft SQL Server

-PostgreSQL

-SQLite

-DB2 (owner IBM)

-SAP HANA (owner SAP)

-MongoDB (non relational database or NoSQL-semi structured data)

> many of them are open source",list relat databas oracl databas 12c mystructur qu,"['list', 'relat', 'databas', 'oracl', 'databas', '12c', 'mystructur', 'qu']",list of relational database  -oracle database 12c  -mystructured query language (oracle)  -microsoft
298,300,"Difference between SQL and Python

SQL is a query language primarily aimed to store, manipulate and retrieve data

Python is a general-purpose programming language that enables experimentation with the data.

But SQL is faster than Python, thus used for extreamly large data",differ structur queri languag python structur quer,"['differ', 'structur', 'queri', 'languag', 'python', 'structur', 'quer']",difference between structured query language and python  structured query language is a query langua
299,301,"Basics of Relational Database (RDBMS)

A relational database is a type of database that stores and provides access to data points that are related to one another (from inter-related tables).

 The relationship between tables and field types is called a schema. Schema is a logical collection of database objects

> The columns of the table hold attributes of the data, and each record usually has a value for each attribute.

> When the values of minimum one attribute of two tables are same, they are called relational tables or relational database 
e.g. 
Say two tables contains one same attribute types of job with same values (service,business), then the two tables are relational",basic relat databas rdbms relat databas type datab,"['basic', 'relat', 'databas', 'rdbms', 'relat', 'databas', 'type', 'datab']",basics of relational database (rdbms)  a relational database is a type of database that stores and p
300,302,"Basics of Non-Relational Database

A non-relational database is any database that does not use the tabular schema of rows and columns like in relational databases. Rather, its storage model is optimized for the type of data it’s storing

For extreamly large dataset non-relational databases (ms word is non-relational database) are used.

NoSQL databases

1. Document-oriented databases
2. Key-Value Stores
3. Wide-Column Stores
4. Graph Stores",basic nonrel databas nonrel databas databas use ta,"['basic', 'nonrel', 'databas', 'nonrel', 'databas', 'databas', 'use', 'ta']",basics of non-relational database  a non-relational database is any database that does not use the t
301,303,"Parts of SQL

1) DDL - Data Definition Language
2) DML - Data Manipulation Language
3) DCL-Data Control Language (View Definition)
4) TCL-Transaction Control Language 

> Group of operations that form a single logical unit of work is known as Transaction",part structur queri languag 1 ddl data definit lan,"['part', 'structur', 'queri', 'languag', '1', 'ddl', 'data', 'definit', 'lan']",parts of structured query language  1) ddl - data definition language 2) dml - data manipulation lan
302,304,"SQL Statements

DDL statements are used to define the database structure or schema. 

1) CREATE
2) ALTER
3) DROP
4) RENAME
5) TRUNCATE

DML statements are used for managing data within schema objects. 
1) SELECT
2) INSERT
3) UPDATE
4) DELETE
5) LOCK
6) CALL
7) EXPLAIN PLAN

DCL statements

1. GRANT ( to allow specified users to perform specified tasks)
2. REVOKE (to remove the user accessibility to database object)

TCL statements

1. COMMIT
2. ROLLBACK",structur queri languag statement ddl statement use,"['structur', 'queri', 'languag', 'statement', 'ddl', 'statement', 'use']",structured query language statements  ddl statements are used to define the database structure or sc
303,305," Practicing sql queries

> In PgAdmin schema, we can find all the table names and column names for each table. 

> But while practicing sql queries in colab, we need to see all the table names and column names of each table to understand the schema of the database

import sqlite3

connnection_engine=sqlite3.connect('database_path')

my_cursor = connnection_engine.cursor()",practic structur queri languag queri pgadmin schem,"['practic', 'structur', 'queri', 'languag', 'queri', 'pgadmin', 'schem']"," practicing structured query language queries  > in pgadmin schema, we can find all the table names "
304,306,">  To view all the table names

my_cursor.execute(""SELECT name FROM sqlite_master WHERE type='table';"")

my_cursor.fetchall()
",view tabl name mycursorexecuteselect name sqlitema,"['view', 'tabl', 'name', 'mycursorexecuteselect', 'name', 'sqlitema']",">  to view all the table names  my_cursor.execute(""select name from sqlite_master where type=table;"""
305,307,">  To view all the column names along with datatype of a table

my_cursor.execute(""PRAGMA table_info(table_name); "")

info_list=my_cursor.fetchall()

columns=[k[1] for k in info_list]",view column name along datatyp tabl mycursorexecut,"['view', 'column', 'name', 'along', 'datatyp', 'tabl', 'mycursorexecut']",">  to view all the column names along with datatype of a table  my_cursor.execute(""pragma table_info"
306,308,"> To check the shape of the table

SELECT COUNT(*)
FROM table_name;

Returns the number of rows that match a specific condition of a query
",check shape tabl select count tablenam return numb,"['check', 'shape', 'tabl', 'select', 'count', 'tablenam', 'return', 'numb']",> to check the shape of the table  select count(*) from table_name;  returns the number of rows that
307,309,"> To see the head of the table

SELECT * FROM table_name LIMIT 5;",see head tabl select tablenam limit 5,"['see', 'head', 'tabl', 'select', 'tablenam', 'limit', '5']",> to see the head of the table  select * from table_name limit 5;
308,310,"SQL Query/ Statements/ commands

1. Structural commands: SELECT, DISTINCT, ALL,  AS, FROM,
WHERE

SELECT *
FROM table_name
WHERE column_name=5;

2. Comparison commands: IN, BETWEEN, LIKE, ILIKE

3. Grouping commands (aggregate functions): GROUP BY, HAVING, COUNT(),
SUM(), AVG(), MAX(), MIN(), ROUND()

SELECT COUNT(DISTINCT column_name) FROM table_name;

4. Display commands: ORDER BY, ASC/ DESC, LIMIT
""SELECT column_name FROM table_name ORDER BY column_name DESC LIMIT 10; (default for ORDER BY is ASC)

5. Logical Commands: AND, OR, NOT
6. Output commands: INTO TABLE/ CURSOR, TO SCREEN
7. Union Commands: UNION

>> HAVING and WHERE are similar operation. “WHERE” is used to filter rows but “HAVING” is used to filter groups

>> WHERE” is always used before “GROUP BY” and HAVING after “GROUP BY”",structur queri languag queri statement command 1 s,"['structur', 'queri', 'languag', 'queri', 'statement', 'command', '1', 's']","structured query language query/ statements/ commands  1. structural commands: select, distinct, all"
309,311,"SELECT and SELECT DISTINT statements

In order to select the entire table SELECT * is used

SELECT DISTINCT column_name1, column_name2 FROM table_name;

-returns only the unique values from the columns
",select select distint statement order select entir,"['select', 'select', 'distint', 'statement', 'order', 'select', 'entir']",select and select distint statements  in order to select the entire table select * is used  select d
310,312,"Conditional operators

=
<
>
<=
>=
!=

SELECT city, temperature, condition FROM weather WHERE condition = 'sunny' OR condition = 'cloudy' AND temperature >= 60",condit oper select citi temperatur condit weather ,"['condit', 'oper', 'select', 'citi', 'temperatur', 'condit', 'weather']","conditional operators  = < > <= >= !=  select city, temperature, condition from weather where condit"
311,313,"Reading SQLite Database file (.sqlite3, .sqlite, .db) as pandas df 

my_df = pd.read_sql_query(""select * from table_name;"", connnection_engine)",read sqlite databas file sqlite3 sqlite db panda d,"['read', 'sqlite', 'databas', 'file', 'sqlite3', 'sqlite', 'db', 'panda', 'd']","reading sqlite database file (.sqlite3, .sqlite, .db) as pandas df   my_df = pd.read_sql_query(""sele"
312,314,"SQLite

SQLite is very lightweight and comes as already installed dbms server in colab and requires no username and password. 

Thus, SQLite is very useful for practicing SQL queries.

We can convert sql dadabase file to .sqlite file (rebasedata.com)",structur queri languageit structur queri languagei,"['structur', 'queri', 'languageit', 'structur', 'queri', 'languagei']",structured query languageite  structured query languageite is very lightweight and comes as already 
313,315,"File extensions

> Common database file extensions: .tar, .db, .accdb, .nsf, and .fp7

> Postgresql, mysql, mssql and other database file extension: .tar

.db-
Mobile Device Database File

.accdb-
Access 2007 Database File (mainly used for offline applications)

",file extens common databas file extens tar db accd,"['file', 'extens', 'common', 'databas', 'file', 'extens', 'tar', 'db', 'accd']","file extensions  > common database file extensions: .tar, .db, .accdb, .nsf, and .fp7  > postgresql,"
314,316,"LIKE and ILIKE allows us to perform pattern matching against string data with the use of wildcard characters (%, _)

percentage(%)- matches any sequence of characters

underscore(_)- matches any group of characters

LIKE is case-sensitive while ILIKE is case-insensitive

> Wildcard characters are useful When an exact match is not possible in a SELECT statement

WHERE column_name LIKE A% # returns names that begins with A

WHERE column_name LIKE %a # returns names that ends with a

WHERE column_name LIKE 'any_string_' # returns names with any_string

> both the wildcard characters can also be used in combination",like ilik allow us perform pattern match string da,"['like', 'ilik', 'allow', 'us', 'perform', 'pattern', 'match', 'string', 'da']",like and ilike allows us to perform pattern matching against string data with the use of wildcard ch
315,317,"value IN(option1, option2,..etc.)",valu inoption1 option2etc,"['valu', 'inoption1', 'option2etc']","value in(option1, option2,..etc.)"
316,318,"ADVANCED SQL COMMANDS

1)TIMESTAMP & EXTRACT
2)Math Functions
3)String Functions
4)Sub-query
5)Self Joins
",advanc structur queri languag command 1timestamp e,"['advanc', 'structur', 'queri', 'languag', 'command', '1timestamp', 'e']",advanced structured query language commands  1)timestamp & extract 2)math functions 3)string functio
317,319,"TIMESTAMPS 

1)TIME : Contains only Time
2)DATE : Contains only Date
3)TIMESTAMP : Contains Time and Date
4)TIMESTAMPTZ : Contains Time, Date and Time Zone
",timestamp 1time contain time 2date contain date 3t,"['timestamp', '1time', 'contain', 'time', '2date', 'contain', 'date', '3t']",timestamps   1)time : contains only time 2)date : contains only date 3)timestamp : contains time and
318,320,"Extracts YEAR/ MONTH/ DAY/ WEEK/ QUARTER

EXTRACT : Extracts YEAR/ MONTH/ DAY/ WEEK/ QUARTER from a date_column

AGE : Calculates and returns the current age given a timestamp

TO_CHAR : Converts date types to text and is useful for formatting",extract year month day week quarter extract extrac,"['extract', 'year', 'month', 'day', 'week', 'quarter', 'extract', 'extrac']",extracts year/ month/ day/ week/ quarter  extract : extracts year/ month/ day/ week/ quarter from a 
319,321,"SUB-QUERY 

(Allows we to construct complex queries, essentially performing a query on the results of another query)
>> This is one of the basic approaches for joining tables. Other approaches are Union Join and  Natural join
>> When we have a subquery inside of the main query, subquery  is executed first
SELF JOIN
(Query in which a table is joined to itself. They are useful for comparing values in a column within the same table)

> EQUI JOIN is also called an INNER JOIN",subqueri allow construct complex queri essenti per,"['subqueri', 'allow', 'construct', 'complex', 'queri', 'essenti', 'per']","sub-query   (allows we to construct complex queries, essentially performing a query on the results o"
320,322,"DATA/ DATA STRUCTURE TYPES IN SQL

1)Boolean : True or False
2)Character : Char, Varchar or Text
3)Numeric : Integer or Floating Point Numbers
4)Temporary :  Date, Time, Timestamp and Interval
5)UUID : Universally Unique Identifier
6)Array : Stores an array of strings or numbers
7)JSON (array like data structure)
8)Hstore : Key Value pair
9)Special : Geometrical Data or Network Address",data data structur type structur queri languag 1bo,"['data', 'data', 'structur', 'type', 'structur', 'queri', 'languag', '1bo']",data/ data structure types in structured query language  1)boolean : true or false 2)character : cha
321,323,"Primary Key

1)Primary Key : Column or Group of Columns that are used to uniquely identify a row in a table. They allow us to easily discern what columns are to be used when joining tables

> We can have only one primary key in a table ",primari key 1primari key column group column use u,"['primari', 'key', '1primari', 'key', 'column', 'group', 'column', 'use', 'u']",primary key  1)primary key : column or group of columns that are used to uniquely identify a row in 
322,324,"

Foreign Key

2)Foreign Key: column or group of columns that refers to the primary key/unique key of other table.
i)  Parent Table : Table to which FK references
ii) Child Table : Table that contains the FK

> It is used to establish and enforce a link between the data in two tables",foreign key 2foreign key column group column refer,"['foreign', 'key', '2foreign', 'key', 'column', 'group', 'column', 'refer']",  foreign key  2)foreign key: column or group of columns that refers to the primary key/unique key o
323,325,"CONSTRAINTS IN SQL

The rules enforced on data columns in tables and are used to prevent invalid data being entered in a table. Constraints can be divided into 2 types : COLUMN CONSTRAINTS and TABLE CONSTRAINTS

1)NOT NULL (enforces a column to NOT accept NULL values)

2)UNIQUE (ensures that all values in a column are different)
3)PRIMARY KEY (automatically has a UNIQUE constraint and cannot contain NULL values)
4)FOREIGN KEY (to prevent actions that would destroy links between tables)
5)CHECK

Example of CONSTRAINTS

CREATE TABLE Orders (
    OrderID int NOT NULL,
    OrderNumber int NOT NULL,
    PersonID int,
    PRIMARY KEY (OrderID),
    FOREIGN KEY (PersonID) REFERENCES Persons(PersonID)
);
",constraint structur queri languag rule enforc data,"['constraint', 'structur', 'queri', 'languag', 'rule', 'enforc', 'data']",constraints in structured query language  the rules enforced on data columns in tables and are used 
324,326,"CREATING DATABASES & TABLES

1)CREATE
2)INSERT
3)UPDATE
4)DELETE
5)ALTER 
6)DROP
7)CHECK (CONSTRAINTS)
",creat databas tabl 1creat 2insert 3updat 4delet 5a,"['creat', 'databas', 'tabl', '1creat', '2insert', '3updat', '4delet', '5a']",creating databases & tables  1)create 2)insert 3)update 4)delete 5)alter  6)drop 7)check (constraint
325,327,"CONDITIONAL EXPRESSIONS & PROCEDURES

1. CASE-Very similar to IF/ ELSE

GENERAL CASE : Allows us to do all kinds of conditional checks

CASE EXPRESSION : Allows us to do only certain checks

2. COALESCE

Accepts unlimited  number of arguments and returns the first argument which is not NULL

COALESCE( arg_1, arg_2......arg_n)

-substituting the NULL values with a variable

SELECT (PRICE - COALESCE(DISCOUNT,0))  AS FINAL_PRICE FROM table

3. CAST
Let’s we convert one Data Type into another.

4. NULLIF
This returns a NULL Value if the arguments inside NULLIF() are equal.

5. VIEWS
View can be accessed as a Virtual Table and it does not store the data physically. It simply stores the query. ",condit express procedur 1 caseveri similar els gen,"['condit', 'express', 'procedur', '1', 'caseveri', 'similar', 'els', 'gen']",conditional expressions & procedures  1. case-very similar to if/ else  general case : allows us to 
326,328,"
Simple View and Complex View

>> There are 2 types of Views in SQL: Simple View and Complex View. Simple views can only contain a single base table. Complex views can be constructed on more than one base table",simpl view complex view 2 type view sql simpl view,"['simpl', 'view', 'complex', 'view', '2', 'type', 'view', 'sql', 'simpl', 'view']", simple view and complex view  >> there are 2 types of views in sql: simple view and complex view. s
327,329,"Syntax for the CREATE VIEW

CREATE VIEW view_name AS SELECT columns FROM tables [WHERE conditions];

",syntax creat view creat view viewnam select column,"['syntax', 'creat', 'view', 'creat', 'view', 'viewnam', 'select', 'column']",syntax for the create view  create view view_name as select columns from tables [where conditions]; 
328,330,"SQL queries in Python

Step 1: Importing SQLAlchemy, psycopg2 etc.

from sqlalchemy import create_engine

Step 2: Creating a SQL engine

Step 3: Running queries using SQL statements

Step 4: Writing to DB

Step 5: Creating a Table in DB",structur queri languag queri python step 1 import ,"['structur', 'queri', 'languag', 'queri', 'python', 'step', '1', 'import']","structured query language queries in python  step 1: importing structured query languagealchemy, psy"
329,331,"> First install postgreesql and then pgadmin (needs windows 64 bit)

set Port: 5432
set password:  prabir
user: postgres (default)
> Open pgadmin

> load the .tar database

> Then we can see different tables names under schemas

> Now we can write queries to check all the tables 
",first instal postgreesql pgadmin need window 64 bi,"['first', 'instal', 'postgreesql', 'pgadmin', 'need', 'window', '64', 'bi']",> first install postgreesql and then pgadmin (needs windows 64 bit)  set port: 5432 set password:  p
330,332,"Ways to see the files inside a .tar or .tar.gz file (zip file) without uncompressing by winzip

import tarfile

tar = tarfile.open('.tar file path')

file_names=tar.getnames() # most of the files are actually different relational tables in .dat format (.dat files can only be accessed by the application that created them)",way see file insid tar targz file zip file without,"['way', 'see', 'file', 'insid', 'tar', 'targz', 'file', 'zip', 'file', 'without']",ways to see the files inside a .tar or .tar.gz file (zip file) without uncompressing by winzip  impo
331,333,"SQL aliases are used to give a table, or a column in a table, a temporary name. Aliases are often used to make column names more readable. 

Syntax

SELECT column_name AS alias_name
FROM table_name;

Example

SELECT CustomerID AS ID, CustomerName AS Customer
FROM Customers;",structur queri languag alias use give tabl column ,"['structur', 'queri', 'languag', 'alias', 'use', 'give', 'tabl', 'column']","structured query language aliases are used to give a table, or a column in a table, a temporary name"
332,334,"Different Types of SQL JOINs

(INNER) JOIN: Returns records that have matching values in both tables
LEFT (OUTER) JOIN: Returns all records from the left table, and the matched records from the right table
RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched records from the left table
FULL (OUTER) JOIN: Returns all records when there is a match in either left or right table
e.g.
SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate
FROM Orders
INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID;",differ type structur queri languag join inner join,"['differ', 'type', 'structur', 'queri', 'languag', 'join', 'inner', 'join']",different types of structured query language joins  (inner) join: returns records that have matching
333,335,"CREATE DATABASE in SQL

CREATE DATABASE databasename;

DROP DATABASE Syntax

DROP DATABASE databasename;

BACKUP DATABASE Syntax

BACKUP DATABASE databasename
TO DISK = 'filepath';",creat databas structur queri languag creat databas,"['creat', 'databas', 'structur', 'queri', 'languag', 'creat', 'databas']",create database in structured query language  create database databasename;  drop database syntax  d
334,336,"CREATE TABLE in SQL

CREATE TABLE table_name (
    column1 datatype,
    column2 datatype,
    column3 datatype,
   ....
);

Example

CREATE TABLE Persons (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);",creat tabl structur queri languag creat tabl table,"['creat', 'tabl', 'structur', 'queri', 'languag', 'creat', 'tabl', 'table']","create table in structured query language  create table table_name (     column1 datatype,     colum"
335,337,"DELETE in SQL

DELETE FROM table_name WHERE condition;

Example

DELETE FROM Customers WHERE CustomerName='Alfreds Futterkiste';",delet structur queri languag delet tablenam condit,"['delet', 'structur', 'queri', 'languag', 'delet', 'tablenam', 'condit']",delete in structured query language  delete from table_name where condition;  example  delete from c
336,338,"SQL TRUNCATE TABLE 

In SQL TRUNCATE TABLE statement always first drops, then re-creates a new table.  It removes all rows from a table without logging the individual row deletions

DROP TABLE command deletes complete table but it would remove complete table structure form the database and we would need to re-create this table once again if we wish we store some data.

> DELETE command can be rolled back but TRUNCATE and DROP command cannot be rolled back

> TRUNCATE is usually faster than DELETE command",structur queri languag truncat tabl structur queri,"['structur', 'queri', 'languag', 'truncat', 'tabl', 'structur', 'queri']",structured query language truncate table   in structured query language truncate table statement alw
337,339,"SQL ALTER TABLE 

The SQL ALTER TABLE clause modifies a table definition by altering, adding, or deleting table columns and/or constraints

1. To add a column in a table
ALTER TABLE table_name
ADD column_name datatype;

2. To delete a column in a table
ALTER TABLE table_name
DROP COLUMN column_name;

3. To change the data type of a column in a table
ALTER TABLE table_name
ALTER COLUMN column_name datatype;

> In place of ALTER COLUMN, MODIFY may be used",structur queri languag alter tabl structur queri l,"['structur', 'queri', 'languag', 'alter', 'tabl', 'structur', 'queri', 'l']",structured query language alter table   the structured query language alter table clause modifies a 
338,340,"SQL string datatypes

CHAR , 
VARCHAR , 
BINARY , 
VARBINARY , 
BLOB , 
TEXT , 
ENUM , and 
SET",structur queri languag string datatyp char varchar,"['structur', 'queri', 'languag', 'string', 'datatyp', 'char', 'varchar']","structured query language string datatypes  char ,  varchar ,  binary ,  varbinary ,  blob ,  text ,"
339,341,"INSERT INTO in SQL

INSERT INTO table_name (column1, column2, column3, ...)
VALUES (value1, value2, value3, ...);

INSERT INTO table_name
VALUES (value1, value2, value3, ...); # When inserting values for all the columns",insert structur queri languag insert tablenam colu,"['insert', 'structur', 'queri', 'languag', 'insert', 'tablenam', 'colu']","insert into in structured query language  insert into table_name (column1, column2, column3, ...) va"
340,342,"UPDATE in SQL

UPDATE table_name
SET column1 = value1, column2 = value2, ...
WHERE condition;

Example

UPDATE Customers
SET ContactName = 'Alfred Schmidt', City= 'Frankfurt'
WHERE CustomerID = 1;

> we can UPDATE a single table at a time
",updat structur queri languag updat tablenam set co,"['updat', 'structur', 'queri', 'languag', 'updat', 'tablenam', 'set', 'co']","update in structured query language  update table_name set column1 = value1, column2 = value2, ... w"
341,343,"SQL CHECK 

SQL CHECK can be used along with CREATE, ALTER, DROP commands

CHECK on CREATE TABLE

CREATE TABLE Persons (
    ID int NOT NULL,
    LastName varchar(255) NOT NULL,
    FirstName varchar(255),
    Age int CHECK (Age>=18)
);

CHECK on ALTER TABLE

ALTER TABLE Persons
ADD CHECK (Age>=18);
",structur queri languag check structur queri langua,"['structur', 'queri', 'languag', 'check', 'structur', 'queri', 'langua']","structured query language check   structured query language check can be used along with create, alt"
342,344,"UNION in SQL

SELECT column_name(s) FROM table1
UNION
SELECT column_name(s) FROM table2;

UNION ALL

SELECT column_name(s) FROM table1
UNION ALL
SELECT column_name(s) FROM table2;

> SELECT on a MERGE table is like UNION ALL ",union structur queri languag select columnnam tabl,"['union', 'structur', 'queri', 'languag', 'select', 'columnnam', 'tabl']",union in structured query language  select column_name(s) from table1 union select column_name(s) fr
343,345,"SQL TIMESTAMP and DATETIME 

SQL TIMESTAMP and DATETIME both store without time zone.

TIMESTAMP is stored in UTC (Coordinated Universal Time) values, and DATETIME is stored in without time zone.

UTC is effectively a successor to Greenwich Mean Time (GMT).

Both the data type can be used for values that contain both date and time parts.

In ' YYYY-MM-DD hh:mm:ss ' format, the supported range DATETIME is '1000-01-01 00:00:00' to '9999-12-31 23:59:59' . 

 TIMESTAMP has a range of '1970-01-01 00:00:01' UTC to '2038-01-19 03:14:07' UTC.",structur queri languag timestamp datetim structur ,"['structur', 'queri', 'languag', 'timestamp', 'datetim', 'structur']",structured query language timestamp and datetime   structured query language timestamp and datetime 
344,346,"SQL NOT IN

NOT IN operator can be used with a multiple-row subquery

SHOW TRIGGERS; returns a list of triggers in the current database

A SQL trigger is a database object which fires when an event occurs in a database. It consist of procedural code (includes procedural and SQL statements)

For example, a trigger can be set on a record insert in a database table.",structur queri languag oper use multiplerow subque,"['structur', 'queri', 'languag', 'oper', 'use', 'multiplerow', 'subque']",structured query language not in  not in operator can be used with a multiple-row subquery  show tri
345,347,"Operation with Null values in SQL

Null +1 = Null
Null * 2 = Null

> IS NULL operator is used to compare the NULL values in SQL",oper null valu structur queri languag null 1 null ,"['oper', 'null', 'valu', 'structur', 'queri', 'languag', 'null', '1', 'null']",operation with null values in structured query language  null +1 = null null * 2 = null  > is null o
346,348,"SQL CONCAT

In SQL, the CONCAT() takes 2 to 255 input strings and joins them into one

SELECT CONCAT('W3Schools', '.com');

To convert 212-555-1212 to ***-555-1212 where 212-555-1212 is a data from the column named ""phone""

concat(left(phone,3),'***')

MERGE()  is the combination of three INSERT, DELETE and UPDATE statements 

In MERGE, we specify a ""Source"" record set and a ""Target"" table and the JOIN condition between the two.",structur queri languag concat structur queri langu,"['structur', 'queri', 'languag', 'concat', 'structur', 'queri', 'langu']","structured query language concat  in structured query language, the concat() takes 2 to 255 input st"
347,349,"SQL and String

String  index starts from 1 in SQL

INSTR ('ALMAX POINT', 'P') returns 7 as the index of 'P' is 7 in the given string

LEFT function allows we to extract a substring from a string, starting from the left-most character.

Syntax

LEFT(string_value , number_of_characters_to_be_returned)

> MOD() function in MySQL is used to find the remainder of one number divided by another.",structur queri languag string string index start 1,"['structur', 'queri', 'languag', 'string', 'string', 'index', 'start', '1']",structured query language and string  string  index starts from 1 in structured query language  inst
348,350,"Temporary table and Heap table

In SQL, a temporary table is a base table that is not stored in the database, but instead exists only while the database session in which it was created is active.

Heap tables are tables without a Clustered Index. In a heap table, the data is not sorted in any way, it's just a pile of unordered, unstructured records",temporari tabl heap tabl sql temporari tabl base t,"['temporari', 'tabl', 'heap', 'tabl', 'sql', 'temporari', 'tabl', 'base', 't']","temporary table and heap table  in sql, a temporary table is a base table that is not stored in the "
349,351,"Sequence in SQL

A sequence in SQL can generate a maximum 38 digits number

>  Sequence can generate Numeric value or Alphanumeric value",sequenc structur queri languag sequenc structur qu,"['sequenc', 'structur', 'queri', 'languag', 'sequenc', 'structur', 'qu']",sequence in structured query language  a sequence in structured query language can generate a maximu
350,352,"In SQL When we need an exact copy of a table with all columns and indexes

SHOW CREATE TABLE tablename

",structur queri languag need exact copi tabl column,"['structur', 'queri', 'languag', 'need', 'exact', 'copi', 'tabl', 'column']",in structured query language when we need an exact copy of a table with all columns and indexes  sho
351,353,"Primary Key, Super Key, and Candidate Key

I. Minimal super key is a candidate key
II. Only one candidate key can be a primary key

Say, we have a column ‘A’ in table1 which is a primary key and it refers to column ‘B’ in table2. Further, column ‘A’ has only three values (1,2,3). Then

> Inserting a value 4 in column ‘B’ of table2 will result in an error
> Inserting a value 4 in column ‘A’ of table1 will be successful",primari key super key candid key minim super key c,"['primari', 'key', 'super', 'key', 'candid', 'key', 'minim', 'super', 'key', 'c']","primary key, super key, and candidate key  i. minimal super key is a candidate key ii. only one cand"
352,354,"Main branches of pure mathematics

1. Arithmetic (It deals with numbers and the basic operations +-*/)

2. Algebra(: It is a kind of arithmetic where we use unknown quantities along with numbers)

3. Geometry (It is the most practical branch of mathematics that deals with shapes and sizes of figures and their properties)

4. Trigonometry(it is the study of relationships between angles and sides of triangles)

>> Base, perpendicular and hypotenuse of a right triangle

5. Calculus(It is the branch that deals with the study of the rate of change in different quantities. Calculus forms the base of analysis)

6. Statistics and Probability (Probability is all about chance of any future event. Whereas statistics is all about understanding a data or dataset based on different measurements or scores)


",main branch pure mathemat 1 arithmet deal number b,"['main', 'branch', 'pure', 'mathemat', '1', 'arithmet', 'deal', 'number', 'b']",main branches of pure mathematics  1. arithmetic (it deals with numbers and the basic operations +-*
353,355,"Application of mathematics in machine learning

""What gets measured gets managed""

>> From the knowledge of arithmetic, we measure different feature values of different observations of any experience and store in a dataset or table
>> From the knowledge of statistics, we understand the dataset through different scores of the features
>> From the knowledge of probability, we understand the dataset through the probability distributions of  features for its ability of future prediction
>> From the knowledge of algebra, we form different equations and find the values of the dependent variable
>> From the knowledge of calculus, we analyse the dataset with respect to different feature variables. It is also used (GD) to find values of the dependent variable for large dataset
>> From the knowledge of geometry and trigonometry, we understand the dataset and calculations through visualization
>>  Then form the knowledge of coding, we feed the dataset in a data based a soft machine 
",applic mathemat machin learn get measur get manag ,"['applic', 'mathemat', 'machin', 'learn', 'get', 'measur', 'get', 'manag']","application of mathematics in machine learning  ""what gets measured gets managed""  >> from the knowl"
354,356,"Basics of Calculus

> Calculus is always done in radian measure. Radians make it possible to relate a linear measure and an angle measure.

> Radian is a unit of measurement of angles equal to about 57.3° and 
π (3.14) radians = 180°

> π is the is the ratio of the circumference of any circle to the diameter of that circle and the value of π is 3.14",basic calculus calculus alway done radian measur r,"['basic', 'calculus', 'calculus', 'alway', 'done', 'radian', 'measur', 'r']",basics of calculus  > calculus is always done in radian measure. radians make it possible to relate 
355,357,"Machine Learning Use Cases of Calculus

1. Numerical Optimization
2. Gradient Computations
3. Probability Density Functions
4. Variational Inference and Related Techniques ",machin learn use case calculus 1 numer optim 2 gra,"['machin', 'learn', 'use', 'case', 'calculus', '1', 'numer', 'optim', '2', 'gra']",machine learning use cases of calculus  1. numerical optimization 2. gradient computations 3. probab
356,358,"Basics of derivative 

1. Instantaneous rate of change (Physics)
2. Slope of a line at a specific point (Geometry)

>> If a function is differentiable, it must it be continuous

Derivative of y = f(x) with respect to x is represented as dy/dx or f’(x)

>> A function is differentiable at a < c < b if and only if the left hand derivatives (LHD-slope calculated at the left side of the point)  and right hand derivatives (RHD- slope calculated at the right side of the point) at c both exist and are equal",basic deriv 1 instantan rate chang physic 2 slope ,"['basic', 'deriv', '1', 'instantan', 'rate', 'chang', 'physic', '2', 'slope']",basics of derivative   1. instantaneous rate of change (physics) 2. slope of a line at a specific po
357,359,"The chain rule

The chain rule is a formula for calculating the derivatives of composite functions (functions composed of functions inside another function(s))

f(x) = h(g(x))
df/dx  = df/dg*dg/dx",chain rule chain rule formula calcul deriv composi,"['chain', 'rule', 'chain', 'rule', 'formula', 'calcul', 'deriv', 'composi']",the chain rule  the chain rule is a formula for calculating the derivatives of composite functions (
358,360,"Point of maxima and point of minima

In a smoothly changing function, a maximum or minimum is always where the function flattens out (except for a saddle point). Where the slope is zero.
f'(x)=0
=>x= x0 , x1

The second derivative will tell us!
If f’’(x0) < 0 => x0 is point of maxima
If f’’(x0) > 0 => x0 is point of minima",point maxima point minima smooth chang function ma,"['point', 'maxima', 'point', 'minima', 'smooth', 'chang', 'function', 'ma']","point of maxima and point of minima  in a smoothly changing function, a maximum or minimum is always"
359,361,"Partial Derivatives

Let’s take an example: 
z = f(x, y)

If we change x, but hold all other variables constant, how does f(x, y) change? That’s one partial derivative. 

The next variable is y. If we change y but hold x constant, how does f(x, y) change?",partial deriv let take exampl z fx chang x hold va,"['partial', 'deriv', 'let', 'take', 'exampl', 'z', 'fx', 'chang', 'x', 'hold', 'va']","partial derivatives  let’s take an example:  z = f(x, y)  if we change x, but hold all other variabl"
360,362,"Ways to find the slope of f(x,y)

slope of f(x) is a line but slope of z=f(x,y) will be a plane

slope of a plane is represented by a vector 

<dz/dx, dz/dy>",way find slope fxi slope fx line slope zfxi plane ,"['way', 'find', 'slope', 'fxi', 'slope', 'fx', 'line', 'slope', 'zfxi', 'plane']","ways to find the slope of f(x,y)  slope of f(x) is a line but slope of z=f(x,y) will be a plane  slo"
361,363,"Jacobian Matrix

Jacobian matrices are used to transform the infinitesimal vectors from one coordinate system to another. We will mostly be interested in the Jacobian matrices that allow transformation from the Cartesian to a different coordinate system.

The determinant of the jacobian matrix is called jacobian",jacobian matrix jacobian matric use transform infi,"['jacobian', 'matrix', 'jacobian', 'matric', 'use', 'transform', 'infi']",jacobian matrix  jacobian matrices are used to transform the infinitesimal vectors from one coordina
362,364,"Derivatives Formulas 

f(x)= x^n, 
f’(x)= nx^(n-1)

f(x)= e^g(x)
f’(x)= e^g(x)* g'(x)

f(x)= m(x)*g(x)
f’(x)= m(x)*g'(x)+ m'(x)*g(x)

f(x)=  ln(x)
f’(x)= 1/x

f(x)=  ln(g(x))
f’(x)= 1/g(x)*g'(x)
f(x)=  x^x
f’(x)= x^x*(1+ln(x))
f(x)= sin(x)
f'(x) = cos(x)
f(x)= tan(x)
f'(x) = sec2(x)
f(x)= cot(x)=1/tan(x)
f'(x) = -cosec2(x)",deriv formula fx xn f'x nxn1 fx egx f'x egx gx fx ,"['deriv', 'formula', 'fx', 'xn', ""f'x"", 'nxn1', 'fx', 'egx', ""f'x"", 'egx', 'gx', 'fx']","derivatives formulas   f(x)= x^n,  f’(x)= nx^(n-1)  f(x)= e^g(x) f’(x)= e^g(x)* g(x)  f(x)= m(x)*g(x"
363,365,"Definite Integrals

The integral of f(x) corresponds to the computation of the area under the graph of f(x).",definit integr integr fx correspond comput area gr,"['definit', 'integr', 'integr', 'fx', 'correspond', 'comput', 'area', 'gr']",definite integrals  the integral of f(x) corresponds to the computation of the area under the graph 
364,366,"Formulas for integration

∫x^n dx =x^(n+1)/(n+1) + C

∫e^g(x)dx = e^g(x)*1/g'(x) + C

∫m(x)*g(x) dx = m(x)*∫g(x)dx -∫g(x)*m'(x)dx + C

∫(1/x) dx = ln(x) + C

∫sin(x)dx = - cos(x) + C",formula integr ∫xn dx xn1n1 c ∫egxdx egx1gx c ∫mxg,"['formula', 'integr', '∫xn', 'dx', 'xn1n1', 'c', '∫egxdx', 'egx1gx', 'c', '∫mxg']",formulas for integration  ∫x^n dx =x^(n+1)/(n+1) + c  ∫e^g(x)dx = e^g(x)*1/g(x) + c  ∫m(x)*g(x) dx =
365,367,"Basics of Limit

> Limit is used to solve a function for a particular value where it becomes indeterminate (0/0, 1/0, ∞/∞)

e.g. f(x)=(x^2-4)/(x-2) is indeterminate at x=2

Then we need to find Lim f(x) at x approaches 2
>> x can tend to 2 from two sides : positive higher values or from negative higher values",basic limit limit use solv function particular val,"['basic', 'limit', 'limit', 'use', 'solv', 'function', 'particular', 'val']",basics of limit  > limit is used to solve a function for a particular value where it becomes indeter
366,368,"Solution method of limit of indeterminate form

1. Factorization method

In factorization method, we convert indeterminate form to determinate form by factorizing the numerator and denominator with negative limit value first

Lim f(x) at x approaches 2=(x^2-4)/(x-2)=(x-2)(x+2)/(x-2)=x+2=4

2. By L' Hospital's Rule (also known as Bernoulli's rule)
 
Differentiate the numerator and differentiate the denominator and then take the limit.

Lim f(x)/g(x) at x approaches a = Lim f'(x)/g'(x) at x approaches a

Lim f(x) at x approaches 2 =(x^2-4)/(x-2)=2x/1=2x=4",solut method limit indetermin form 1 factor method,"['solut', 'method', 'limit', 'indetermin', 'form', '1', 'factor', 'method']","solution method of limit of indeterminate form  1. factorization method  in factorization method, we"
367,369,A fraction has two parts. The number on the top of the line is called the numerator. The number below the line is called the denominator.,fraction two part number top line call numer numbe,"['fraction', 'two', 'part', 'number', 'top', 'line', 'call', 'numer', 'numbe']",a fraction has two parts. the number on the top of the line is called the numerator. the number belo
368,370,"Sub-branches  of Algebra 

Algebra is divided into different sub-branches such as
 
1. Elementary algebra 
2. Advanced algebra 
3. Abstract algebra 
4. Linear algebra and 
5. Commutative algebra
",subbranch algebra algebra divid differ subbranch 1,"['subbranch', 'algebra', 'algebra', 'divid', 'differ', 'subbranch', '1']",sub-branches  of algebra   algebra is divided into different sub-branches such as   1. elementary al
369,371,"Linear algebra

It concerns the linear equations for the linear functions with their representation in vector spaces (=sample spaces) and through the matrices.

To fully comprehend machine learning, linear algebra fundamentals are the essential prerequisite.

In machine learning, the majority of data is most often represented as vectors, matrices or tensors. Therefore, the machine learning heavily relies on the linear algebra.",linear algebra concern linear equat linear functio,"['linear', 'algebra', 'concern', 'linear', 'equat', 'linear', 'functio']",linear algebra  it concerns the linear equations for the linear functions with their representation 
370,372,"Scaler, Vector, Matrix and Tensor

Scaler: a single number.

A vector is a 1D array: a list of numbers

A matrix is a 2D array of numbers, that has a fixed number of rows and columns: a list of lists

The term tensor is generally used for >2D array. But actually tensor can be of 1 to n dmensions.

A tensor of dimension one is a vector. ",scaler vector matrix tensor scaler singl number ve,"['scaler', 'vector', 'matrix', 'tensor', 'scaler', 'singl', 'number', 've']","scaler, vector, matrix and tensor  scaler: a single number.  a vector is a 1d array: a list of numbe"
371,373,"Vector and Array

A vector <1,2,3...,n> can be called a matrix because a list n elements may be represented as a list of n number of lists each having one element [[1],[2],[3],...[n]]

A Tensor of Dimension three may be flattened to dimension two and then it can again be flattened to dimension one. 

Thus tensor and array are same.

> An array can be of one dimension to n dimensions as we can divide the space in n dimension. 

> Numpy is a linear algebra library for python. 
> Pandas is built on Numpy through Numpy API for Data Science area.
> Sklearn is built on Numpy through Numpy API for Machine Learning area.
> TensorFlow is built on Numpy through Numpy API for deep learning area.",vector array vector 123n call matrix list n elemen,"['vector', 'array', 'vector', '123n', 'call', 'matrix', 'list', 'n', 'elemen']","vector and array  a vector <1,2,3...,n> can be called a matrix because a list n elements may be repr"
372,374,"Dimension and Direction

Dimension is the mathematical structure of the environment for storing and managing data. An environment can be of one dimensional space to n dimensional space. 

Direction is the movement of an experience in the environment.

>> With two dimensions we can create four(2^2) spaces (x,y), (x,-y), (-x,y), (-x,-y)
>> With three dimensions we can create eight(2^3) spaces (x,y,z), (x,-y,z), (-x,y,z), (-x,-y,z), (x,y,-z), (x,-y,-z), (-x,y,-z), (-x,-y,-z) ",dimens direct dimens mathemat structur environ sto,"['dimens', 'direct', 'dimens', 'mathemat', 'structur', 'environ', 'sto']",dimension and direction  dimension is the mathematical structure of the environment for storing and 
373,375,"position vector/ location vector/ radius vector

The magnitude of movement of an experience in any direction is the scalar quantity or numbers.

A vector is a mathematical quantity with both magnitude and direction. 

> In geometry, a position or position vector, also known as location vector or radius vector, is a Euclidean vector that represents the position of a point P (x,y,z) in space in relation to an arbitrary reference origin
> A position vector,r may be represented as <x,y> or <rcosθ, rsiinθ> 
> Here r is magnitude and θ is derection of the position vector",posit vector locat vector radius vector magnitud m,"['posit', 'vector', 'locat', 'vector', 'radius', 'vector', 'magnitud', 'm']",position vector/ location vector/ radius vector  the magnitude of movement of an experience in any d
374,376,"Applications of Linear Algebra in Data Science

1. Coordinate Transformations
2. Linear Regression
3. Dimensionality Reduction
4. Natural Language Processing
5. Computer Vision
6. Network Graphs",applic linear algebra data scienc 1 coordin transf,"['applic', 'linear', 'algebra', 'data', 'scienc', '1', 'coordin', 'transf']",applications of linear algebra in data science  1. coordinate transformations 2. linear regression 3
375,377,"Vector operations

1. Scalar multiplication with vector
2. Vector Addition
3. Vector Subtraction
4. Vector dot product
5. Vector projection
6.Vector cross product
7. Vector norms

> Vector operation makes the calculation simple: Thus we can directly perfom different operations on the column (vector) of a dataset without going for individual operation on every operation ",vector oper 1 scalar multipl vector 2 vector addit,"['vector', 'oper', '1', 'scalar', 'multipl', 'vector', '2', 'vector', 'addit']",vector operations  1. scalar multiplication with vector 2. vector addition 3. vector subtraction 4. 
376,378,"Vector Dot Product

a.b=|a||b|cosθ, where θ is the angle between vector a and b

a.b=a1b1+a2b2+a3b3+…..+anbn, where vector a=(a1,a2,a3,..an) and vector b=(b1,b2,b3,…bn)

>> The dot product is a fundamental way we can combine two vectors. 
>> Intuitively, it tells us something about how much two vectors point in the same direction.
>> In coordinate system, as the vector i, j and k are perpendicular to each other, i.i=1, j.j=1 and k.k=1, i.j=0, j.k=0, i.k=0",vector dot product ababcosθ θ angl vector b aba1b1,"['vector', 'dot', 'product', 'ababcosθ', 'θ', 'angl', 'vector', 'b', 'aba1b1']","vector dot product  a.b=|a||b|cosθ, where θ is the angle between vector a and b  a.b=a1b1+a2b2+a3b3+"
377,379,"Vector Projection

The projection of u onto v is another vector that is parallel to v and has a length equal to what vector u's shadow would be

projection of vector a onto the vector b: 
vector p = (a·b / b·b) * b 

>> The magnitude of any vector is also called the modulus or the length of the vector.",vector project project u onto v anoth vector paral,"['vector', 'project', 'project', 'u', 'onto', 'v', 'anoth', 'vector', 'paral']",vector projection  the projection of u onto v is another vector that is parallel to v and has a leng
378,380,"Vector Cross product

a x b=|a||b|sinθ
The cross product of any two vectors is a vector that is perpendicular to the two vectors. It has both magnitude and direction. The magnitude of the resultant vector is equal to the area of the parallelogram, whose side lengths are equal to the magnitude of the two given vectors.
>> In coordinate system, as the unit vector i, j and k are perpendicular to each other, iXi=0, jXj=0 and kXk=0, iXj=k, jXk=i, kXi=j, j x i= -k. k x j= -i. i x k= -j.",vector cross product x babsinθ cross product two v,"['vector', 'cross', 'product', 'x', 'babsinθ', 'cross', 'product', 'two', 'v']",vector cross product  a x b=|a||b|sinθ the cross product of any two vectors is a vector that is perp
379,381,"L1 norm 

It is defined as the sum of magnitudes of each component
        a =  ( a1 , a2 , a3 )   
L1 norm of vector a  =  |a1| + |a2| + |a3|

> This is like manhatten distance (Minkowski distance with p = 1)

L2 norm 
It is defined as the square root of sum of squares of each component 
L2 norm of vector a =  √( a1^2  + a2^2 + a3^2 )

> This is like euclidean distance (Minkowski distance with p = 2)

> L1 and L2 norms are used for minimizing the loss function",l1 norm defin sum magnitud compon a1 a2 a3 l1 norm,"['l1', 'norm', 'defin', 'sum', 'magnitud', 'compon', 'a1', 'a2', 'a3', 'l1', 'norm']","l1 norm   it is defined as the sum of magnitudes of each component         a =  ( a1 , a2 , a3 )    "
380,382,"Representing multivariable linear equation(y=x1+x2+x3) in vector space

1. Say we are considering an list type data structure with 3 numbers of integer data, i.e. [2,4,6]
2. [2,4,6] is a 1D array or a vector
3. So, we can write 
vector y= <2,4,6> or
vector y= vector2+ vector4+ vector6 or 
y=x1+x2+x3 considering x,y and z axis
4. Here the resultant vector y will be from point (0,0,0) to point (2,4,6) in x,y,z coordinate system",repres multivari linear equationyx1x2x3 vector spa,"['repres', 'multivari', 'linear', 'equationyx1x2x3', 'vector', 'spa']",representing multivariable linear equation(y=x1+x2+x3) in vector space  1. say we are considering an
381,383,"Multivariable linear equations

y=1*x1+1*x2+1*x3, 1 is the slope of the line when x2 vector and x3 vector are zero.

y=5*x1+3*x2+4*x3, where 5 is the slope of the line when x2 vector and x3 vector are zero.

",multivari linear equat y1x11x21x3 1 slope line x2 ,"['multivari', 'linear', 'equat', 'y1x11x21x3', '1', 'slope', 'line', 'x2']","multivariable linear equations  y=1*x1+1*x2+1*x3, 1 is the slope of the line when x2 vector and x3 v"
382,384,"Visualization of an experience

Vector we can understand as straight pipeline.

The slopes 1,5,3,4 are scalar quantities and these are multiplied with vectors and we know that scalar multiplication changes the magnitude of a vector. Thus it is better to understand them as weights per unit length.

If the magnitude of a vector is length, then it is length vector and if the magnitude of a vector is weight, then it is a weight vector (after multiplication with weight per unit length)

Thus we can write an observation row (vector) having a list of elements as linear equation and visualize the experience in a vector space as a piping layout.

The changing weight per unit length in different direction can be understood as changing diameter of the pipeline (weight per unit length is the property of the dimension)",visual experi vector understand straight pipelin s,"['visual', 'experi', 'vector', 'understand', 'straight', 'pipelin', 's']","visualization of an experience  vector we can understand as straight pipeline.  the slopes 1,5,3,4 a"
383,385,"Basics Matrix Algebra

A List of Lists or a 2D Numpy array or a Pandas DataFrame is a 2D array means a matrix or a list of vectors or a list of experiences",basic matrix algebra list list 2d numpi array pand,"['basic', 'matrix', 'algebra', 'list', 'list', '2d', 'numpi', 'array', 'pand']",basics matrix algebra  a list of lists or a 2d numpy array or a pandas dataframe is a 2d array means
384,386,"Types of Matrices

1. Symmetric matrix
2. Anti-symmetric matrix
3. Column matrix
4. Row matrix",type matric 1 symmetr matrix 2 antisymmetr matrix ,"['type', 'matric', '1', 'symmetr', 'matrix', '2', 'antisymmetr', 'matrix']",types of matrices  1. symmetric matrix 2. anti-symmetric matrix 3. column matrix 4. row matrix
385,387,"Matrix operations

Addition, Subtraction, multiplication

> For addition or subtraction of matrix A and B, both the matrices needs to have same dimension",matrix oper addit subtract multipl addit subtract ,"['matrix', 'oper', 'addit', 'subtract', 'multipl', 'addit', 'subtract']","matrix operations  addition, subtraction, multiplication  > for addition or subtraction of matrix a "
386,388,"Matrix multiplication

Rule is Column of 1st matrix=  Row of 2nd matrix

Size of resultant matrix=Row of 1st matrix* Column of 2nd matrix

Matrix A=(aij) where i=rows, j=columns

Thus, 1st element of resultant matrix=(a11*b11+ a12*b21+ a13*b31+ a1n*bn1)

Matrix multiplication induces some transformation (rotation/ reflection/ shearing etc.) on other vector or matrix and gives the resultant vector or matrix",matrix multipl rule column 1st matrix row 2nd matr,"['matrix', 'multipl', 'rule', 'column', '1st', 'matrix', 'row', '2nd', 'matr']",matrix multiplication  rule is column of 1st matrix=  row of 2nd matrix  size of resultant matrix=ro
387,389,"Equation in matrix form

K11X1+K12X2=R1,
K21X1+K22X2=R2 can be written as

(K11 K12)(X1)=(R1)
k21   K22   X2     R2

or, KX = R

or, X= (K−inverse)*R
 
Thus, we can find the values of x1 and x2",equat matrix form k11x1k12x2r1 k21x1k22x2rsquar wr,"['equat', 'matrix', 'form', 'k11x1k12x2r1', 'k21x1k22x2rsquar', 'wr']","equation in matrix form  k11x1+k12x2=r1, k21x1+k22x2=r-squared can be written as  (k11 k12)(x1)=(r1)"
388,390,"Identity matrix

A square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros. The effect of multiplying a given matrix by an identity matrix is to leave the given matrix unchanged.

> If a matrix is multiplied with its conjugate matrix and give an identity matrix, we call it an unitary matrix

> A diagonal matrix  has non-negative real numbers on the diagonal",ident matrix squar matrix element princip diagon o,"['ident', 'matrix', 'squar', 'matrix', 'element', 'princip', 'diagon', 'o']",identity matrix  a square matrix in which all the elements of the principal diagonal are ones and al
389,391,"Determinant of a matrix

In mathematics, the determinant is a scalar value that is a function of the entries of a square matrix.

The determinant is useful for solving linear equations.
>> Determiants can only be found for a square matrix. Means, no. of equations (observations) must be equal to no. of unknowns (features)
Easy way is diagonal method
For 3X3 matrix
|A| = a(ei − fh) − b(di − fg) + c(dh − eg)",determin matrix mathemat determin scalar valu func,"['determin', 'matrix', 'mathemat', 'determin', 'scalar', 'valu', 'func']","determinant of a matrix  in mathematics, the determinant is a scalar value that is a function of the"
390,392,"Transpose of a matrix

In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal; simply means we are interchanging the rows and columns.",transpos matrix linear algebra transpos matrix ope,"['transpos', 'matrix', 'linear', 'algebra', 'transpos', 'matrix', 'ope']","transpose of a matrix  in linear algebra, the transpose of a matrix is an operator which flips a mat"
391,393,"Adjoint of a matrix

the adjugate or classical adjoint of a square matrix is the transpose of its cofactor matrix. It is also occasionally known as adjunct matrix
",adjoint matrix adjug classic adjoint squar matrix ,"['adjoint', 'matrix', 'adjug', 'classic', 'adjoint', 'squar', 'matrix']",adjoint of a matrix  the adjugate or classical adjoint of a square matrix is the transpose of its co
392,394,"cofactor matrix 

A cofactor matrix is formed by finding the cofactors for all elements. Cofactor of any element is obtained by eliminating the row and column of that particular element and then finding the determinant.",cofactor matrix cofactor matrix form find cofactor,"['cofactor', 'matrix', 'cofactor', 'matrix', 'form', 'find', 'cofactor']",cofactor matrix   a cofactor matrix is formed by finding the cofactors for all elements. cofactor of
393,395,"Inverse of a matrix 

Inverse of matrix A= adj(A)/det(A)

Why Do We Need an Inverse? Because with matrices we don't divide! There is no concept of dividing by a matrix by another matrix. But we can multiply by an inverse, which achieves the same thing.",invers matrix invers matrix adjadeta need invers m,"['invers', 'matrix', 'invers', 'matrix', 'adjadeta', 'need', 'invers', 'm']",inverse of a matrix   inverse of matrix a= adj(a)/det(a)  why do we need an inverse because with mat
394,396,"Eigenvalues and Eigenvectors of a square matrix

The eigenvector is a vector which does not change in direction after transformation (matrix) is applied

In simple words, when we find the eigenvector of any big matrix no. of features or columns are reduced substantially. Thus a complex data can be better understood.
>> Eigenvalues are just the scaling or magnification factors of the eigenvector or the simple data

Av=λv

or, (A-λI)=0

or, |A-λI|=0

where v is the eigenvector of the matrix, A and matrix, A only does scaling of the eigenvector. λ is the eigen value or the scaling factor. I is the identity matrix of size A. 

Thus we can find the eigenvalues and eigenvectors of any matrix. 

For some matrices there may not be any eigenvectors. ",eigenvalu eigenvector squar matrix eigenvector vec,"['eigenvalu', 'eigenvector', 'squar', 'matrix', 'eigenvector', 'vec']",eigenvalues and eigenvectors of a square matrix  the eigenvector is a vector which does not change i
395,397,"Use of eigen values and eigen vectors

> Compress the data
> To transform the original features into another feature subspace
> Optimal computational efficiency
> Cluster optimization in k-means clustering
> To better understand and visualize linear mappings
> To understand the stability of mechanical constructions
> For solving systems of differential equations, > To recognize images, > To interpret and visualize quadratic equations, 
> For image segmentation.
",use eigen valu eigen vector compress data transfor,"['use', 'eigen', 'valu', 'eigen', 'vector', 'compress', 'data', 'transfor']",use of eigen values and eigen vectors  > compress the data > to transform the original features into
396,398,"Nilpotent matrix 

Nilpotent matrix is a square matrix with n-dimensional triangular matrix with zeros along the main diagonal.",nilpot matrix nilpot matrix squar matrix ndimensio,"['nilpot', 'matrix', 'nilpot', 'matrix', 'squar', 'matrix', 'ndimensio']",nilpotent matrix   nilpotent matrix is a square matrix with n-dimensional triangular matrix with zer
397,399,"Probability theory 

Probability theory is very important in machine learning because, our ML model learns the rules/trend of the training data or training experiences and does prediction about future event. Prdiction is all about calculating the probability/chances of a event to occur.  ",probabl theori probabl theori import machin learn ,"['probabl', 'theori', 'probabl', 'theori', 'import', 'machin', 'learn']","probability theory   probability theory is very important in machine learning because, our machine l"
398,400,"Set theory in Probability Theory

An event is a subset of the sample space or universal set (with all possible outcomes).

Universal Set: Set containing all elements and of which all other sets are subsets. Denoted by U.",set theori probabl theori event subset sampl space,"['set', 'theori', 'probabl', 'theori', 'event', 'subset', 'sampl', 'space']",set theory in probability theory  an event is a subset of the sample space or universal set (with al
399,401,"Experiment, sample space, observation and experience

> An experiment (e.g. studying in any class) can be infinitely repeated and has a well-defined set of possible outcomes, called the sample space (e.g. column names of this excel file). 

> One observation means one outcome (e.g. one cell or this excel file) and one experience means a set of observations (e.g. one row of this excel file).

> Thus from an experiment, we get a dataset or a set of experiences (e.g. the entire worksheet of this excel file)",experi sampl space observ experi experi eg studi c,"['experi', 'sampl', 'space', 'observ', 'experi', 'experi', 'eg', 'studi', 'c']","experiment, sample space, observation and experience  > an experiment (e.g. studying in any class) c"
400,402,"Set Theory

Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects.

",set theori set theori branch mathemat logic studi ,"['set', 'theori', 'set', 'theori', 'branch', 'mathemat', 'logic', 'studi']","set theory  set theory is the branch of mathematical logic that studies sets, which can be informall"
401,403,"Venn Diagrams

> Complement of A (denoted by A’ or A^c)- Everything that is not in A 
> Intersection of A and B (denoted by A ∩ B)- Everything in A and B

n(A ∩ B)= n(A)+ n(B)-n(A U B)

n(A∪B) = n(A - B) + n(A ∩ B) + n(B - A) 

n(B) = n(A ∩ B) + n(B - A)

(A ∩ B) means (A and B)
> Union of A and B (denoted by A U B)-Everything in A or everything in B or both in A and B
(A U B) means (A or B)",venn diagram complement denot a’ ac everyth inters,"['venn', 'diagram', 'complement', 'denot', 'a’', 'ac', 'everyth', 'inters']",venn diagrams  > complement of a (denoted by a’ or a^c)- everything that is not in a  > intersection
402,404,"Mutually exclusive sets

If no element is common between two sets, we say that they are mutually exclusive.

A ∩ B = φ,",mutual exclus set element common two set say mutua,"['mutual', 'exclus', 'set', 'element', 'common', 'two', 'set', 'say', 'mutua']","mutually exclusive sets  if no element is common between two sets, we say that they are mutually exc"
403,405,"Permutation & Combination (Counting)

nPr=n!/(n-r)!

nCr=n!/{r!(n-r)!}

For counting, we can import libraries in python

from itertools import permutations

from itertools import combinations

combinations(my_list,4)",permut combin count nprnnr ncrnrnr count import li,"['permut', 'combin', 'count', 'nprnnr', 'ncrnrnr', 'count', 'import', 'li']","permutation & combination (counting)  npr=n!/(n-r)!  ncr=n!/{r!(n-r)!}  for counting, we can import "
404,406,"Basic Probability

> Probability is a numerical way of describing how likely (or not) an event is to happen. If each of the elements in the sample space are equally likely, then we can define the probability of event A as P(A) = n(A) / n(S)

n(S) is the no. of elements in the set or no. of possible outcomes

> Not event A, P(~A)=1-P(A)",basic probabl probabl numer way describ like event,"['basic', 'probabl', 'probabl', 'numer', 'way', 'describ', 'like', 'event']",basic probability  > probability is a numerical way of describing how likely (or not) an event is to
405,407,"Probability Axiom#1

The relative frequency of event that is certain to occur must be 1.

The probability of getting any one of the numbers 1 to 6 on a dice is certain! 

P(S)=1, where S={1,2,3,4,5,6}",probabl axiom1 relat frequenc event certain occur ,"['probabl', 'axiom1', 'relat', 'frequenc', 'event', 'certain', 'occur']",probability axiom#1  the relative frequency of event that is certain to occur must be 1.  the probab
406,408,"Probability Axiom#2

The relative frequency of occurrence of any event must not be negative, that is, probabilities can never be negative. 
So the probability of an impossible event is 0.

So rules 1 and 2 together are telling us that probabilities lie between 0 (impossible) and 1 (certain). 
",probabl axiom2 relat frequenc occurr event must ne,"['probabl', 'axiom2', 'relat', 'frequenc', 'occurr', 'event', 'must', 'ne']","probability axiom#2  the relative frequency of occurrence of any event must not be negative, that is"
407,409,"Probability Axiom#3 or Special Addition Rule

P(A U B) = P(A) + P(B)    if A ∩ B = φ

This property is known as additivity.",probabl axiom3 special addit rule pa u b pa pb ∩ b,"['probabl', 'axiom3', 'special', 'addit', 'rule', 'pa', 'u', 'b', 'pa', 'pb', '∩', 'b']",probability axiom#3 or special addition rule  p(a u b) = p(a) + p(b)    if a ∩ b = φ  this property 
408,410,"Addition Rule

In general, where two sets are not necessarily mutually exclusive, the rule can be extended as follows: 

P(A U B) = P(A) + P(B) - P(A ∩ B)

> Event A and B can occur at the same time (not mutually exclussive)",addit rule general two set necessarili mutual excl,"['addit', 'rule', 'general', 'two', 'set', 'necessarili', 'mutual', 'excl']","addition rule  in general, where two sets are not necessarily mutually exclusive, the rule can be ex"
409,411,"Conditional Probability

A conditional probability is the probability of an event, given some other event has already occurred. It is denoted by P(A|B) inferred as probability of A given B.

P(A | B) = P(A ∩ B) / P(B)
 > If P(A | B) or P(A/B) =P(A), then A and B are independent events. Then probability of A not given B=
P(A /~B) = P(A)",condit probabl condit probabl probabl event given ,"['condit', 'probabl', 'condit', 'probabl', 'probabl', 'event', 'given']","conditional probability  a conditional probability is the probability of an event, given some other "
410,412,"Multiplication Rule

The probability that Events A and B both occur is equal to the probability that Event B occurs times the probability that Event A occurs, given that B has occurred. 
P(A ∩ B) = P(B) * P(A | B)

If A and B are independent, 
P(A ∩ B) = P(B) * P(A)",multipl rule probabl event b occur equal probabl e,"['multipl', 'rule', 'probabl', 'event', 'b', 'occur', 'equal', 'probabl', 'e']",multiplication rule  the probability that events a and b both occur is equal to the probability that
411,413,"Bayes' Theorem

Bayes' theorem provides a way to revise existing predictions or theories (update probabilities) given new or additional evidence.

P(Ej) -> Prior Probabilities
P(Ei | A) -> Posterior Probability

P(C / X)= P(X / C) *P(C) ) / P(X) 

We calculate posterior probability with the help of prior probability and additional evidence event A.

> Baye's theorem or rule can be used for answering probabilistic query",bay theorem bay theorem provid way revis exist pre,"['bay', 'theorem', 'bay', 'theorem', 'provid', 'way', 'revis', 'exist', 'pre']",bayes theorem  bayes theorem provides a way to revise existing predictions or theories (update proba
412,414,"Solving any set or probability problem

> Identify the event, subsets or groups: A, B

> Identify n(A) or P(A)

> Identify n(B) or P(B)

>  Identify n(A U B) or P(A U B) or P(A or B)

> Identify n(A ∩ B) or P(A ∩ B) or P(A and B)
> Understand the concept in totality: 
1. experiment, 
2. sample space,
3. observation,
4. experience and
5. dataset ",solv set probabl problem identifi event subset gro,"['solv', 'set', 'probabl', 'problem', 'identifi', 'event', 'subset', 'gro']","solving any set or probability problem  > identify the event, subsets or groups: a, b  > identify n("
413,415,"Difference between mutually exclusive and independent events

> A mutually exclusive event can simply be defined as a situation when two events cannot occur at same time whereas independent event occurs when one event remains unaffected by the occurrence of the other event.

> If two events are mutually exclusive, then they cannot be independent.",differ mutual exclus independ event mutual exclus ,"['differ', 'mutual', 'exclus', 'independ', 'event', 'mutual', 'exclus']",difference between mutually exclusive and independent events  > a mutually exclusive event can simpl
414,416,"P(~A ∩ ~B)=
1- P(A U B)",pa ∩ b 1 pa u b,"['pa', '∩', 'b', '1', 'pa', 'u', 'b']",p(~a ∩ ~b)= 1- p(a u b)
415,417,"Basics of Summarizing Data

It is used for understanding large to very large dataset through different statistics",basic summar data use understand larg larg dataset,"['basic', 'summar', 'data', 'use', 'understand', 'larg', 'larg', 'dataset']",basics of summarizing data  it is used for understanding large to very large dataset through differe
416,418,"Numerical, Categorical, Dichotomous, 
 and Ordinal Data

> structured data can be of two types-Numerical and Categorical

> Numerical data are two types-Discrete and Continuous

> Categorical data can be of three types-Dichotomous(attribute like Yes or No), 

Nominal (Names like bus, train, car or red, yellow, blue) and 

Ordinal(order like hot, hotter, hotest)",numer categor dichotom ordin data structur data tw,"['numer', 'categor', 'dichotom', 'ordin', 'data', 'structur', 'data', 'tw']","numerical, categorical, dichotomous,   and ordinal data  > structured data can be of two types-numer"
417,419,"1. Measure of central tendency 

(Mean, Median, ,Mode)",1 measur central tendenc mean median mode,"['1', 'measur', 'central', 'tendenc', 'mean', 'median', 'mode']","1. measure of central tendency   (mean, median, ,mode)"
418,420,"Understanding Mean

Mean is denoted by X bar or mu(μ) and is the most common method to measure central tendency

> It is influenced by outliers
> Not applicable to categorical data",understand mean mean denot x bar muμ common method,"['understand', 'mean', 'mean', 'denot', 'x', 'bar', 'muμ', 'common', 'method']",understanding mean  mean is denoted by x bar or mu(μ) and is the most common method to measure centr
419,421,"Understanding Median

After sorting n observations in ascending or decending order,
If n is odd,then the median is the middle observation. If n is even, then the median is the mean of the middle two observations.
> It is robust or resistant to the effects of extreme observations (outliers)
> Not applicable to categorical data",understand median sort n observ ascend decend orde,"['understand', 'median', 'sort', 'n', 'observ', 'ascend', 'decend', 'orde']","understanding median  after sorting n observations in ascending or decending order, if n is odd,then"
420,422,"Understanding Mode

Mode is defined as the value which occurs with the greatest frequency or the most typical value.

> Its use in practice is limited
> Not influenced by outliers
> Applicable to all types of data
> More than one mode possible",understand mode mode defin valu occur greatest fre,"['understand', 'mode', 'mode', 'defin', 'valu', 'occur', 'greatest', 'fre']",understanding mode  mode is defined as the value which occurs with the greatest frequency or the mos
421,423,"2. Measure of spread

(Range, Variance, Standard Deviation, Interquartile Range. Confidence interval)",2 measur spread rang varianc standard deviat inter,"['2', 'measur', 'spread', 'rang', 'varianc', 'standard', 'deviat', 'inter']","2. measure of spread  (range, variance, standard deviation, interquartile range. confidence interval"
422,424,"Understanding Range

The range is a very simple measure of spread defined as the difference between the largest and smallest observations in the data set.

The range is a poor measure of the spread of the data as it relies on the extreme values, which aren’t necessarily representative of the data as a whole.",understand rang rang simpl measur spread defin dif,"['understand', 'rang', 'rang', 'simpl', 'measur', 'spread', 'defin', 'dif']",understanding range  the range is a very simple measure of spread defined as the difference between 
423,425,"Understanding Variance

> Variance is denoted by σ^2 or S^2
> Variance is the expectation of the squared deviation of a random variable from its mean. In other words, it measures how far a set of numbers is spread out from their average value or mean.
",understand varianc varianc denot σ2 s2 varianc exp,"['understand', 'varianc', 'varianc', 'denot', 'σ2', 's2', 'varianc', 'exp']",understanding variance  > variance is denoted by σ^2 or s^2 > variance is the expectation of the squ
424,426,"Understanding Standard Deviation

> The standard deviation is the positive square root of the variance.
> A quantity expressing by how much the members of a group differ from the mean value for the group
> A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range",understand standard deviat standard deviat posit s,"['understand', 'standard', 'deviat', 'standard', 'deviat', 'posit', 's']",understanding standard deviation  > the standard deviation is the positive square root of the varian
425,427,"Understanding Interquartile Range

> IQR is another measure of spread which is like the range but not affected by the data extremes
> Q1, Q2 and Q3 are the quartiles that divide a set of data into four quarters. 
> Note that Q2 is just the median, while Q1 is called the lower quartile and Q3 the upper quartile
> The interquartile range is defined as (Q3  - Q1)",understand interquartil rang interquartil rang ano,"['understand', 'interquartil', 'rang', 'interquartil', 'rang', 'ano']",understanding interquartile range  > interquartile range is another measure of spread which is like 
426,428,"Steps to Calculate IQR

> Arrange the data in asc or desc order
> Q2= median of the data
> Q1=The median of data values below the main median
> Q3=The median of data values above the main median
> Q3-Q1

>> Q1=percentile 25
Q2=percentile 50
Q3=percentile 75
Q4=percentile 100

Percentile = (Number of values below ""X""/Total Number of all the Values) * 100",step calcul interquartil rang arrang data asc desc,"['step', 'calcul', 'interquartil', 'rang', 'arrang', 'data', 'asc', 'desc']",steps to calculate interquartile range  > arrange the data in asc or desc order > q2= median of the 
427,429,"Outliers with respect to IQR

Q1-1.5*IQR

Q3+1.5*IQR",outlier respect interquartil rang q115interquartil,"['outlier', 'respect', 'interquartil', 'rang', 'q115interquartil']",outliers with respect to interquartile range  q1-1.5*interquartile range  q3+1.5*interquartile range
428,430,"3. Measures of Symmetry

Coefficient of Skewness

> Positively skewed (Mean > Median > Mode) 
> Negatively skewed (Mean < Median < Mode)
> Normal or Symmetrical (Mean = Median = Mode)",3 measur symmetri coeffici skew posit skew mean me,"['3', 'measur', 'symmetri', 'coeffici', 'skew', 'posit', 'skew', 'mean', 'me']",3. measures of symmetry  coefficient of skewness  > positively skewed (mean > median > mode)  > nega
429,431,"Understanding skewness

When we plot graph between two conclusive parameters (e.g. house price range and number of houses), we can draw conclusion from the skewness.
> If the plot is positively skewed, there is highest no. of houses with lower price range-Its a village
> If the plot is negatively skewed, there is highest no. of houses with higher price range-Its a metro
> If the distribution is normal, there is highest no. of houses with medium price range-Its a town

We can understand the skewness just by checking the mode and median of the house price

If mode (of x axis parameter)<median (of x axis parameter), positively skewed

If mode (of x axis parameter)>median(of x axis parameter), negatively skewed

If mode(of x axis parameter)=median(of x axis parameter), normal house price distribution

Therefore skewness is just the comparison between mode and median",understand skew plot graph two conclus paramet eg ,"['understand', 'skew', 'plot', 'graph', 'two', 'conclus', 'paramet', 'eg']",understanding skewness  when we plot graph between two conclusive parameters (e.g. house price range
430,432,"Libraries for Summarizing Data

from statistics import mode

mode(my_list)

from statistics import median

from statistics import mean

we can also calculate median and mean  using numpy library
 
Another way
from scipy import stats

stats.mode(a)

",librari summar data statist import mode modemylist,"['librari', 'summar', 'data', 'statist', 'import', 'mode', 'modemylist']",libraries for summarizing data  from statistics import mode  mode(my_list)  from statistics import m
431,433,"Skewness and Kurtosis measurements

my_df['column_name'].skew()

my_df['column_name'].kurt()

",skew kurtosi measur mydfcolumnnameskew mydfcolumnn,"['skew', 'kurtosi', 'measur', 'mydfcolumnnameskew', 'mydfcolumnn']",skewness and kurtosis measurements  my_df[column_name].skew()  my_df[column_name].kurt()  
432,434,"Understanding random variable

A random variable (numerical feature or column name) is a numerical description of the outcome of a statistical experiment

Variable can store single data or multiple data as data structure. Thus random variable can store single random data or multiple random data as data structure.

Let X = number of heads when we toss a coin (X can take two values, 0 and 1)

Let Y = number that comes up when we roll a die (Y can take values 1, 2, 3, 4, 5 or 6)

It is conventional to denote the random variable by a capital letter and the possible values it can take by a small letter.

Let X = number of heads when we toss a coin, 
then x belongs to {0, 1}

Let Z = weight of a randomly selected student in a class, 
then z belongs to (0, ∞)

> We can use set data structure to store all possible outcomes of an experiment.
> A random variable is a rule for associating a number with each element in the set. Thus random variable is a function of the outcomes.
> Experiment: Tossing a coin
Sample space: S = {H, T}. 
X is the number of heads when we toss a coin.
Then, X(H) = 1 and X(T) = 0.

Random variable is introduced for graphical representation of the probability of outcomes. 

Because without the numerical values of the outcome, we can not plot them on an axis.",understand random variabl random variabl numer fea,"['understand', 'random', 'variabl', 'random', 'variabl', 'numer', 'fea']",understanding random variable  a random variable (numerical feature or column name) is a numerical d
433,435,"Python Code Random Variables

import random
X=random.randint(1,6)
print(""You rolled"",X) 

returns any random number between 1 to 6

X=random.random()
print(""The random float is"",X)

> random library creates random data. Data structure can not be created using random library. Numpy is used in that case",python code random variabl import random xrandomra,"['python', 'code', 'random', 'variabl', 'import', 'random', 'xrandomra']","python code random variables  import random x=random.randint(1,6) print(""you rolled"",x)   returns an"
434,436,"Random seed

Repeat the same random numbers using seed (useful for presentation)

random.seed(11)
X=random.random()
print(""The random float is"",X)

NumPy random seed
np.random.seed(0) ; np.random.rand(4)

It is a pseudo-random number generator

np.random.rand() # generates a single random float between 0 and 1",random seed repeat random number use seed use pres,"['random', 'seed', 'repeat', 'random', 'number', 'use', 'seed', 'use', 'pres']",random seed  repeat the same random numbers using seed (useful for presentation)  random.seed(11) x=
435,437,"Types of Random Variables

1. Discrete Random Variables (countable)

2. Continuous Random Variables (uncountable)",type random variabl 1 discret random variabl count,"['type', 'random', 'variabl', '1', 'discret', 'random', 'variabl', 'count']",types of random variables  1. discrete random variables (countable)  2. continuous random variables 
436,438,"Continuous and Discrete Random variables using numpy array

X=np.random.uniform(1,3)
returns a float between 1 and 3

X=np.random.uniform(1,3,5)
returns a numpy array with 5 floats between 1 and 3
 
X=np.random.randint(1,3)
returns an int between 1 and 3

X=np.random.randint(1,3,5)
returns a numpy 1D array with 5 int between 1 and 3",continu discret random variabl use numpi array xnp,"['continu', 'discret', 'random', 'variabl', 'use', 'numpi', 'array', 'xnp']","continuous and discrete random variables using numpy array  x=np.random.uniform(1,3) returns a float"
437,439,"Discrete Random Variables

1. Probability Distribution or Mass Function (pf or pdf or pmf)

The function fX(x) = P(X=x) for each x in the range of X is the probability function (pf) of X.

2. Cumulative Distribution Function(cdf or df)-summation

The cumulative distribution function (cdf) of X is FX(x) = P(X ≤ x). It gives the probability that X assumes a value that does not exceed x. CDFs are also known as “Distribution functions (df)""",discret random variabl 1 probabl distribut mass fu,"['discret', 'random', 'variabl', '1', 'probabl', 'distribut', 'mass', 'fu']",discrete random variables  1. probability distribution or mass function (pf or pdf or pmf)  the func
438,440,"Continuous Random Variables 

1. Probability Density Function

In case of continuous variables we always take intervals into account. 

The probability associated with an interval of values, (a, b) say, is represented as P(a < X < b) – and is the area under the curve of the probability density function (pdf) from a to b.

2. Cumulative Distribution Function-Integration

The cumulative distribution function (cdf) is defined to be the function: FX(x) = P(X ≤ x) For a continuous random variable, FX(x) is a continuous, non-decreasing function, defined for all real values of x.",continu random variabl 1 probabl densiti function ,"['continu', 'random', 'variabl', '1', 'probabl', 'densiti', 'function']",continuous random variables   1. probability density function  in case of continuous variables we al
439,441,"Mean of Random Variables

E[X] is a measure of the average/centre/location/level of the distribution of X. It is called the expected value of X, or mean of X, and is usually denoted as μ.
>> Mean of Discrete Random Variables
E(X) = ∑xP(X=x)
>> Mean of Continuous Random Variables by integration
>> The expected value of a random variable is the 
mean value over an infinite number of observations of the variable.",mean random variabl ex measur averagecentrelocatio,"['mean', 'random', 'variabl', 'ex', 'measur', 'averagecentrelocatio']",mean of random variables  e[x] is a measure of the average/centre/location/level of the distribution
440,442,"Variance of Random Variables

The variance σ2 is a measure of the spread/dispersion/variability of the distribution. Specifically, it is a measure of the spread of the distribution about its mean.

Variance of Discrete Random Variables
Var(X) 
=∑(x-μ)^2*P(X=x)
= E(X^2)- E(X)^2

Variance of Continuous Random Variables",varianc random variabl varianc σ2 measur spreaddis,"['varianc', 'random', 'variabl', 'varianc', 'σ2', 'measur', 'spreaddis']",variance of random variables  the variance σ2 is a measure of the spread/dispersion/variability of t
441,443,"Point probability, cumulative probability

If the definition of random variable matches with the experiment of any scientist for finding out the probability distribution, we can use experimented probability distribution function. 

Thus we can answer point probability or cumulative probability for any outcome or mean and variance for the set of outcomes",point probabl cumul probabl definit random variabl,"['point', 'probabl', 'cumul', 'probabl', 'definit', 'random', 'variabl']","point probability, cumulative probability  if the definition of random variable matches with the exp"
442,444,"Formulas for expectation and variance

E(aX+b)= a*E(X)+b

e.g.
E(5X - 2Y) = 
5*E(X) - 2*E(Y)

Var(aX+b)= a^2*Var(X)

e.g. 
Var(5X - 2Y) =  5^2*Var(X) + 
(-2)^2*Var(Y)

",formula expect varianc eaxb aexb eg e5x 2y 5ex 2ey,"['formula', 'expect', 'varianc', 'eaxb', 'aexb', 'eg', 'e5x', '2y', '5ex', '2ey']",formulas for expectation and variance  e(ax+b)= a*e(x)+b  e.g. e(5x - 2y) =  5*e(x) - 2*e(y)  var(ax
443,445,"Basics of Probability distribution

Probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.

Simply, probability is the function of outcomes or function of function of outcomes (random variable)

Thus probability distribution is the graph between all possible numerical outcomes of an experiment vs. probability of occurance of the outcome.

The set of all possible numerical outcome is the random variable.

Thus probability distribution is the graph between random variable vs. probability",basic probabl distribut probabl distribut mathemat,"['basic', 'probabl', 'distribut', 'probabl', 'distribut', 'mathemat']",basics of probability distribution  probability distribution is the mathematical function that gives
444,446,"Types of Discrete Probability Distribution

> Depending on definition of random variable,X, we can identify the pdf

1. Uniform Distribution
2. Bernoulli Distribution
3. Binomial Distribution
4. Geometric Distribution
5. Poisson Distribution",type discret probabl distribut depend definit rand,"['type', 'discret', 'probabl', 'distribut', 'depend', 'definit', 'rand']","types of discrete probability distribution  > depending on definition of random variable,x, we can i"
445,447,"1. Uniform Distribution-all outcomes are equally likely

X is the no. of outcome from 1 to k. Thus,
x= 1,2,3,...k

P(X=x)=1/k
 
E(X)=(k+1)/2

Var(X)=(k^2-1)/12

",1 uniform distributional outcom equal like x outco,"['1', 'uniform', 'distributional', 'outcom', 'equal', 'like', 'x', 'outco']","1. uniform distribution-all outcomes are equally likely  x is the no. of outcome from 1 to k. thus, "
446,448,"2. Bernoulli Distribution-Bernauli trial is an experiment which has only two possible outcomes ""success"" and ""failure""

X is the success in a trial. Thus,
x=0,1
P(X=1)= θ
P(X=0)=1- θ

P(X=x)= 
θ^x*(1- θ)^(1-x)

E(X) = θ

Var(X) = θ - θ^2

> Bernoulli trials are also called as yes or no questions.",2 bernoulli distributionbernauli trial experi two ,"['2', 'bernoulli', 'distributionbernauli', 'trial', 'experi', 'two']","2. bernoulli distribution-bernauli trial is an experiment which has only two possible outcomes ""succ"
447,449,"3. Binomial Distribution-Binomial Distribution is the widely used probability distribution, derived from Bernoulli Process (Only two possible outcomes, i.e. success or failure).

Binomial distribution is one in which the probability of repeated number of trials (sequence of n bernoulli trials) are studied.

In, binomial distribution X is the number of success (or failure) in n trials. Thus 
x= 0,1,2...n

Also denoted as Bin(n,θ)

P(X=x)= C(n,x)*θ^x*(1- θ)^(n-x)

E(X) = nθ

Var(X) = nθ(1-θ)",3 binomi distributionbinomi distribut wide use pro,"['3', 'binomi', 'distributionbinomi', 'distribut', 'wide', 'use', 'pro']","3. binomial distribution-binomial distribution is the widely used probability distribution, derived "
448,450,"4. Geometric Distribution-gives the probability of first success with number of trials.
X is the no. of trial on which the first success occurs. Thus,
x=1,2,3,…

P(X=x)= 
θ*(1- θ)^(x-1)

E(X) = 1/θ

Var(X) = (1- θ)/θ^2
>> The probability of success (successive trials are without replacement) changes from trial to trial in 
Hypergeometric Distribution ",4 geometr distributiong probabl first success numb,"['4', 'geometr', 'distributiong', 'probabl', 'first', 'success', 'numb']",4. geometric distribution-gives the probability of first success with number of trials. x is the no.
449,451,"5. Poisson Distribution-
models/trials the no. of events that occurs within a specified interval of time.
Thus, rate of occurance, λ=E(X)=Var(X)

Unlimited number of possible outcomes(can  be used for two outcomes-success or failure).
X is the number of specified outcome (success, failure or anything else) in a specified interval of time. Thus,
x= 0,1,2,....
P(X=x)=λ^x*e^(-λ)/x!

from scipy.stats import bernoulli, poisson",5 poisson distribut modelstri event occur within s,"['5', 'poisson', 'distribut', 'modelstri', 'event', 'occur', 'within', 's']",5. poisson distribution- models/trials the no. of events that occurs within a specified interval of 
450,452,"Moments of probability distribution

1) The first moment is the mean, which indicates the central tendency of a distribution. 
2) The second moment is the variance, which indicates the width or spread 
3) The third moment is the skewness, which indicates any asymmetric 'leaning' to either left or right.
4) The fourth standardized moment is the kurtosis, which indicates the heaviness of the tail of the distribution.",moment probabl distribut 1 first moment mean indic,"['moment', 'probabl', 'distribut', '1', 'first', 'moment', 'mean', 'indic']","moments of probability distribution  1) the first moment is the mean, which indicates the central te"
451,453,"Moments in Physics

> The total mass is the zeroth moment of mass.
> The first moment is the center of the mass, and
> The second moment is the rotational inertia.
> From these moments we can understand how the physical quantity is arranged. Similarly, from the moments of pdf, we can understand its arrangement. ",moment physic total mass zeroth moment mass first ,"['moment', 'physic', 'total', 'mass', 'zeroth', 'moment', 'mass', 'first']",moments in physics  > the total mass is the zeroth moment of mass. > the first moment is the center 
452,454,"Basics of continuous distribution

Since continuous probability functions are defined for an infinite number of points over a continuous interval, the probability at a single point is always zero (e.g. P(weight=50.555)=0).

Hence, we will use P(X<=x) and P(X<x) interchangeably, P(X<=x) = P(X<x).Also, P(X>=x) = P(X>x)

 Since sum of probabilities over the entire range of x is 1, hence,P(X>x) = 1 - P(X<x)",basic continu distribut sinc continu probabl funct,"['basic', 'continu', 'distribut', 'sinc', 'continu', 'probabl', 'funct']",basics of continuous distribution  since continuous probability functions are defined for an infinit
453,455,"Types of continuous probability distribution

1. Uniform Distribution
2. Normal Distribution 
3. Standard Normal Distribution
4. Exponential Distribution
5. Gamma Distribution
 6. Chi-square Distribution
7. t-Distribution
8. F-Distribution

",type continu probabl distribut 1 uniform distribut,"['type', 'continu', 'probabl', 'distribut', '1', 'uniform', 'distribut']",types of continuous probability distribution  1. uniform distribution 2. normal distribution  3. sta
454,456,"1. Uniform Distribution-

X is the outcome between α and β

P(X<x)=1/(β- α)

E(X) = (β+ α)/2

Var(X) = (β- α)^2/12",1 uniform distribut x outcom α β pxx1β α ex β α2 v,"['1', 'uniform', 'distribut', 'x', 'outcom', 'α', 'β', 'pxx1β', 'α', 'ex', 'β', 'α2', 'v']",1. uniform distribution-  x is the outcome between α and β  p(x<x)=1/(β- α)  e(x) = (β+ α)/2  var(x)
455,457,"2. Normal Distribution (or Gaussian Distribution)-It has a symmetrical “bell-shaped” density curve

X is the outcome between -∞ and 
+ ∞ and expectation, μ and standard deviation, σ

Normal distribution is the function of x, μ and σ 

X ~ N(μ, σ^2)

> It is asymptotic, means each end approaches the horizontal axis but never reaches it",2 normal distribut gaussian distributionit symmetr,"['2', 'normal', 'distribut', 'gaussian', 'distributionit', 'symmetr']",2. normal distribution (or gaussian distribution)-it has a symmetrical “bell-shaped” density curve  
456,458,"3. Standard Normal Distribution- 

X is a normally distributed random variable with mean μ=0 and standard deviation σ=1. It will always be denoted by z_score",3 standard normal distribut x normal distribut ran,"['3', 'standard', 'normal', 'distribut', 'x', 'normal', 'distribut', 'ran']",3. standard normal distribution-   x is a normally distributed random variable with mean μ=0 and sta
457,459,"4. Exponential Distribution

X is the outcome between 0 and ∞ and has a rate of occurance is λ

P(X<x)= λ*e^-λx

E(X) = 1/λ

Var(X) = 1/λ^2

X ~ Exp(λ) means random variable X has an exponential distribution with rate parameter λ

> Exponential distribution is a special case of gamma distribution

The Exponential distribution also describes the time between events in a Poisson process

> It is used to model the lifetime of any equipment.

> It also gives the distribution of the waiting time, T

P(T>t)= e^-λt

P(T<t)= 1- e^-λt
",4 exponenti distribut x outcom 0 ∞ rate occur λ px,"['4', 'exponenti', 'distribut', 'x', 'outcom', '0', '∞', 'rate', 'occur', 'λ', 'px']",4. exponential distribution  x is the outcome between 0 and ∞ and has a rate of occurance is λ  p(x<
458,460,"Gamma Function

Gamma Function, Γ(α) = Integral on the interval [0, ∞ ] of 
∫t^(α −1)* e^(−t) dt.

The gamma function is one commonly used extension of the factorial function to complex numbers.

A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers, and i is a symbol called the imaginary unit, and satisfying the equation i² = −1",gamma function gamma function γα integr interv 0 ∞,"['gamma', 'function', 'gamma', 'function', 'γα', 'integr', 'interv', '0', '∞']","gamma function  gamma function, γ(α) = integral on the interval [0, ∞ ] of  ∫t^(α −1)* e^(−t) dt.  t"
459,461,"5. Gamma Distribution

Widely used distribution and its importance is largely due to its relation to exponential and normal distributions.

X is the outcome between 0 and ∞, rate of occurance, λ  and has a shape parameter α

P(X<x) =
λ^α*x^(α-1)*e^-λx/Γ(α)
",5 gamma distribut wide use distribut import larg d,"['5', 'gamma', 'distribut', 'wide', 'use', 'distribut', 'import', 'larg', 'd']",5. gamma distribution  widely used distribution and its importance is largely due to its relation to
460,462,"6. Chi-square Distribution

> The chi-squared distribution is a special case of the gamma distribution

X is the chi_square_score between 0 and ∞,  degrees of freedom=k 

P(X<x) =
x^(k/2-1)*e^(-x/2)/(2^k/2*Γ(k/2))

chi_square_score= (n-1)*S^2/σ^2",6 chisquar distribut chisquar distribut special ca,"['6', 'chisquar', 'distribut', 'chisquar', 'distribut', 'special', 'ca']",6. chi-square distribution  > the chi-squared distribution is a special case of the gamma distributi
461,463,"7. t-Distribution (It is similar to the normal distribution, just with fatter tails. Have higher kurtosis than normal distributions)
X is the t_score between -∞ and 
+ ∞, degrees of freedom=k 

",7 tdistribut similar normal distribut fatter tail ,"['7', 'tdistribut', 'similar', 'normal', 'distribut', 'fatter', 'tail']","7. t-distribution (it is similar to the normal distribution, just with fatter tails. have higher kur"
462,464,"8. F-Distribution

X is f_score between 0 and 
∞, when two independent random samples of size n1 and n2 are taken respectively ",8 fdistribut x fscore 0 ∞ two independ random samp,"['8', 'fdistribut', 'x', 'fscore', '0', '∞', 'two', 'independ', 'random', 'samp']","8. f-distribution  x is f_score between 0 and  ∞, when two independent random samples of size n1 and"
463,465,"log-normal distribution 

In probability theory, a log-normal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.",lognorm distribut probabl theori lognorm distribut,"['lognorm', 'distribut', 'probabl', 'theori', 'lognorm', 'distribut']","log-normal distribution   in probability theory, a log-normal distribution is a continuous probabili"
464,466,"Exponential expressions

Exponential expressions are just a way to write powers in short form. The exponent indicates the number of times the base is used as a factor. Exponentiation is the Arithmatic Operation for power.

> f(x) = e^x is called the natural exponential function  or in short, exponential function",exponenti express exponenti express way write powe,"['exponenti', 'express', 'exponenti', 'express', 'way', 'write', 'powe']",exponential expressions  exponential expressions are just a way to write powers in short form. the e
465,467,"Euler's number

The number e, also known as Euler's number (like pi, is a transcendental number), is a mathematical constant approximately equal to 2.71828, and can be characterized in many ways. It is the base of the natural logarithm. 
> The reason Euler's number is such an important constant is that it has unique properties that simplify many equations and patterns.

Other two logarithms are 

1. common logarithm-logarithm of base 10 

2. binary logarithm-logarithm of base 2

Logarithmic operation returns the power of the base. Thus, logarithms are a convenient way to express large numbers.",euler number number e also known euler number like,"['euler', 'number', 'number', 'e', 'also', 'known', 'euler', 'number', 'like']","eulers number  the number e, also known as eulers number (like pi, is a transcendental number), is a"
466,468,"Python Code for Continuous Distributions

from scipy.stats import uniform

from scipy.stats import norm

from scipy.stats import chi2

from scipy.stats import t

from scipy.stats import f

from scipy.stats import * (this imports all the modules)

lower_limit = 50
upper_limit = 150
range = upper_limit - lower_limit

# P(X < 74) is
uniform.cdf(74, lower_limit, range)

standard_deviation=math.sqrt(variance)

P(X<28) is norm.cdf(28, mean, standard_deviation)

P(X<z_score) is norm.cdf(z_score)
chi2.cdf(chi_square_score, degrees_of_freedom)
t.cdf(t_score, degrees_of_freedom)
f.cdf(f_score, degrees_of_freedom1, degrees_of_freedom2)",python code continu distribut scipystat import uni,"['python', 'code', 'continu', 'distribut', 'scipystat', 'import', 'uni']",python code for continuous distributions  from scipy.stats import uniform  from scipy.stats import n
467,469,"Continuous distributions in ML

The different probability distributions serve different purposes and represent different data generation processes.

Continuous distributions in ML-in the distribution of numerical input and output variables for models and in the distribution of errors made by models.",continu distribut machin learn differ probabl dist,"['continu', 'distribut', 'machin', 'learn', 'differ', 'probabl', 'dist']",continuous distributions in machine learning  the different probability distributions serve differen
468,470,"Most frequent types of distribution for data scientist:

1. Bernoulli 

2. Uniform 

3. Binomial 

4. Poisson 

5. Normal 

6. Exponential ",frequent type distribut data scientist 1 bernoulli,"['frequent', 'type', 'distribut', 'data', 'scientist', '1', 'bernoulli']",most frequent types of distribution for data scientist:  1. bernoulli   2. uniform   3. binomial   4
469,471,"Basics of Joint Distribution

For a given experiment, we are often interested not only in probability distribution functions of individual random variables but also in the relationships between two or more random variables.

The joint probability distribution can be expressed either in terms of a joint cumulative distribution function or in terms of a joint probability density function (in the case of continuous variables) or joint probability mass function (in the case of discrete variables).

>> Let A and B be the two events, joint probability is the probability of event B occurring at the same time that event A occurs. This can be written as P(A, B) or P(A ∩ B). Thus, the joint probability is also called the intersection of two or more events.",basic joint distribut given experi often interest ,"['basic', 'joint', 'distribut', 'given', 'experi', 'often', 'interest']","basics of joint distribution  for a given experiment, we are often interested not only in probabilit"
470,472,"Two dimensional random vector

The combination of two random variable is called two dimensional random vector (discrete or continuous). Better to call it random matrix.",two dimension random vector combin two random vari,"['two', 'dimension', 'random', 'vector', 'combin', 'two', 'random', 'vari']",two dimensional random vector  the combination of two random variable is called two dimensional rand
471,473,"Types of Joint 
(Bivariate) Probability Distributions (discrete and continuous)

> Marginal Distributions (discrete and continuous)-the probabilities for any one of the variables with no reference to any specific ranges of values for the other variables
> Conditional Probability Distribution- the probabilities for any subset of the variables conditional on particular values of the remaining variables. Conditional Distribution and Conditional expectation can be discrete or continuous.

Say the joint density function is f(x,y,z)

Then the marginal density functions are

fX(x)=∫∫f(x,y,z)dydz
fY(y)=∫∫f(x,y,z)dxdz
fZ(z)=∫∫f(x,y,z)dxdy

For independent random variables,

fX,Y,Z(x,y,z)=
fX(x)*fY(y)*fZ(z)",type joint bivari probabl distribut discret contin,"['type', 'joint', 'bivari', 'probabl', 'distribut', 'discret', 'contin']",types of joint  (bivariate) probability distributions (discrete and continuous)  > marginal distribu
472,474,Independent Random Variables (discrete and continuous) means outcomes are equally likely,independ random variabl discret continu mean outco,"['independ', 'random', 'variabl', 'discret', 'continu', 'mean', 'outco']",independent random variables (discrete and continuous) means outcomes are equally likely
473,475,> Probability mass function (for discrete random variable) or joint probability mass function is always expressed as matrix,probabl mass function discret random variabl joint,"['probabl', 'mass', 'function', 'discret', 'random', 'variabl', 'joint']",> probability mass function (for discrete random variable) or joint probability mass function is alw
474,476,"Degree of association

covariance and correlation

These are two important measures which describe the degree of association between two random variables X1 & X2

> Covariance gives the direction of relation
> Correlation give the strength of relation

> Covariance can vary between -∞ and +∞ 

> Correlation ranges between -1 and +1",degre associ covari correl two import measur descr,"['degre', 'associ', 'covari', 'correl', 'two', 'import', 'measur', 'descr']",degree of association  covariance and correlation  these are two important measures which describe t
475,477,"Understanding of Covariance 

Covariance  signifies  the  direction  of  the  linear  relationship between the two variables. By direction, we mean, if the variables are directly proportional or inversely proportional to each other. 

In general, it can be shown that a positive value of Cov(X1 , X2 ) is an indication that X2 tends to increase as X1 does, whereas a negative value indicates that X2 tends to decrease as X1 increases. 

The strength of the relationship between X1 and X2 is indicated by the correlation between X1 and X2 , a dimensionless quantity",understand covari covari signifi direct linear rel,"['understand', 'covari', 'covari', 'signifi', 'direct', 'linear', 'rel']",understanding of covariance   covariance  signifies  the  direction  of  the  linear  relationship b
476,478,"Understanding of Correlation

Cov(X1,X2)=
E(X1*X2)-E(X1)*E(X2)

Cov(X1,X1)=Var(X1)

Cov((X1+X2),X3)=
Cov(X1,X3)+ Cov(X2,X3) 

Cov(aX, Y) = Cov(X,aY)

Corr(X1,X2)= Cov(X1,X2)/sqrt(Var(X1)*Var(X2))
 
Thus correlation refers to the scaled form of covariance.
> Correlation is dimensionless
and it is not influenced by the change in scale.",understand correl covx1x2 ex1x2ex1ex2 covx1x1varx1,"['understand', 'correl', 'covx1x2', 'ex1x2ex1ex2', 'covx1x1varx1']","understanding of correlation  cov(x1,x2)= e(x1*x2)-e(x1)*e(x2)  cov(x1,x1)=var(x1)  cov((x1+x2),x3)="
477,479,"Basics of Sampling & Statistical Inference

Population of Data or simply population indicates the full set of data or experience of any topic or column_name which is impossible to gain  for our practical experiments or problems.
>> A  set  of  items  selected  from  a  parent  population  is  a random sample if the probability that any item in the population is included in the sample is proportional to its frequency in the parent population and the inclusion/ exclusion of any item in the sample operates independently 
From the statistics of the sample, we can draw inference about population.",basic sampl statist infer popul data simpli popul ,"['basic', 'sampl', 'statist', 'infer', 'popul', 'data', 'simpli', 'popul']",basics of sampling & statistical inference  population of data or simply population indicates the fu
478,480,"Random sample

Therefore, random sample is a set of possible outcomes of sampling operation. 

Random sample is denoted by X=(X1,X2,X3,…Xn)

Thus random sample is nothing but the values of a single column.

> Inferential statistics helps us to 
draw conclusions about a problem",random sampl therefor random sampl set possibl out,"['random', 'sampl', 'therefor', 'random', 'sampl', 'set', 'possibl', 'out']","random sample  therefore, random sample is a set of possible outcomes of sampling operation.   rando"
479,482,"Understanding of Statistics

STATISTIC is the score or measurement of each individual or a singular data or experience. STATISTICS is therefore, the process of designing, comparing, interpreting and analysing data. 

Statistics is concerned with the sample and not the population.

A statistics are also  random variables that is a function of the random sample, but not a function of unknown parameters.

Therefore, Random variable is a function of random sample.

Thus, statistics means creating multiple  random variables (i.e. mean, mode, median, variance etc.) from the random sample data or random sample experiences.

These random variables (mean, mode, median, variance etc.) or statistics are the function of each column of a dataset.

Pandas .describe() function gives the statistics.

.describe(include='all') to include all the categorical columns",understand statist statist score measur individu s,"['understand', 'statist', 'statist', 'score', 'measur', 'individu', 's']",understanding of statistics  statistic is the score or measurement of each individual or a singular 
480,483,"Statistical Inference 

To reach the final conclusion about the population, we take the help of different probability distribution as probability distribution of the sample mean is the indicator of population probability distribution.
(we can find individual probability, marginal probability or conditional probability of the random variables) 

Thus, Statistical Inference is the theory, methods, and practice of forming judgements about the parameters of a population.

> Looking at the sample, concluding about the population

The reliability of statistical inference typically depends on the random sampling.",statist infer reach final conclus popul take help ,"['statist', 'infer', 'reach', 'final', 'conclus', 'popul', 'take', 'help']","statistical inference   to reach the final conclusion about the population, we take the help of diff"
481,484,"Sample Mean and Sample Variance

Sample Mean, 
X̄ = (1/n) * Σ Xi

Sample Variance, 
S^2 = (1/(n-1)) * Σ (Xi - X̄)^2


variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)

variance(my_sample_list)- returns the sample variance

Sample mean is denoted by x̄ and population mean is denoted by μ.

Sample standard deviation is denoted by S and population standard deviation is denoted by σ

> Statistic is related to the sample  and parameter is related to the population. A statistic is used to estimate a parameter.

> Descriptive statistics is responsible for examining trends or distributions",sampl mean sampl varianc sampl mean x̄ 1n σ xi sam,"['sampl', 'mean', 'sampl', 'varianc', 'sampl', 'mean', 'x̄', '1n', 'σ', 'xi', 'sam']","sample mean and sample variance  sample mean,  x̄ = (1/n) * σ xi  sample variance,  s^2 = (1/(n-1)) "
482,485,"Independent and indentically distributed (IID) random variables

A collection of random variables is independent and identically distributed if each random variable has the same probability distribution (same μ and σ^2) as the others and all are mutually independent.
> This holds true when we take multiple samples randomly from the same population",independ indent distribut iid random variabl colle,"['independ', 'indent', 'distribut', 'iid', 'random', 'variabl', 'colle']",independent and indentically distributed (iid) random variables  a collection of random variables is
483,486,"Sampling with Replacement

Sampling ''with replacement'' means that when a unit selected at random from the population, it is returned to the population (replaced), and then a second element is selected at random.",sampl replac sampl replac mean unit select random ,"['sampl', 'replac', 'sampl', 'replac', 'mean', 'unit', 'select', 'random']",sampling with replacement  sampling with replacement means that when a unit selected at random from 
484,487,"Central Limit Theorem (CLT) or z-results or z-score

> The Central Limit Theorem states that the  distribution of the sample means approaches a normal distribution as the sample size gets larger — no matter what the shape of the sample distribution. This fact holds especially true for sample size, n> 30 (more accuracy n>50)
As per CLT,
Expectation of the sample mean, E(x̄) = μ
Variance of the sample mean, Var(x̄) = σ^2/n
z-result of the sample mean, z_score = (x̄ – μ) / (σ/√n)

> Sample size is the total no. of observations/ rows in the set of samples
> One sample means a set with all observations for one feature of the samples space
> z-score (also called a standard score) gives us an idea of how far from the mean a data point (x) is. 
 z_score = (x – μ) /√σ^2

> We can find the cdf after getting the score (a generated statistic from the basic statistics like expectation and variance) of any probability distribution 

CLT is is the bridge between statistics and probability

> CLT simply tells us that for large sample, population mean μ is the expected value of sample mean, E(x̄) having  highest probability of occurance and rest of the probability distribution will be symmetrically decreasing from the mean",central limit theorem central limit theorem zresul,"['central', 'limit', 'theorem', 'central', 'limit', 'theorem', 'zresul']",central limit theorem (central limit theorem) or z-results or z-score  > the central limit theorem s
485,488,"t Result or t Score

In most cases, we can get population mean but can not get population standard deviation. Thus, z result cannot be used. We use the t result in such cases.
> t result is valid for samples from normal distribution only.
> The t distribution is symmetrical about zero.
t_score = (x̄ – μ) / (S/√n)

Then we can calculate the probability for this t-score",result score case get popul mean get popul standar,"['result', 'score', 'case', 'get', 'popul', 'mean', 'get', 'popul', 'standar']","t result or t score  in most cases, we can get population mean but can not get population standard d"
486,489,"F result or F Score

> F result is valid for samples from normal distribution only.

If two independent random samples of size n1 and n2 are taken respectively,

f_score= (sample_1_variance/sample_2_variance)*(population_2_variance/population_1_variance)",f result f score f result valid sampl normal distr,"['f', 'result', 'f', 'score', 'f', 'result', 'valid', 'sampl', 'normal', 'distr']",f result or f score  > f result is valid for samples from normal distribution only.  if two independ
487,490,"Point Estimation

> Method of moments- The basic principle is to equate population moments to  corresponding  sample moments and solve for the parameter(s)

> Maximum likelihood estimator- The method of maximum likelihood is widely regarded as the best general method of finding estimators

Point estimation is to estimate a single value of population parameter (mean or variance)",point estim method moment basic principl equat pop,"['point', 'estim', 'method', 'moment', 'basic', 'principl', 'equat', 'pop']",point estimation  > method of moments- the basic principle is to equate population moments to  corre
488,491,"Mean and Expectation

> Mean of a random variable is the simple average of all the values, expectation of a random variable is the  probability-weighted average. 
> Thus, expectation is the value of random variable where the probability is maximum.
> For normal distribution, probability is maximum at mean. So, here mean= expectation
> The expected value of a random variable is the 
value that has the highest probability of occurring
> Normal distribution is the plot for feature (height, age, price, temp, speed etc.) vs. its probability density (mean of the feature is the middle point of the bell. The  distribution is spreaded in both  side of μ with μ ± σ, μ ± 2σ, μ ± 3σ)",mean expect mean random variabl simpl averag valu ,"['mean', 'expect', 'mean', 'random', 'variabl', 'simpl', 'averag', 'valu']","mean and expectation  > mean of a random variable is the simple average of all the values, expectati"
489,492,"Basics of Confidence interval

Confidence interval helps to determine the range of population parameters from sample parameters

Thus, a confidence interval provides an “interval estimate” of an unknown population parameter (as opposed to a “point estimate”)",basic confid interv confid interv help determin ra,"['basic', 'confid', 'interv', 'confid', 'interv', 'help', 'determin', 'ra']",basics of confidence interval  confidence interval helps to determine the range of population parame
490,493,"Calculating population parameters for one sample

When σ is known, then assumming normal distribution of the sample mean, we are 95 % confident that 

μ= X̄ ± 1.96*σ*1/√n

or 
CI= X̄ ± 1.96*σ*1/√n

When σ is unknown, then assumming t distribution of the sample mean, we are 95 % confident that 

μ= X̄ ± t0.025, (n-1)*S*1/√n

95% means 100*(1-0.05), Here α=0.05, α/2=0.025

When σ is unknown, then assumming Chi-square Distribution, we are 95% confident that 

σ^2= 
(n-1)*S^2/χ2  0.025,(n-1) and (n-1)*S^2/χ2  0.975,(n-1)

We can calculate population proportion (p or θ) from sample proportion (p̂) with some confidence interval

It is valid only when the binomial distribution (categorical data) can be approximated by normal distribution.

p= p̂ ± 1.96*sqrt(p̂*(1-p̂)/n)",calcul popul paramet one sampl σ known assum norma,"['calcul', 'popul', 'paramet', 'one', 'sampl', 'σ', 'known', 'assum', 'norma']","calculating population parameters for one sample  when σ is known, then assumming normal distributio"
491,494,"Population proportion(P or Pi) 
for binomial experiment

A population proportion is a fraction of the population that has a certain characteristic.

For example, let’s say we had 1,000 people in the population and 237 of those people have blue eyes. The fraction of people who have blue eyes is 237 out of 1,000, or 237/1000 or 23.7%",popul proportionp pi binomi experi popul proport f,"['popul', 'proportionp', 'pi', 'binomi', 'experi', 'popul', 'proport', 'f']",population proportion(p or pi)  for binomial experiment  a population proportion is a fraction of th
492,495,"Calculating population parameters for two samples from difference population

When σ1 and σ2 are known, then assumming normal distribution of the sample mean, we are 95 % confident that,

μ1- μ2= (X̄1-X̄2) ± 1.96*σp*√(1/n1+1/n2))

σp is the pooled population standard deviation

When σ1 and σ2 are unknown, then assumming t distribution of the sample mean, we are 95 % confident that 

μ1- μ2 = (X̄1-X̄2) ± t0.025, (n1+n2-2)*Sp*√(1/n1+1/n2)

Sp is the pooled sample standard deviation

When σ1 and σ2 are unknown, then we can calculate 

σ1^2/σ2^2

We can calculate difference in population proportion from sample proportions  with some confidence interval

For 95% confidence interval,

p1-p2 = (p̂1- p̂2)  ± 1.96*sqrt(p̂1*(1-p̂1)/n1+p̂2*(1-p̂2)/n2)",calcul popul paramet two sampl differ popul σ1 σ2 ,"['calcul', 'popul', 'paramet', 'two', 'sampl', 'differ', 'popul', 'σ1', 'σ2']",calculating population parameters for two samples from difference population  when σ1 and σ2 are kno
493,496,"Pooled variance is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same as Sp.

Sp^2 = [(n1-1)*S1^2+ (n2-1)*S2^2]/(n1+n2-2)",pool varianc method estim varianc sever differ pop,"['pool', 'varianc', 'method', 'estim', 'varianc', 'sever', 'differ', 'pop']",pooled variance is a method for estimating variance of several different populations when the mean o
494,497,"Confidence limits

Confidence limits are the values that mark the boundaries of the confidence interval.

95% confidence interval means 
the process we used will capture the true parameter 95% of the time in the long run

e.g.
 The statement, ""the 95% confidence interval for the population mean is (350, 400)"" means that 95% of the population values are between 350 and 400.

95% confidence level means the probability that the true value of the population parameter falls between the bounds of an already computed confidence interval is roughly 95%.",confid limit confid limit valu mark boundari confi,"['confid', 'limit', 'confid', 'limit', 'valu', 'mark', 'boundari', 'confi']",confidence limits  confidence limits are the values that mark the boundaries of the confidence inter
495,498,"Sampling schemes from best to worst

i. simple random, 
ii. stratified, 
iii. convenience (or close to hand sample)

>> A simple random sample randomly selects individuals from the population without any other consideration. 
>> A stratified random sample, on the other hand, first divides the population into smaller groups, or strata, based on shared characteristics.",sampl scheme best worst simpl random ii stratifi i,"['sampl', 'scheme', 'best', 'worst', 'simpl', 'random', 'ii', 'stratifi', 'i']","sampling schemes from best to worst  i. simple random,  ii. stratified,  iii. convenience (or close "
496,499,"Basics of Hypothesis Testing

> A hypothesis can be defined as a proposed explanation for a phenomenon. It is not the absolute truth but a provisional working assumption.
> In statistics, a hypothesis is considered to be a particular assumption about a set of parameters of a population distribution
> It is called a hypothesis because it is not known whether  it is true or not

A hypothesis test is a standard procedure for testing a claim about a property of a population.

A statement whose validity is tested on the basis of a sample is called Statistical Hypothesis",basic hypothesi test hypothesi defin propos explan,"['basic', 'hypothesi', 'test', 'hypothesi', 'defin', 'propos', 'explan']",basics of hypothesis testing  > a hypothesis can be defined as a proposed explanation for a phenomen
497,500,"Rare Event Rule for Inferential Statistics

If, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct",rare event rule inferenti statist given assumpt pr,"['rare', 'event', 'rule', 'inferenti', 'statist', 'given', 'assumpt', 'pr']","rare event rule for inferential statistics  if, under a given assumption, the probability of a parti"
498,501,"Components of a formal hypothesis test

> Given  a  claim,  identify  the  null  hypothesis  and  the  alternative hypothesis, and express them both in symbolic form
> Given a claim and sample data, calculate the value of the test statistic.
> Given a significance level, identify the critical value(s)
> Given a value of the test statistic, identify the P-value
> State  the  conclusion  of  a  hypothesis  test  in  simple,  non-technical terms",compon formal hypothesi test given claim identifi ,"['compon', 'formal', 'hypothesi', 'test', 'given', 'claim', 'identifi']","components of a formal hypothesis test  > given  a  claim,  identify  the  null  hypothesis  and  th"
499,502,"Null Hypothesis : Ho

The null hypothesis (denoted by Ho) is  a statement that the value of a population  parameter  (such  as  proportion,  mean,  or  standard deviation) is equal to or <=  or  >= some claimed value",null hypothesi ho null hypothesi denot ho statemen,"['null', 'hypothesi', 'ho', 'null', 'hypothesi', 'denot', 'ho', 'statemen']",null hypothesis : ho  the null hypothesis (denoted by ho) is  a statement that the value of a popula
500,503,"Alternative Hypothesis : HA

The alternative hypothesis (denoted by H1 or Ha or HA) is the statement that  the  statistic  has  a  value  that  somehow  differs  from  the  null hypothesis",altern hypothesi ha altern hypothesi denot h1 ha h,"['altern', 'hypothesi', 'ha', 'altern', 'hypothesi', 'denot', 'h1', 'ha', 'h']",alternative hypothesis : ha  the alternative hypothesis (denoted by h1 or ha or ha) is the statement
501,504,"Identifying the null and alternative hypothesis

If  we  are  conducting  a  study  and  want  to  use  a hypothesis test to support our claim, the claim must be worded so that it becomes the alternative hypothesis",identifi null altern hypothesi conduct studi want ,"['identifi', 'null', 'altern', 'hypothesi', 'conduct', 'studi', 'want']",identifying the null and alternative hypothesis  if  we  are  conducting  a  study  and  want  to  u
502,505,"Test Statistic

With the assumption that the null hypothesis is true, we find test statistic from the sample statistics and hypothesised statistics

i. Test statistic for proportions 

z_score= (p̂ -p)/sqrt(p*(1-p)/n)

ii. Test statistic for mean

z_score = (x̄ – μ) / (σ/√n)

or,
t_score = (x̄ – μ) / (S/√n)

for two samples,
t_score= (X̄1-X̄2) - (μ1- μ2)/Sp*√(1/n1+1/n2))

iii Test statistic for variance

chi_square_score= (n-1)*S^2/σ^2

for two samples, 
when, Ho=σ1^2=σ2^2,
f_score=S1^2/S2^2",test statist assumpt null hypothesi true find test,"['test', 'statist', 'assumpt', 'null', 'hypothesi', 'true', 'find', 'test']","test statistic  with the assumption that the null hypothesis is true, we find test statistic from th"
503,506,"Significance Level

The significance level (denoted by α) defines how much evidence we require to reject H0 in favor of HA

> significance level is the lowest probability of null hypothesis in the population

> α  is the probability  at test statistic.

confidence level=
1-significance level

For 95% confidence level, significance level is 0.05.

0.95= 1-0.05",signific level signific level denot α defin much e,"['signific', 'level', 'signific', 'level', 'denot', 'α', 'defin', 'much', 'e']",significance level  the significance level (denoted by α) defines how much evidence we require to re
504,507,"Critical Region

The critical region (or rejection region) is  a set of values for the test statistic for which the null hypothesis is rejected.",critic region critic region reject region set valu,"['critic', 'region', 'critic', 'region', 'reject', 'region', 'set', 'valu']",critical region  the critical region (or rejection region) is  a set of values for the test statisti
505,508,"Critical Value

A critical value is any value that separates the critical region  from the values of the test statistic that do not lead to rejection of the null hypothesis.  

The critical values depend on the nature of the null hypothesis, the sampling distribution that applies, and the significance level

The two‐tailed critical value will be larger than one tailed test for same significance level.

Z= +1.96 and Z= -1.96 are the critical values for 0.05 significance level in normal distribution",critic valu critic valu valu separ critic region v,"['critic', 'valu', 'critic', 'valu', 'valu', 'separ', 'critic', 'region', 'v']",critical value  a critical value is any value that separates the critical region  from the values of
506,509,"Two-tailed, Right-tailed, Left-tailed Tests

The tails in a distribution are the extreme regions bounded by critical values.

A two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater or less than a range of values
",twotail righttail lefttail test tail distribut ext,"['twotail', 'righttail', 'lefttail', 'test', 'tail', 'distribut', 'ext']","two-tailed, right-tailed, left-tailed tests  the tails in a distribution are the extreme regions bou"
507,510,"P-value or probability value

> The P-value (or probability value) is the probability of observing results as extreme or more extreme than currently
observed, given that the null hypothesis is true. 

The level of statistical significance is often expressed as a p-value between 0 and 1. The smaller the p-value, the stronger the evidence that we should reject the null hypothesis.

The null hypothesis is rejected if the P-value is very small, such as 0.05 or less.
> If a P-value is small enough, then we say the results are statistically significant

1. Ha ≠ Ho- It is Two-tailed test

if cdf(test_statistic)> 0.5:

p_value=2*( 
1- cdf(test_statistic))

else:

p_value=2*cdf(test_statistic)

2. Ha> Ho- It is Right-tailed test

p_value= 
1- cdf(test_statistic)

3. Ha< Ho- It is Left-tailed test

p_value= 
cdf(test_statistic)
",pvalu probabl valu pvalu probabl valu probabl obse,"['pvalu', 'probabl', 'valu', 'pvalu', 'probabl', 'valu', 'probabl', 'obse']",p-value or probability value  > the p-value (or probability value) is the probability of observing r
508,511,"Conclusions in Hypothesis Testing based on P-value

We always test the null hypothesis.  The initial conclusion will always be one of the following
i) Reject the null hypothesis - if the P-value ≤ α 
ii) Fail to reject the null hypothesis - if the P-value > α",conclus hypothesi test base pvalu alway test null ,"['conclus', 'hypothesi', 'test', 'base', 'pvalu', 'alway', 'test', 'null']",conclusions in hypothesis testing based on p-value  we always test the null hypothesis.  the initial
509,512,"Type - I error

A Type I error is the mistake of rejecting the null hypothesis when it is true.",type error type error mistak reject null hypothesi,"['type', 'error', 'type', 'error', 'mistak', 'reject', 'null', 'hypothesi']",type - i error  a type i error is the mistake of rejecting the null hypothesis when it is true.
510,513,"Type - II error

A Type II error is the mistake of failing to reject the null hypothesis when it is false.

β (Beta) is the probability of Type II error in any hypothesis test",type ii error type ii error mistak fail reject nul,"['type', 'ii', 'error', 'type', 'ii', 'error', 'mistak', 'fail', 'reject', 'nul']",type - ii error  a type ii error is the mistake of failing to reject the null hypothesis when it is 
511,514,"Power of a hypothesis test

The power of a hypothesis test is the probability (1 - beta) of rejecting a false null hypothesis (correct decision)

That is, the power of the hypothesis test is the probability of supporting an alternative hypothesis that is true.",power hypothesi test power hypothesi test probabl ,"['power', 'hypothesi', 'test', 'power', 'hypothesi', 'test', 'probabl']",power of a hypothesis test  the power of a hypothesis test is the probability (1 - beta) of rejectin
512,515,"encoding and decoding

Encoding means the creation of a messages and decoding means listener or audience of encoded message. So, decoding means interpreting the meaning of the message",encod decod encod mean creation messag decod mean ,"['encod', 'decod', 'encod', 'mean', 'creation', 'messag', 'decod', 'mean']",encoding and decoding  encoding means the creation of a messages and decoding means listener or audi
513,516,"> Quantum computing is a type of computation that harnesses the collective properties of quantum states, to perform calculations (massive computing power).",quantum comput type comput har collect properti qu,"['quantum', 'comput', 'type', 'comput', 'har', 'collect', 'properti', 'qu']",> quantum computing is a type of computation that harnesses the collective properties of quantum sta
514,517,"> Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. ",blockchain system record inform way make difficult,"['blockchain', 'system', 'record', 'inform', 'way', 'make', 'difficult']",> blockchain is a system of recording information in a way that makes it difficult or impossible to 
515,518," Father of Machine Learning: 
Geoffrey Everest Hinton (for ANN)
",father machin learn geoffrey everest hinton ann,"['father', 'machin', 'learn', 'geoffrey', 'everest', 'hinton', 'ann']", father of machine learning:  geoffrey everest hinton (for ann) 
516,519,Arthur Samuel coined the term “Machine Learning” in 1952.,arthur samuel coin term “machin learning” 1952,"['arthur', 'samuel', 'coin', 'term', '“machin', 'learning”', '1952']",arthur samuel coined the term “machine learning” in 1952.
517,520,"Degrees of Freedom

Degrees of Freedom refers to the number of logically independent values, which are values that have the freedom to vary, in the data sample. 
degrees of freedom= sample_size-1",degre freedom degre freedom refer number logic ind,"['degre', 'freedom', 'degre', 'freedom', 'refer', 'number', 'logic', 'ind']","degrees of freedom  degrees of freedom refers to the number of logically independent values, which a"
518,521,"Understanding computer science

Computer science means the knowledge of computer. With the knowledge of computer we can create different functions, models or environments in computer.

> Artificial intelligence is an important part of Computer science

> Machine learning is an important part of Artificial intelligence

> Deep learning is an important part of Machine learning",understand comput scienc comput scienc mean knowle,"['understand', 'comput', 'scienc', 'comput', 'scienc', 'mean', 'knowle']",understanding computer science  computer science means the knowledge of computer. with the knowledge
519,522,"Understanding convensional programming

Convensional programming is nothing but creating a function, model or environment based on different rules as per our understanding. Non-intelligent robot or collaborative robot is also an example of convensional programming.

",understand convension program convension program n,"['understand', 'convension', 'program', 'convension', 'program', 'n']","understanding convensional programming  convensional programming is nothing but creating a function,"
520,523,"Types of Computer languages

1. Low level language (binary language)-Machine language (First Generation, 1GL) and Assembly language (Second Generation, 2GL)
2. High Level language (based on low level language, human like language)-3GL(C,C++,Java), 4GL(Perl, Javascript, PHP, Python, Ruby, and SQL), 5GL (contains visual tools to help develop a program)",type comput languag 1 low level languag binari lan,"['type', 'comput', 'languag', '1', 'low', 'level', 'languag', 'binari', 'lan']",types of computer languages  1. low level language (binary language)-machine language (first generat
521,524,"Programming language (1GL,2GL,3GL) vs scripting language(4GL,5GL)

> Programming language is compiler based and scripting is interpreter based.
> Programming language is difficult to write but scripting language is easy to write and use
> Programming language does not require host, scripting requires host (any computer that has the capability of permitting access to a network)

> C++ programming language needs a compiler (g++)
> Code editor (notepad, Turbo C, VS Code) is needed for writing  code and make .cpp file. Then complier converts .cpp file to .exe file",program languag 1gl2gl3gl vs script language4gl5gl,"['program', 'languag', '1gl2gl3gl', 'vs', 'script', 'language4gl5gl']","programming language (1gl,2gl,3gl) vs scripting language(4gl,5gl)  > programming language is compile"
522,525,"AI technique

AI technique is nothing but implementing small fragments of human like intelligence in the model or environment created by convensional programming. Then the model or environment becomes intelligent model or environment.",artifici intellig techniqu artifici intellig techn,"['artifici', 'intellig', 'techniqu', 'artifici', 'intellig', 'techn']",artificial intelligence technique  artificial intelligence technique is nothing but implementing sma
523,526,"Basics of machine learning

The main feature of human intelligence is learning from the experiences. Thus an intelligent environment must have learning capability and machine learning algorithm serves this purpose to make the environment intelligent.
> Human or Machine learning model learns the rule or true function from a noisy realworld experience set.",basic machin learn main featur human intellig lear,"['basic', 'machin', 'learn', 'main', 'featur', 'human', 'intellig', 'lear']",basics of machine learning  the main feature of human intelligence is learning from the experiences.
524,527,"Basics of deep learning

If we need the environment to be intelligent enough to learn from the raw experiences from the sensors like image, audio, video etc., then we need to implement raw experience processor (a algorithm) to convert raw experiences into organised experience table and deep learning algorithm to learn the rules from the experience table.
",basic deep learn need environ intellig enough lear,"['basic', 'deep', 'learn', 'need', 'environ', 'intellig', 'enough', 'lear']",basics of deep learning  if we need the environment to be intelligent enough to learn from the raw e
525,528,"Basics of data science

> Data Science means the knowledge of data or reading the experiences

> Knowledge of data is required in the field of computer science, artificial intelligence, machine learning, deep learning and business management.",basic data scienc data scienc mean knowledg data r,"['basic', 'data', 'scienc', 'data', 'scienc', 'mean', 'knowledg', 'data', 'r']",basics of data science  > data science means the knowledge of data or reading the experiences  > kno
526,529,"Role of a data scientist

> A dataset is an experience set and a data scientist is able to understand the features (means Dharma) of the dataset and their relation.

> Thus, a data scientist can  decide the bestfit predictive ML model (algorithm) for a particular problem statement.  

> In simple words, a data scientist is like a teacher or guru who is expert in training and testing a model",role data scientist dataset experi set data scient,"['role', 'data', 'scientist', 'dataset', 'experi', 'set', 'data', 'scient']",role of a data scientist  > a dataset is an experience set and a data scientist is able to understan
527,530,"Difference between Business Analyst and Data Scientist

While a business analyst typically focuses on finding trends in data and developing ways to leverage that information to improve an organization's operations, data scientists tend to look more at what drives those trends.",differ busi analyst data scientist busi analyst ty,"['differ', 'busi', 'analyst', 'data', 'scientist', 'busi', 'analyst', 'ty']",difference between business analyst and data scientist  while a business analyst typically focuses o
528,531,"Predictive ML model

Predictive ML model is a machine learning algorithm which can predict the answer from a new problem statement on completion of training and testing of the model.",predict machin learn model predict machin learn mo,"['predict', 'machin', 'learn', 'model', 'predict', 'machin', 'learn', 'mo']",predictive machine learning model  predictive machine learning model is a machine learning algorithm
529,532,"Meaning of heuristic technique

A heuristic, or a heuristic technique, is any approach to problem-solving that uses a practical method or various shortcuts in order to produce solutions that may not be optimal but are sufficient given a limited timeframe or deadline.",mean heurist techniqu heurist heurist techniqu app,"['mean', 'heurist', 'techniqu', 'heurist', 'heurist', 'techniqu', 'app']","meaning of heuristic technique  a heuristic, or a heuristic technique, is any approach to problem-so"
530,533,"Comparison between Heuristic technique and ML technique

After understanding the dataset for a particular problem statement, a data scientist may decide a heuristic technique (rule based or conventional model) or ML technique (data based or ML model) for problem solving.

Rule based model can create an environment in which data based model can be fitted.",comparison heurist techniqu machin learn techniqu ,"['comparison', 'heurist', 'techniqu', 'machin', 'learn', 'techniqu']",comparison between heuristic technique and machine learning technique  after understanding the datas
531,534,"Types of learning models

1. Supervised learning model (Regression and classification)

2. Unsupervised learning model (Clustering)

3. Reinforcement learning model (Q-learning, policy learning)",type learn model 1 supervis learn model regress cl,"['type', 'learn', 'model', '1', 'supervis', 'learn', 'model', 'regress', 'cl']",types of learning models  1. supervised learning model (regression and classification)  2. unsupervi
532,535,"Application of regression model

> Real estate prediction

> Weather forecasting

> Financial porfolio prediction

> ETA (Estimated Time of Arrival) etc.",applic regress model real estat predict weather fo,"['applic', 'regress', 'model', 'real', 'estat', 'predict', 'weather', 'fo']",application of regression model  > real estate prediction  > weather forecasting  > financial porfol
533,536,"Application of classification model

> Credit card fraud detection

> Image classification

> Spam detection

> Insurance decisioning etc. ",applic classif model credit card fraud detect imag,"['applic', 'classif', 'model', 'credit', 'card', 'fraud', 'detect', 'imag']",application of classification model  > credit card fraud detection  > image classification  > spam d
534,537,"Application of Clustering model

> Document theme extraction

> Customer segmentation

> Insurance fraud detection

> Delivery store optimization etc. 

",applic cluster model document theme extract custom,"['applic', 'cluster', 'model', 'document', 'theme', 'extract', 'custom']",application of clustering model  > document theme extraction  > customer segmentation  > insurance f
535,538,"Components of reinforcement learning

> Environment
> Agent
> Action
> State and Reward

- It gathers its own experience from the environment and take action. A reward mechanism works inside and thus a reinforcement learning model learns from mistakes.",compon reinforc learn environ agent action state r,"['compon', 'reinforc', 'learn', 'environ', 'agent', 'action', 'state', 'r']",components of reinforcement learning  > environment > agent > action > state and reward  - it gather
536,539,"Application of Reinforcement learning model

> Traffic light control

> Resource management

> Robotics

> Games

> Bidding and advertisement etc.",applic reinforc learn model traffic light control ,"['applic', 'reinforc', 'learn', 'model', 'traffic', 'light', 'control']",application of reinforcement learning model  > traffic light control  > resource management  > robot
537,540,"Supervised, parametric, regression algorithm

> Linear regression is a supervised, parametric, regression algorithm. 
> If the organised experience set have answers or labels, then it is called a supervised experience.
> If the answers are continuous numerical values, then it is regression problem.
> The linear regression model consists of a predictor variable and a dependent variable related linearly to each other. 
> We try to find the relationship between independent variable(input) and a corresponding dependent variable (output)",supervis parametr regress algorithm linear regress,"['supervis', 'parametr', 'regress', 'algorithm', 'linear', 'regress']","supervised, parametric, regression algorithm  > linear regression is a supervised, parametric, regre"
538,541,"Basics of linear regression (OLS)

If the decesion or answer (dependent variable,Y) from an experience is linearly dependent on the dimensions (independent variables,X1,X2,X3,...Xn), then we can assume that the dependent variable is a function of all individual weights. Y=W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn+ E

Linear regression is also known as ordinary least squares (OLS) or linear least squares

> The term “least squares” is used because it is the smallest sum of squares of errors

The goal of linear regression is to create a trend line or best fit line

Best fit line is the spinal cord of whole experiences about which the pipeline weights are placed. It is also called the regression line.",basic linear regress ol deces answer depend variab,"['basic', 'linear', 'regress', 'ol', 'deces', 'answer', 'depend', 'variab']","basics of linear regression (ols)  if the decesion or answer (dependent variable,y) from an experien"
539,542,"Error or residuals

The errors between the predicted value and the actual value is called the error or residuals.

Reason of squaring the errors

Predicted value can have positive or negative error. If we don’t square the error, then the positive and negative points will cancel each other out during summation.  That is why we use SSE : Sum square error

SSE is also called Residual sum square (RSS)",error residu error predict valu actual valu call e,"['error', 'residu', 'error', 'predict', 'valu', 'actual', 'valu', 'call', 'e']",error or residuals  the errors between the predicted value and the actual value is called the error 
540,543,"Loss function or cost function

SSE= Σ(y-W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn)^2

SSE= Σ(y_test - y_pred)^2

In this equation, error of prediction for one observation is squared and then it is summed up for all the observations.

This equation is also called as loss function or cost function as it provides the cost of  loss from interpreting the data using a linear regression.
> Loss function is used for single observation and cost fuction is used for the entire dataset",loss function cost function sse σyw0w1x1w2x2w3x3wn,"['loss', 'function', 'cost', 'function', 'sse', 'σyw0w1x1w2x2w3x3wn']",loss function or cost function  sse= σ(y-w0+w1*x1+w2*x2+w3*x3+...+wn*xn)^2  sse= σ(y_test - y_pred)^
541,544,"Types of loss function

Mean Square Error (MSE) or Root Mean Square Error (RMSE) is used instead of SSE to save space and avoid memory explosion with large no. of observations
MSE= 1/n*Σ(y_test - y_pred)^2
RMSE = sqrt(MSE)
Other errors
Mean Absolute Error, Mean Absolute Percentage Error(MAPE), R2 (R – Squared), 
adjusted_r2 =1 - (((n-1)/(n-k-1))*(1-r2_score(actual, predicted)))
> Residual Standard Deviation or Residual Standard Error (RSE)
> RSE is calculated based on RSS and degrees of freedom
> MAPE= np.mean(np.abs((actual - predicted) / actual)) * 100)

> From the RMSE, we can understand the amount of the error in the prediction by our model

> From r2_score, we can understand the perfection of our model (R-Squared Higher the better)",type loss function mean squar error mse root mean ,"['type', 'loss', 'function', 'mean', 'squar', 'error', 'mse', 'root', 'mean']",types of loss function  mean square error (mse) or root mean square error (root mean squared error) 
542,545,"OLS method for finding out the model parameters

We differentiate MSE first with respect to β0(or W0) and then to β1(or W1) and βn(or Wn)

As per the rule of calculus, setting these partial derivatives equal to zero yields 'n' equation with 'n' unknowns to get the minimum values of β0, β1,....βn .

These equations are known as the normal equations.

Thus we can solve these 'n' equation with matrix method.",ordinari least squar method find model paramet dif,"['ordinari', 'least', 'squar', 'method', 'find', 'model', 'paramet', 'dif']",ordinary least squares method for finding out the model parameters  we differentiate mean squared er
543,546,"
Gradient Descent Fundamentals

> Gradient is the slope of cost function and Descent means moving downward

> Gradient Descent is an optimization algorithm which runs on repeated steps (iteration) starting with a random prediction. It helps machine learning models to find the values of the parameters for which the error in prediction will be minimum. It is applied when Parametric ML model is fitted on training data.

> OLS (Non-iterative process) provides exact solution but gradient decent provides approximate but good enough (decent)solution.

> Gradient descent requires additional inputs like alpha and intial random prediction

> Learning rate, α (hyper-parameter) is the step size of moving downward in gradient descent.

> GD is suitable for datasets with large no. of features and observations",gradient descent fundament gradient slope cost fun,"['gradient', 'descent', 'fundament', 'gradient', 'slope', 'cost', 'fun']", gradient descent fundamentals  > gradient is the slope of cost function and descent means moving do
544,548,"
Assumptions of regression

1.The relation between the dependent and independent variables should be almost linear.
2.Mean of residuals should be zero or close to 0 as much as possible. It is done to check whether our line is actually the line of “best fit” (running through the middle of the data).
3.There should be homoscedasticity or equal variance in a regression model. This assumption means that the variance around the regression line is the same for all values of the predictor variable (X).

4.There should not be multicollinearity in regression model. Multicollinearity generally occurs when there are high correlations between two or more independent variables.

> from the heatmap we can find the high positive or negative correlations between the independent variables",assumpt regress 1the relat depend independ variabl,"['assumpt', 'regress', '1the', 'relat', 'depend', 'independ', 'variabl']", assumptions of regression  1.the relation between the dependent and independent variables should be
545,549,"Multicollinearity issue

i.Remove some of the highly correlated independent variables.
ii.Linearly combine the independent variables, such as adding them together.
iii.Perform an analysis designed for highly correlated variables, such as principal components analysis",multicollinear issu iremov high correl independ va,"['multicollinear', 'issu', 'iremov', 'high', 'correl', 'independ', 'va']",multicollinearity issue  i.remove some of the highly correlated independent variables. ii.linearly c
546,550,"Heteroscedasticity issue

i.Log-transformation of features
ii.Outlier treatment
iii.Try polynomial fit
",heteroscedast issu ilogtransform featur iioutlier ,"['heteroscedast', 'issu', 'ilogtransform', 'featur', 'iioutlier']",heteroscedasticity issue  i.log-transformation of features ii.outlier treatment iii.try polynomial f
547,551,"Properties of regression line

Regression line passes through the mean of independent variable (x) as well as mean of the dependent variable (y).

This means that for mean of x, the actual y and predicted y will be same (zero error).",properti regress line regress line pass mean indep,"['properti', 'regress', 'line', 'regress', 'line', 'pass', 'mean', 'indep']",properties of regression line  regression line passes through the mean of independent variable (x) a
548,552,"Advantages of linear regression

1.Linear regression is simple to implement and easier to interpret the output coefficients

2.When we know the relationship between the independent and dependent variable is linear, this algorithm is the best to use because it’s less complex as compared to other algorithms

3.It works well irrespective of data size",advantag linear regress 1linear regress simpl impl,"['advantag', 'linear', 'regress', '1linear', 'regress', 'simpl', 'impl']",advantages of linear regression  1.linear regression is simple to implement and easier to interpret 
549,553,"Limitations of linear regression

1.Outliers can have huge effect on the regression line.

3.Prone to underfitting (or Overgeneralizing) - Linear regression sometimes fails to capture the underneath pattern in data properly due to simplicity of the algorithm.
",limit linear regress 1outlier huge effect regress ,"['limit', 'linear', 'regress', '1outlier', 'huge', 'effect', 'regress']",limitations of linear regression  1.outliers can have huge effect on the regression line.  3.prone t
550,554,"Data preparation for linear regression

1. Linear Assumption: We may need to log transform ""generally natural logarithm"" (on X's) for an exponential relationship-It makes our skewed original data more normal. It improves linearity between our dependent and independent variables. 

2. Gaussian Distributions: Linear regression will make more reliable predictions if our independent and dependent variables have a Gaussian distribution or normal distribution.

3. Remove Collinearity

After analyzing the dataset, we need to find the best possible independent variables (have highest possitive or highest negative correlation with y, means the most linear relations) to make any future predictions about our dependent variable

4. Remove Outlier
5. Rescale Inputs",data prepar linear regress 1 linear assumpt may ne,"['data', 'prepar', 'linear', 'regress', '1', 'linear', 'assumpt', 'may', 'ne']","data preparation for linear regression  1. linear assumption: we may need to log transform ""generall"
551,555,> PyTorch is a machine learning library developed and released by Facebook’s AI group (FAIR),pytorch machin learn librari develop releas facebo,"['pytorch', 'machin', 'learn', 'librari', 'develop', 'releas', 'facebo']",> pytorch is a machine learning library developed and released by facebook’s artificial intelligence
552,556,"Omission of relevant variable from a regression equation

If a relevant variable is omitted from a regression equation, the consequences would be

1. The standard errors would be biased
2. If the excluded variable is uncorrelated with all of the included variables, all of the slope and intercept coefficients will be inconsistent

Including relevant lagged values of the dependent variable on the right hand side of a regression equation could lead to 
Biased but consistent coefficient estimates",omiss relev variabl regress equat relev variabl om,"['omiss', 'relev', 'variabl', 'regress', 'equat', 'relev', 'variabl', 'om']",omission of relevant variable from a regression equation  if a relevant variable is omitted from a r
553,557,"Visualizing Linear Regression

We can visualize an experience as a pipeline which consists of indivisual straight pipelines in different dimension. As different dimension has different feature, we can assume that the diameter of pipeline is different for different dimension. Therefore weight per unit length is different for different dimension.

Weight per unit length is the feature or property of the dimension. Here it is known as coefficient of dimension (W1,W2,...Wn).

W1, W2,…Wn are also called model parameters.

Here we are considering material and thickness of the pipeline is same for all the dimensions. 

One experience row tells us about different values in deifferent dimensions. Here the values we are assuming as length. Therefore, to get the weight of one indivisual straight pipeline, we need to multiply the weight/length with the length. To get the total weight(y), we will sum up the individual weights in all dimension (W1*X1+W2*X2+ W3*X3+...+Wn*Xn).

Linear regression model learns the properties of all dimension from the experience set.

Intercept, W0 is nothing but a fixed weight of the pipeline which is independent of observations in different dimensions. This also called Bias term. 

Linear regression model also learns this parameter.

f(X)=W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn is the equation of total predicted weight of the pipeline.

X1, X2,…Xn are the feature values

For building a supervised learning model, the actual total weight of any experience pipeline is known for the train or test data.

The model initially starts with the prediction of any random total weight of the experience pipeline in the the gradient descent. Then, it starts minimizing the error between actual and prediction. Finally the model learns the properties of all dimensions where the total error for total weight prediction is minimum.",visual linear regress visual experi pipelin consis,"['visual', 'linear', 'regress', 'visual', 'experi', 'pipelin', 'consis']",visualizing linear regression  we can visualize an experience as a pipeline which consists of indivi
554,558,"Understanding of Feature scaling 

Feature scaling  must be performed to bring all data in certain range 

scikit-learn methods to preprocess data

-MinMaxScaler (min 0 and max  1)-a normalization technique 

(x-Min)/Range

-StandardScaler (mean 0 and standard deviation 1)-a standardization technique, 

(x-Mean)/Std. Dev

If required, we can perform MinMaxScaler operation after performing StandardScaler operation, when we want to see the data in same minimum (0) and maximum (1) range

> Unlike normalization, standardization does not have a bounding range.",understand featur scale featur scale must perform ,"['understand', 'featur', 'scale', 'featur', 'scale', 'must', 'perform']",understanding of feature scaling   feature scaling  must be performed to bring all data in certain r
555,559,"Difference bwteen Matrix and metric

Matrix is matrix while metric is a measure for something; a means of deriving a quantitative measurement or approximation for otherwise qualitative phenomena (especially used in software engineering)",differ bwteen matrix metric matrix matrix metric m,"['differ', 'bwteen', 'matrix', 'metric', 'matrix', 'matrix', 'metric', 'm']",difference bwteen matrix and metric  matrix is matrix while metric is a measure for something; a mea
556,560,"Libraries for linear regression

from numpy import math

from sklearn.preprocessing import PolynomialFeatures

from sklearn.preprocessing import MinMaxScaler

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error",librari linear regress numpi import math sklearnpr,"['librari', 'linear', 'regress', 'numpi', 'import', 'math', 'sklearnpr']",libraries for linear regression  from numpy import math  from sklearn.preprocessing import polynomia
557,561,"Importance of csv file

> csv format is used for importing and exporting data in different softwares.
> A csv file can be viewed through notepad, ms word, ms excel, google spreadsheet, sql, python etc.
> Thus a file which is required to be viewed through multiple different softwares, are saved in csv format
> Formats like xls, xlsx, json (JavaScript Object Notation) etc. are also used for import-export purpose",import csv file csv format use import export data ,"['import', 'csv', 'file', 'csv', 'format', 'use', 'import', 'export', 'data']",importance of csv file  > csv format is used for importing and exporting data in different softwares
558,562,"Implementation Steps of Linear Regression

1. Create dummy numerical variables for the catgeorical variable

> Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups. The dummy variables act like 'switches' that turn various parameters on and off in an equation.

dataset['NewYork_State'] = np.where(dataset['State']=='New York', 1, 0)

2. Create two separate dataset(numpy array) for independent variables and dependent variable

dependent_variable = 'Profit'
independent_variables = list(set(dataset.columns.tolist()) - {dependent_variable})

X = dataset[independent_variables].values
y = dataset[dependent_variable].values

3. Splitting both the datasets into the Training set and Test set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

4. Scaling and transforming both the training and test data

5. Training the ML model
regressor = LinearRegression()

regressor.fit(X_train, y_train)

6. Testing the ML Model
y_pred = regressor.predict(X_test)
7. Evaluating the ML Model
mean_squared_error(y_test, y_pred)

r2_score(y_test, y_pred)
> compare the prediction error both for train and test set
8. Explaining the model
regressor.coef_
regressor.intercept_",implement step linear regress 1 creat dummi numer ,"['implement', 'step', 'linear', 'regress', '1', 'creat', 'dummi', 'numer']",implementation steps of linear regression  1. create dummy numerical variables for the catgeorical v
559,563,"transform and fit_transform

>  Why we use fit_transform() on training data but transform() on the test data?

scaler= MinMaxScaler()

X_train_scaled=scaler.fit_transform(X_train)

X_test_scaled= scaler.transform(X_test)

.fit_transform() is the combined method for .fit() and .transform()

fit method calculates the minimum and maximum values for each of the features and transform method transforms all the features using the respective min and max.

Using the transform method we can use the same min and max (mean and variance for StandardScaler) as it is calculated from our training data to transform our test data.",transform fittransform use fittransform train data,"['transform', 'fittransform', 'use', 'fittransform', 'train', 'data']",transform and fit_transform  >  why we use fit_transform() on training data but transform() on the t
560,564,"Optimal Model

The learning model is called best fit model when the total error (Bias error+Variance error) in prediction on test data is optimal.

For the best performance of our ML models, there shall be great randomness in the selection of  experiences from the population. 

The bias or influence in the selection of experiences shall be optimal. 

Then the sample data will be a true representative of the population data.",optim model learn model call best fit model total ,"['optim', 'model', 'learn', 'model', 'call', 'best', 'fit', 'model', 'total']",optimal model  the learning model is called best fit model when the total error (bias error+variance
561,565,"Underfit Model

1. When the model is very simple it is called underfit model. Here the total error in prediction is high due to high bias and low variance.
2. Simple model is not able to catch proper signal from the experience. It learns very low weight per unit length for the features and learns a very high fixed weight or intercept value (bias)

> We can call them overgeneralizing model

3. As the weight/length in all the dimensions are small enough, there is a very little impact of the dimensions of the experience. Thus for different experience there will be very little variation in the answer or prediction. This is called low variance.",underfit model 1 model simpl call underfit model t,"['underfit', 'model', '1', 'model', 'simpl', 'call', 'underfit', 'model', 't']",underfit model  1. when the model is very simple it is called underfit model. here the total error i
562,566,"Overfit Model

1. When the model is very complex it is called overfit model. Here the total error in prediction is high due to high variance and low bias.

2.Complex model catches the signal along with noise (outliers or exceptions) from the experience. It provides overimportance to each dimension of the experience and learns very high weight/length for the dimensions of the experience. Thus for different experience there will be huge variation in the prediction. This is called high variance.
3. As the weight/length is very high for a complex model, the intercept value is very low. This is called low bias.",overfit model 1 model complex call overfit model t,"['overfit', 'model', '1', 'model', 'complex', 'call', 'overfit', 'model', 't']",overfit model  1. when the model is very complex it is called overfit model. here the total error in
563,567,"Understanding Estimator

Estimator is the rule or equation learned by the ML model which is a close approximation of the true function hidden or underlying in the experience set.

In other words, an estimator, models the relationship between independent and dependent variable

Thus an estimator is able to estimate or predict the dependent variable for a set of unseen independent variables",understand estim estim rule equat learn machin lea,"['understand', 'estim', 'estim', 'rule', 'equat', 'learn', 'machin', 'lea']",understanding estimator  estimator is the rule or equation learned by the machine learning model whi
564,568,"Bias and Variance of an Estimator or ML Model

Bias and Variance are statistical quantities

Bias and variance are used to analyse the performance of the model on unseen data

Total error in prediction=error due to Bias+error due to Variance

We try to keep the total error minimum for optimal model",bias varianc estim machin learn model bias varianc,"['bias', 'varianc', 'estim', 'machin', 'learn', 'model', 'bias', 'varianc']",bias and variance of an estimator or machine learning model  bias and variance are statistical quant
565,569,"Polynomial model 

In place of linear regression model we can choose polynomial model by following and train the model on training data

list_of_polynomial_degrees = [1, 3, 10]
theta = {}
fit = {}

theta[degree_of_polynomial] = np.polyfit(X_train, y_train, degree_of_polynomial)
 
To view the model parameters

fit[degree_of_polynomial] = np.polyval(theta[degree_of_polynomial], x_grid)
",polynomi model place linear regress model choos po,"['polynomi', 'model', 'place', 'linear', 'regress', 'model', 'choos', 'po']",polynomial model   in place of linear regression model we can choose polynomial model by following a
566,570,"Conversion of Categorical column to numerical

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
for k in categorical_columns:
  
X=my_df[k].values
  y = label_encoder.fit_transform(X)
  
name=k+'_new'
  my_df[name]=y

or, 
my_df = my_df.join(pd.get_dummies(my_df['column_name'],drop_first=True))",convers categor column numer sklearnpreprocess imp,"['convers', 'categor', 'column', 'numer', 'sklearnpreprocess', 'imp']",conversion of categorical column to numerical  from sklearn.preprocessing import labelencoder  label
567,571,"Multi Label columns to Binary

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer(sparse_output=True)

df = df.join(
            pd.DataFrame.sparse.from_spmatrix(
                mlb.fit_transform(df.pop('genres_list')),
                index=df.index,
                columns=mlb.classes_))
> One topic_combination column to all individual topic columns",multi label column binari sklearnpreprocess import,"['multi', 'label', 'column', 'binari', 'sklearnpreprocess', 'import']",multi label columns to binary  from sklearn.preprocessing import multilabelbinarizer  mlb = multilab
568,572,"Number-String to numerical value

my_df[list_of columns]=my_df[list_of columns].astype('float64')

or

pd.to_numeric(my_df[list_of columns])",numberstr numer valu mydflistof columnsmydflistof ,"['numberstr', 'numer', 'valu', 'mydflistof', 'columnsmydflistof']",number-string to numerical value  my_df[list_of columns]=my_df[list_of columns].astype(float64)  or 
569,573,"Noise, Underfittiing, Overfitting and Overgeneralizing 

> Underfitting means giving less important to the features

> Overfitting means giving more importance to the features

> Overgeneralizing means giving more importance to the bias

> There is no noise-free or pure data (experience) in the world, because every data or experience is influenced by some surrounding factors. 

> So, larger the dataset, better will be the understanding of noise-free trend by the learning model. This is called optimal model 

> For smaller dataset, learning model can be simple (looking at small no. of features) or complex (looking at large no. of features). 

> With the increase in training data, Bias increases and Variance decreases for an overfitting model",nois underfitti overfit overgener underfit mean gi,"['nois', 'underfitti', 'overfit', 'overgener', 'underfit', 'mean', 'gi']","noise, underfittiing, overfitting and overgeneralizing   > underfitting means giving less important "
570,574,"Basics of Regularized Linear Regression

Regularized linear regression is the first option for linear regression problem on small dataset (<1lakh experiences)

When we fit normal linear regression model in our training data with large no. of features, then there is a chance that the intercept value (bias) is very low and other model coeffients (variances) are high enough, then our model will become a complex model.

Regularized Linear Regression reduces the chance of model complexity

When we fit regularized linear regression with perfect tuning parameter λ, it increases the intercept value and reduces other model parameters towards zero. 

λ is denoted by alpha(α) in sklearn",basic regular linear regress regular linear regres,"['basic', 'regular', 'linear', 'regress', 'regular', 'linear', 'regres']",basics of regularized linear regression  regularized linear regression is the first option for linea
571,575,"Types of Regularization

i. Ridge Regression

ii. Lasso Regression 

Regularized Liear Regression minimizes the sum of RSS and a ""penalty term""

Therefore 
f(model parameters)= RSS+penalty term
is the cost function for Regularized LR model

Penalty term has a tuning parameter lambda(λ) multiplied with model coefficients. ",type regular ridg regress ii lasso regress regular,"['type', 'regular', 'ridg', 'regress', 'ii', 'lasso', 'regress', 'regular']",types of regularization  i. ridge regression  ii. lasso regression   regularized liear regression mi
572,576,"Ridge Regression (L2 Regularization)

Ridge Regression (L2 Regularization)-Penalty term has sum of model coefficient Square

> It has closed form solution (fintie no. of operations)

A ridge is a long, narrow, elevated strip of land.

Ridge regression shrinks coefficients toward zero, but they rarely reach zero.
",ridg regress l2 regular ridg regress l2 regulariza,"['ridg', 'regress', 'l2', 'regular', 'ridg', 'regress', 'l2', 'regulariza']",ridge regression (l2 regularization)  ridge regression (l2 regularization)-penalty term has sum of m
573,577,"Lasso Regression (L1 Regularization)

Lasso Regression (L1 Regularization)-Penalty term has sum of model coefficient Modulus

Lasso stands for least absolute shrinkage and selection operator. 

Lasso regression shrinks coefficients all the way to zero, thus removing few features from the model.

It is best to try both regularization and see which one works better. Usually L2 regularization can be expected to give superior performance over L1.

There's also a ElasticNet regression, which is a combination of Lasso regression and Ridge regression.

Lasso regression is preferred if we want a sparse model, meaning that we believe many features are irrelevant to the output.",lasso regress l1 regular lasso regress l1 regulari,"['lasso', 'regress', 'l1', 'regular', 'lasso', 'regress', 'l1', 'regulari']",lasso regression (l1 regularization)  lasso regression (l1 regularization)-penalty term has sum of m
574,578,"Libraries for Regularized Linear Regression

from sklearn.datasets import load_boston

The Boston housing dataset is a famous dataset from the 1970s. It contains 506 observations and 13 features on housing prices around Boston.

from sklearn.linear_model import Ridge, 
RidgeCV

from sklearn.linear_model import Lasso, LassoCV

from sklearn.preprocessing import StandardScaler
",librari regular linear regress sklearndataset impo,"['librari', 'regular', 'linear', 'regress', 'sklearndataset', 'impo']",libraries for regularized linear regression  from sklearn.datasets import load_boston  the boston ho
575,579,"Alpha value

my_model=Ridge(alpha=0.1, fit_intercept=True)

my_model=Lasso(alpha=0.1 , max_iter= 3000)

For a very small λ or alpha (near zero), Regularized LR model will act like a normal LR model

For a very large λ or alpha, large coefficients are not penalized 

> normal range of alpha is between 0 and 0.1

In Lasso and Ridge regression as alpha value increases, the slope of the regression line reduces and becomes horizontal (means only have bias, no slope or weight).",alpha valu mymodelridgealpha01 fitintercepttru mym,"['alpha', 'valu', 'mymodelridgealpha01', 'fitintercepttru', 'mym']","alpha value  my_model=ridge(alpha=0.1, fit_intercept=true)  my_model=lasso(alpha=0.1 , max_iter= 300"
576,580,"Variance Inflation Factor (VIF) 

It quantifies the severity of multicollinearity

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif[""variables""] = X.columns
    vif[""VIF""] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

calc_vif(dataset[[i for i in dataset.describe().columns if i not in ['car_ID','price']]])

We need to bring the VIF for the final independent numerical feature <10, to get rid of multicolinearity.

By repeated checking of VIF and  heatmap, we remove the independent variables with multicolinearity",varianc inflat factor varianc inflat factor quanti,"['varianc', 'inflat', 'factor', 'varianc', 'inflat', 'factor', 'quanti']",variance inflation factor (variance inflation factor)   it quantifies the severity of multicollinear
577,581,"Cross validation Basics

Cross validation adds more randomness in the selection of training and testing experience set by subseting the main dataset, to make our model more accurate in prediction. 

Thus helps us to avoid overfitting.",cross valid basic cross valid add random select tr,"['cross', 'valid', 'basic', 'cross', 'valid', 'add', 'random', 'select', 'tr']",cross validation basics  cross validation adds more randomness in the selection of training and test
578,582,"Simple Validation vs Cross Validation

> In simple validation we divide the data into two pieces.  80% for training and 20% for testing

> In Cross validation, we divide the data into 'k' pieces or buckets (files) and randomly select any one piece as testing data and the combined rest pieces as training data",simpl valid vs cross valid simpl valid divid data ,"['simpl', 'valid', 'vs', 'cross', 'valid', 'simpl', 'valid', 'divid', 'data']",simple validation vs cross validation  > in simple validation we divide the data into two pieces.  8
579,583," k-fold CV

Cross validation is also known as k-fold CV. In k-fold cross validation, we have multiple(k) train-test sets instead of 1. 
>> This basically means that in a k-fold CV we will be training our model k-times and also testing it k-times. (like multiple unit tests in a class)
>> Then, we check the cross_validation_score for the selected ML model
> Other than k-fold there is stratified Cross Validation (Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label)
> If K=N, then it is called Leave one out cross validation, where N is the number of observations.",kfold cross valid cross valid also known kfold cro,"['kfold', 'cross', 'valid', 'cross', 'valid', 'also', 'known', 'kfold', 'cro']", k-fold cross validation  cross validation is also known as k-fold cross validation. in k-fold cross
580,584,"Python coding for CV

from sklearn.model_selection import cross_val_score

results = cross_val_score(my_model,X_test,y_test, cv = 3)

print(f'[""accuracy"", ""precision"", ""recall""]: {results}')
> classification_report provides simple validation report whereas cross_val_score provides cross validation report

> For K-cross validation ,smaller k implies less variance (because cross validation decreases complexity but larger cross validation again increases complexity).",python code cross valid sklearnmodelselect import ,"['python', 'code', 'cross', 'valid', 'sklearnmodelselect', 'import']",python coding for cross validation  from sklearn.model_selection import cross_val_score  results = c
581,585,"yellowbrick CVScores

The results from each evaluation are averaged together for a final score, then the final model is fit on the entire dataset for operationalization

!pip install yellowbrick

from yellowbrick.model_selection import CVScores

visualizer = CVScores(model, cv=3, scoring='f1_weighted')

visualizer.fit(X, y)        
visualizer.show()",yellowbrick cvscore result evalu averag togeth fin,"['yellowbrick', 'cvscore', 'result', 'evalu', 'averag', 'togeth', 'fin']","yellowbrick cvscores  the results from each evaluation are averaged together for a final score, then"
582,586,"Fundamentals of hyperparameters

The parameters apart from model parameters whose values are used to control the learning process are called hyperparameters.

Examples:
n_iter
test_size
max_depth
random_state
n_neighbors
alpha
C
gamma
n_components
karnel
metric
n_folds
penalty
cv

> random_state hyperparameter of sklearn does similar thing as seed function",fundament hyperparamet paramet apart model paramet,"['fundament', 'hyperparamet', 'paramet', 'apart', 'model', 'paramet']",fundamentals of hyperparameters  the parameters apart from model parameters whose values are used to
583,587,"Hyperparameters tuning

GridSearchCV is a method to tune the hyperparameters

It trains the model using different combinations and gives the best combination based on the best k-fold cv score obtained 

GridSearch is very slow (alternate options may be RandomSearchCV or  Bayesian Hyperparameter Optimization)",hyperparamet tune gridsearchcross valid method tun,"['hyperparamet', 'tune', 'gridsearchcross', 'valid', 'method', 'tun']",hyperparameters tuning  gridsearchcross validation is a method to tune the hyperparameters  it train
584,588,"Coding for Hyperparameters tuning

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
parameters = {'model_param': list_of_values}
cross_validated_model = 
GridSearchCV(my_model, param_grid=parameters, scoring='r2', cv=3)
cross_validated_model.fit(X_train,y_train)
cross_validated_model.best_params_
cross_validated_model.best_score_
y_preds = cross_validated_model.predict(X_test)",code hyperparamet tune sklearnmodelselect import g,"['code', 'hyperparamet', 'tune', 'sklearnmodelselect', 'import', 'g']",coding for hyperparameters tuning  from sklearn.model_selection import gridsearchcv from sklearn.mod
585,589,"Steps of ML modelling

Step-1: Detailed commenting on problem statement at the begining of the code

Step-2: Installing latest libraries with 
pip install 
if required

Step-3: Importing necessary libraries for using readymade functions

Step-4: Data server connection with notebook 
Step-5: Data importing or reading as pandas dataframe

Step-6: Data inspection with pandas methods and functions

Step-7: Initial Data Preparation/Wrangling/ cleaning and saving the file as different csv
Data cleaning activities include:
> Dealing with missing values
> Dealing with outliers
> Correcting typos
> Grouping sparse classes
> Dropping duplicates
Step-8: Initial data exploration with pandas profiling 

Step-9: Final Data Preparation/wrangling  with transformation

Data Transformation activities and techniques include:

> Categorical encoding
> Dealing with skewed data
> Scaling
> Bias mitigation
> Rank transformation
> Power functions

Step-10: Feature Engineering by creating new feature based upon knowledge about current features and required task

Step-11: Final data exploration through data visualization tools

Step-12: Correct ML model selection or solution of problem statement from the understanding of data
 
Step-13: Train-Test spliting of the data

Step-14: Training the ML model for learning the pattern of the data on fitting training experience set


Step-15: Testing the ML model for evaluating its understanding of the trend on fitting testing experience set

Step-16: Improving the ML model through Cross validation and hyperparameter tuning

Step-17: Retraining and Retesting the model for further evaluation

Step-18: Checking the performance of the model in realtime deployment",step machin learn model step1 detail comment probl,"['step', 'machin', 'learn', 'model', 'step1', 'detail', 'comment', 'probl']",steps of machine learning modelling  step-1: detailed commenting on problem statement at the beginin
586,590,"Understanding Warnings

Warnings are provided to warn the developer of situations that aren’t necessarily exceptions. Usually, a warning occurs when there is some obsolete of certain programming elements, such as keyword, function or class, etc. A warning in a program is distinct from an error. Python program terminates immediately if an error occurs. Conversely, a warning is not critical.

import warnings

warnings.filterwarnings('ignore')",understand warn warn provid warn develop situat ar,"['understand', 'warn', 'warn', 'provid', 'warn', 'develop', 'situat', 'ar']",understanding warnings  warnings are provided to warn the developer of situations that aren’t necess
587,591,"Data preparation or data preprocessing

Data preparation (also referred to as “data preprocessing”) is the process of transforming raw data so that data scientists and analysts can run it through machine learning algorithms to uncover insights or make predictions.

Cross-validation gives a more accurate measure of model quality, which is especially important if we are making a lot of modeling decisions.

Before applying any transformation, we need to perform EDA to understand the shape of the distribution

Seaborn distplot or histogram plot may used to understand the probability distribution

To see all the shape of the data in a single graph, we can use pairplot and specify the kind of non-diagonal graphs, kind of diagonal graph and hue for target_feature",data prepar data preprocess data prepar also refer,"['data', 'prepar', 'data', 'preprocess', 'data', 'prepar', 'also', 'refer']",data preparation or data preprocessing  data preparation (also referred to as “data preprocessing”) 
588,592,"Linear Transformation or Scaling

MinMaxScaler or StandardScaler are the linear transformations.

These are applied when dependent variable linearly varies with independent variable

For MinMax scaling:

X_new= (X- min(X)) /(Max(X)-Min(X))

For Standard scaling:

X_new= (X- mean(X)) / Std(X)",linear transform scale minmaxscal standardscal lin,"['linear', 'transform', 'scale', 'minmaxscal', 'standardscal', 'lin']",linear transformation or scaling  minmaxscaler or standardscaler are the linear transformations.  th
589,593,"Non-linear Transformations (for dealing with Sckewed data)

> square-root for moderate skew: sqrt(x) for positively skewed data, sqrt(max(x+1) - x) for negatively skewed data
> log for greater skew: log10(x) for positively skewed data, log10(max(x+1) - x) for negatively skewed data
np.log10(my_df['column_name']

> inverse for severe skew: 1/x for positively skewed data 1/(max(x+1) - x) for negatively skewed data
np.reciprocal(my_df['column_name']
> After the transformation, the distribution becomes more approximate to normal distribution.

> After taking log, zero values in some of columns are transformed with -Inf 
np.isinf(df[col_name]).values.sum()
> Normalization is good to use when we know that the distribution of our data does not follow a Gaussian distribution.

Linearity and heteroscedasticity:

First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable values 

If our data does the opposite – dependent variable values decrease more rapidly with increasing independent variable values – we can first consider a square-root transformation.",nonlinear transform deal sckew data squareroot mod,"['nonlinear', 'transform', 'deal', 'sckew', 'data', 'squareroot', 'mod']",non-linear transformations (for dealing with sckewed data)  > square-root for moderate skew: sqrt(x)
590,594,"Basics of Logistic regression 

Logistic regression  is the first option for linear classification problem on small dataset (<1lakh experiences)

Logistic regression is a supervised, parametric, classification algorithm (non-linear regression technique). 

Why it is called regression?

In both the Logistic regression and Linear regression, the predicted value of y, either continuously increases or decreases with the increase in independent variable.

But in Logistic Regression, the continuous target values are only a limited number (generally forced to give two target values)

>> Logistic regression does not need variables to be normally distributed for good performance",basic logist regress logist regress first option l,"['basic', 'logist', 'regress', 'logist', 'regress', 'first', 'option', 'l']",basics of logistic regression   logistic regression  is the first option for linear classification p
591,595,"Meaning of odds and logit function in probability

The odds are defined as the probability that the event will occur, P divided by the probability that the event will not occur, 1-P

Here, we are considering the probability of dependent variable,Y as P

Odds of event, Y =
P/(1-P)

Logit of event, Y= 
Log-odds= Log(P/(1-P))

Y= Log(P/(1-P)) is called the Logit fuction",mean odd logit function probabl odd defin probabl ,"['mean', 'odd', 'logit', 'function', 'probabl', 'odd', 'defin', 'probabl']",meaning of odds and logit function in probability  the odds are defined as the probability that the 
592,596,"Logistic function or sigmoid function

In linear regression, the value of event, Y is a linear equation. 
Value of event, Y= w0+w1*x1+w2*x2+…wn*xn

In logistic regression, the logit of event, Y is a linear equation.
Logit of event, Y= w0+w1*x1+w2*x2+…wn*xn
or,
Log(P(Y)/(1-P(Y)))=w0+w1*x1+w2*x2+…wn*xn- This is called the logistic function or sigmoid function

Log(p/1-p)=W.T*x (By convention of we can assume that  x0=1)",logist function sigmoid function linear regress va,"['logist', 'function', 'sigmoid', 'function', 'linear', 'regress', 'va']","logistic function or sigmoid function  in linear regression, the value of event, y is a linear equat"
593,597,"Working of parametric model

Parametric algorithms involve two steps when ml_model_name.fit method is applied on the training data:

1. It selects a form for the function as called.

2. Learns the model parameters for the function by running a gradient descent (or maximum likelihood estimator) algorithm to make the total error minimum.",work parametr model parametr algorithm involv two ,"['work', 'parametr', 'model', 'parametr', 'algorithm', 'involv', 'two']",working of parametric model  parametric algorithms involve two steps when ml_model_name.fit method i
594,598,"Benefits of Parametric ML models:

1. Simpler: These methods are easier to understand and interpret results.

2. Speed: Parametric models are very fast to learn from data.

3. Less Data: They do not require large training data ",benefit parametr machin learn model 1 simpler meth,"['benefit', 'parametr', 'machin', 'learn', 'model', '1', 'simpler', 'meth']",benefits of parametric machine learning models:  1. simpler: these methods are easier to understand 
595,599,"Limitations of Parametric ML Models:

1. Constrained: By choosing a functional form these methods are highly constrained to the specified form.
2. Limited Complexity: The methods are more suited to simpler problems.
3. Poor Fit: A real data never perfectly follow a true function, but a data scientist call a ML function based on final EDA. Thus, the methods are unlikely to perfectly match the underlying  trend in the training data.

> Neural network is an expection",limit parametr machin learn model 1 constrain choo,"['limit', 'parametr', 'machin', 'learn', 'model', '1', 'constrain', 'choo']",limitations of parametric machine learning models:  1. constrained: by choosing a functional form th
596,600,"Working of Non-parametric model 

Non-parametric algorithm does the following step when ml_model_name.fit method is applied on the training data:

It selects a method as called to best fit the training data in constructing the mapping function.",work nonparametr model nonparametr algorithm follo,"['work', 'nonparametr', 'model', 'nonparametr', 'algorithm', 'follo']",working of non-parametric model   non-parametric algorithm does the following step when ml_model_nam
597,601,"Benefits of Nonparametric Machine Learning Algorithms:

1. Flexibility: Capable of fitting a large number of functional forms.
2. Power: No assumptions (or weak assumptions) about the underlying function.
3. Performance: Can result in higher performance models for prediction.",benefit nonparametr machin learn algorithm 1 flexi,"['benefit', 'nonparametr', 'machin', 'learn', 'algorithm', '1', 'flexi']",benefits of nonparametric machine learning algorithms:  1. flexibility: capable of fitting a large n
598,602,"Limitations of Nonparametric Machine Learning Algorithms:

1. More data: Require a lot more training data to estimate the mapping function.
2. Slower: A lot slower to train as they often have far more parameters to train.
3. Overfitting: More of a risk to overfit the training data",limit nonparametr machin learn algorithm 1 data re,"['limit', 'nonparametr', 'machin', 'learn', 'algorithm', '1', 'data', 're']",limitations of nonparametric machine learning algorithms:  1. more data: require a lot more training
599,603,"Classification model and probability

A probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to.

It is possible for every probabilistic method to simply return the class with the highest probability and therefore seem deterministic.",classif model probabl probabilist classifi classif,"['classif', 'model', 'probabl', 'probabilist', 'classifi', 'classif']",classification model and probability  a probabilistic classifier is a classifier that is able to pre
600,604,"Generative models  and Discriminative models 

Generative models (learns joint probability, p(x,y) and outputs conditional probability, p(y|x))-able to predict for unseen data point:
1. Linear Regression
2. Naive Bayes
3. Hidden Markov Models

 Deterministic or Discriminative models (learns conditional probability, p(y|x) and outputs conditional probability, p(y|x))-Estimate parameters directly from training data.:
1. Logistic Regression
2. KNN
3. Decision Tree
4. Neural Network (can also become generative)
5. SVM (Non-Probabilistic-does not give the probability for prediction)",generat model discrimin model generat model learn ,"['generat', 'model', 'discrimin', 'model', 'generat', 'model', 'learn']","generative models  and discriminative models   generative models (learns joint probability, p(x,y) a"
601,605,"Libraries for Logistic regression 

from sklearn import metrics

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix

from sklearn.metrics import f1_score

from sklearn.metrics import log_loss
>> A good model should have a smaller log loss value (can never be negative).
>> large log-likelihood statistic (parallel to F-test in OLS regression) indicates that the statistical model is a poor fit of the data.
>> sklearn provides max 3 classes
>> Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value ",librari logist regress sklearn import metric sklea,"['librari', 'logist', 'regress', 'sklearn', 'import', 'metric', 'sklea']",libraries for logistic regression   from sklearn import metrics  from sklearn.linear_model import lo
602,606,"Training Logistic regression model

my_logistic_regress_mdl = LogisticRegression(fit_intercept=True, max_iter=10000)

my_logistic_regress_mdl.fit(X_train, y_train)

my_logistic_regress_mdl.coef_

my_logistic_regress_mdl.intercept_",train logist regress model mylogisticregressmdl lo,"['train', 'logist', 'regress', 'model', 'mylogisticregressmdl', 'lo']","training logistic regression model  my_logistic_regress_mdl = logisticregression(fit_intercept=true,"
603,607,"Evaluating the performance of Logistic regression model

train_accuracy = accuracy_score(y_pred_train,y_train)

test_accuracy = accuracy_score(y_pred_test,y_test)

my_cm = confusion_matrix(y_train, y_pred_train)

sns.heatmap(my_cm, vmin=0, cmap='Greens', annot=True)

f1_score(y_test, y_pred_test, average='macro')",evalu perform logist regress model trainaccuraci a,"['evalu', 'perform', 'logist', 'regress', 'model', 'trainaccuraci', 'a']",evaluating the performance of logistic regression model  train_accuracy = accuracy_score(y_pred_trai
604,608,"Confusion Matrix

Also known as error matrix

It is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one

In unsupervised learning it is usually called a matching matrix",confus matrix also known error matrix specif tabl ,"['confus', 'matrix', 'also', 'known', 'error', 'matrix', 'specif', 'tabl']",confusion matrix  also known as error matrix  it is a specific table layout that allows visualizatio
605,609,"Evaluation through confusion matrix

Confusion matrix finds, in how many cases our model is  correct or incorrect in predicting the target class (TP,TN,FP,FN). 

Thus we can interpret the performance of the model more easily.

Many times, accuracy of the model is not enough to judge the performace, then we also need to check precision and recall
> For regression model evaluation, there is RMSE, MSE, R2 etc.

condition positive (P):
the number of real positive cases in the data
condition negative (N):
the number of real negative cases in the data

true positive (TP)
eqv. with hit
true negative (TN)
eqv. with correct rejection
false positive (FP)
eqv. with false alarm, type I error or underestimation
false negative (FN)
eqv. with miss, type II error or overestimation",evalu confus matrix confus matrix find mani case m,"['evalu', 'confus', 'matrix', 'confus', 'matrix', 'find', 'mani', 'case', 'm']","evaluation through confusion matrix  confusion matrix finds, in how many cases our model is  correct"
606,611,"Accuracy, Precision, Recall and F1-Score

Accuracy= (TP+TN)/(TP+TN+FP+FN)

Precision= TP/(TP+FP)

Recall (sensitivity or hit rate)= TP/(TP+FN)

F1-Score 
This is defined as the harmonic mean of precision and recall.

F1=2*Precision*Recall/(Precision+Recall)",accuraci precis recal f1score accuraci tptntptnfpf,"['accuraci', 'precis', 'recal', 'f1score', 'accuraci', 'tptntptnfpf']","accuracy, precision, recall and f1-score  accuracy= (tp+tn)/(tp+tn+fp+fn)  precision= tp/(tp+fp)  re"
607,612,"Importance of F1 Score

> F1 score penalizes the extreme values.
> Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial
> Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.
> In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric for evaluation.",import f1 score f1 score penal extrem valu accurac,"['import', 'f1', 'score', 'f1', 'score', 'penal', 'extrem', 'valu', 'accurac']",importance of f1 score  > f1 score penalizes the extreme values. > accuracy is used when the true po
608,613,"Checking Cross-validation scores

from sklearn.linear_model import LogisticRegressionCV

from sklearn.model_selection import cross_validate

The cross_validate function differs from cross_val_score in two ways: It allows specifying multiple metrics for evaluation. It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.",check crossvalid score sklearnlinearmodel import l,"['check', 'crossvalid', 'score', 'sklearnlinearmodel', 'import', 'l']",checking cross-validation scores  from sklearn.linear_model import logisticregressioncv  from sklear
609,614,"Types of ML models:

1. For regression problems (supervised)
1A. Linear Regressors

i. Ridge,Lasso or ElasticNet (parametric)
ii. SVM (parametric)
iii. Neural Network-SGD (parametric)

1B. Non-linear Regressors
i. Ensembles of Decision Trees (non-parametric)
2. For Classification problems (supervised)
2A. Linear Classifers
i. SVM (parametric)
ii. Neural Network-SGD (parametric)

2B. Non-linear Classifiers

i. Logistic regression (parametric)-rarely used
ii. KNN (non-parametric)
iii. Naive Bayes (parametric)
iv. Ensembles of Decision trees (non-parametric)
v. Neural Network-SGD+Kernel approximation (parametric)

3. For Clustering problems (unsupervised)

i. K-Means Clustering (non-parametric)
ii. DBSCAN (non-parametric)
iii. Gaussian Mixture Model (GMM) (parametric)
iv. Hierarchical Clustering

Non-linear classifiers are the most important one among all groups of ML models. 

In real life, classifying a large experience set with large number of features is the most important one. 

And the underlying rule of this classification never follow a true function. 

Thus a non-linear classifier is employed.",type machin learn model 1 regress problem supervis,"['type', 'machin', 'learn', 'model', '1', 'regress', 'problem', 'supervis']",types of machine learning models:  1. for regression problems (supervised) 1a. linear regressors  i.
610,615,"Important Terminology in Decision Tree ML Model

> Root node: It represents entire population or sample and this further gets divided into two sets.

> Decision node: When a sub-node splits into further sub-nodes, then it is called decision node.

> Leaf/Terminal node: Nodes do not split is called leaf or terminal node.

> Splitting: It is a process of dividing a node into two sub-nodes.

> Branch or Sub-tree: A sub section of entire tree is called branch or sub-tree.

> Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes whereas sub-nodes are the child of parent node.

> Pruning: When we remove sub-nodes of a decision node, this process is called pruning. we can say opposite process of splitting.",import terminolog decis tree machin learn model ro,"['import', 'terminolog', 'decis', 'tree', 'machin', 'learn', 'model', 'ro']",important terminology in decision tree machine learning model  > root node: it represents entire pop
611,616,"Steps of Decision tree algorithm

1. It takes the entire dataset as root node and finds the most important feature for the decision of classification of experiences and splits the whole sample dataset into two or more datasets (sub-node or child node)

2. Then each subset of data is further splitted in two or more sebsets, on the basis of second important feature.

 3. The process of splitting goes on till the algorithm reach leaf or terminal node of most similar class of experiences. 
 
4. Thus the algorithm learns a treelike sequence and criteria of decisions from the training data.

5. Then the model is fitted with the testing experience set to check the performance scores.

6. When we give one real time experience to the decision tree model, it checks the decision criterias in learned sequence and shows the class for the experience.

Decision tree is also referred to as 
Recursive (recurrence or repetition) partitioning",step decis tree algorithm 1 take entir dataset roo,"['step', 'decis', 'tree', 'algorithm', '1', 'take', 'entir', 'dataset', 'roo']",steps of decision tree algorithm  1. it takes the entire dataset as root node and finds the most imp
612,617,"Methods to measure the similarity of child nodes:

How a decision tree model finds the most important feature for the decision of spliting parent node into child nodes? 

Decision tree splits the parent node on all available features and then selects the split which results in most homogeneous or similar child nodes.

> Following methods are called splitting criterion in the syntax

Information Gain (criterion='entropy' in sklearn)

Gini (criterion='gini' in sklearn is the default criterion)

Chi Square (required to write the code)

Reduction in Variance (required to write the code)-used for continuous target variables that are used for regression problems

Gain ratio

>> Decision node illustrates 
Test specification in a decision tree

>> The entropy of a node decreases as we go down a decision tree. 

>>  Gini-Index and Entropy refer to ""Purity"" of our data points

Gini coefficient, also the Gini index and the Gini ratio, is a measure of statistical dispersion intended to represent the income inequality or the wealth inequality within a nation or a social group.

Gini index = 1 signifies that all the elements are randomly distributed across various classes, and. Gini index= 0.5 denotes the elements are uniformly distributed into some classes.",method measur similar child node decis tree model ,"['method', 'measur', 'similar', 'child', 'node', 'decis', 'tree', 'model']",methods to measure the similarity of child nodes:  how a decision tree model finds the most importan
613,618,"Fitting Decision tree classifier

from sklearn.tree import DecisionTreeClassifier

X = dataset[independent_variables]

y = dataset[dependent_variable]

my_classifier = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=10, random_state=0)

my_classifier.fit(X_train, y_train)

min_sample_split is the minimum no. of sample required for a split. 

For instance, if min_sample_split = 5 and there are 7 samples in the node and say the split happens one with 1 sample and other with 6 samples

min_sample_leaf is the minimum no. of sample required to be a leaf node. 

In the above example, if min_sample_leaf = 2, then the split will not happen because here one node has 1 sample

> min_sample_leaf is positively correlated with overfitting. This is a similar effect like max_depth",fit decis tree classifi sklearntre import decision,"['fit', 'decis', 'tree', 'classifi', 'sklearntre', 'import', 'decision']",fitting decision tree classifier  from sklearn.tree import decisiontreeclassifier  x = dataset[indep
614,619,"Fitting Decision tree regressor

from sklearn.tree import DecisionTreeRegressor

my_regressor = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=10, random_state=0)

my_regressor.fit(X_train, y_train)

> Full form of CART  is Classification And Regression Trees",fit decis tree regressor sklearntre import decisio,"['fit', 'decis', 'tree', 'regressor', 'sklearntre', 'import', 'decisio']",fitting decision tree regressor  from sklearn.tree import decisiontreeregressor  my_regressor = deci
615,620,"Coding for Visualizing Decision Tree

from sklearn.tree import DecisionTreeClassifier, export_graphviz

from sklearn import tree

from IPython.display import SVG

from graphviz import Source

from IPython.display import display

graph = Source(tree.export_graphviz(humidity_classifier, out_file=None, feature_names=X_train.columns, class_names=['0', '1'], filled = True))

display(SVG(graph.pipe(format='svg')))

>> Decision Nodes are represented by squares",code visual decis tree sklearntre import decisiont,"['code', 'visual', 'decis', 'tree', 'sklearntre', 'import', 'decisiont']","coding for visualizing decision tree  from sklearn.tree import decisiontreeclassifier, export_graphv"
616,621,"Advantages of Decision tree

1. Decision tree output is very easy to understand

2. Many times it is used as EDA or Data mining tool, as Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables.

3. Less data cleaning required as it is not influenced by outliers and missing values to a fair degree.

4. Possible Scenarios can be added",advantag decis tree 1 decis tree output easi under,"['advantag', 'decis', 'tree', '1', 'decis', 'tree', 'output', 'easi', 'under']",advantages of decision tree  1. decision tree output is very easy to understand  2. many times it is
617,622,"Disadvantages of Decision tree

High chance of overfitting as the model is a true mapping of training experiences and it does not use any probability of events.

This single disadvantage can be overcome by making a decision through a group of decision trees.",disadvantag decis tree high chanc overfit model tr,"['disadvantag', 'decis', 'tree', 'high', 'chanc', 'overfit', 'model', 'tr']",disadvantages of decision tree  high chance of overfitting as the model is a true mapping of trainin
618,623,"Basics of Ensembles of decision trees 

Ensembles of decision trees is the last option for non-linear regression/ classification problems on small dataset (<1lakh experiences) when KNN or Naive Bayes are not working well",basic ensembl decis tree ensembl decis tree last o,"['basic', 'ensembl', 'decis', 'tree', 'ensembl', 'decis', 'tree', 'last', 'o']",basics of ensembles of decision trees   ensembles of decision trees is the last option for non-linea
619,624,"Ensemble techniques

Meaning of ensemble is group. The concept is, a group of weak learners (which are trained on less data) come together to form a strong learner.

There are three ensemble technique:

1. Bagging
2. Boosting
3. Stacking

>> Decrease the fraction of samples to build a base learners will result in decrease in variance",ensembl techniqu mean ensembl group concept group ,"['ensembl', 'techniqu', 'mean', 'ensembl', 'group', 'concept', 'group']","ensemble techniques  meaning of ensemble is group. the concept is, a group of weak learners (which a"
620,625,"Bagging technique

It reduces the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same dataset.

This is similar to asking same question  to different persons and averaging the answers for the final answer.

Random Forest is an ensemble decision tree algorithm which uses bagging technique to reduce the chance of overfitting.
>> Individual tree is built on a subset of the features and subset of observations
>> It perform similar operations as dropout in a neural network",bag techniqu reduc varianc predict combin result m,"['bag', 'techniqu', 'reduc', 'varianc', 'predict', 'combin', 'result', 'm']",bagging technique  it reduces the variance of our predictions by combining the result of multiple cl
621,626,"Boosting technique

There are many boosting algorithms which impart additional boost to model’s accuracy:

Gradient Boosting 
XGBoost
AdaBoost
LightGBM
CatBoost

In Random Forest, trees are side by side means parallel, whereas in Boosting, multiple trees are in sequence.

Boosting fit a sequence of weak learners (with sub-samples). More weight is given to examples that were misclassified by earlier rounds.",boost techniqu mani boost algorithm impart addit b,"['boost', 'techniqu', 'mani', 'boost', 'algorithm', 'impart', 'addit', 'b']",boosting technique  there are many boosting algorithms which impart additional boost to model’s accu
622,627,"Extreme Gradient Boosting 

Extreme Gradient Boosting (XGBoost) is just an extension of gradient boosting. Advantages are:

1. Regularization,
2. Parallel Processing
3. High Flexibility
4. Handling Missing Values
5. Tree Pruning
6. Built-in Cross-Validation
7. Continue on Existing Model",extrem gradient boost extrem gradient boost xgboos,"['extrem', 'gradient', 'boost', 'extrem', 'gradient', 'boost', 'xgboos']",extreme gradient boosting   extreme gradient boosting (xgboost) is just an extension of gradient boo
623,628,"Stacking technique

In stacking, trees are being arranged in two levels: 

Level 0 (Base models): These trees are sequential like boosting, but all trees are trained on same numbers of experiences like original dataset.

Level 1 (Meta model): This tree learns how to best combine the predictions of the base models. This tree is absent in boosting technique.

>> Stacking is best in case of limited training data, because each classifier is trained on all of the available data",stack techniqu stack tree arrang two level level 0,"['stack', 'techniqu', 'stack', 'tree', 'arrang', 'two', 'level', 'level', '0']","stacking technique  in stacking, trees are being arranged in two levels:   level 0 (base models): th"
624,629,"Python coding for Ensembles of decision trees

from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import GradientBoostingClassifier

from xgboost import XGBClassifier

rf = RandomForestClassifier(n_estimators=100)

model = XGBRegressor(learning_rate =.1, n_estimators=10,
                     max_depth=2, min_child_weight=3, gamma=0, 
                     subsample=.8, colsample_bytree=.7, reg_alpha=1, 
                     objective= 'reg:linear')

>> Extra Trees (Extremely Randomized Trees) and Random Forest function does not use learning_rate hyperparameter
>> Increasing the value of max_depth may overfit the data

from sklearn.ensemble import StackingRegressor

estimators = [('lr', RidgeCV()), ('svr', LinearSVR(random_state=42))

reg = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=10, random_state=42))",python code ensembl decis tree sklearnensembl impo,"['python', 'code', 'ensembl', 'decis', 'tree', 'sklearnensembl', 'impo']",python coding for ensembles of decision trees  from sklearn.ensemble import randomforestclassifier  
625,630,"Finding feature importance

from sklearn.ensemble import ExtraTreesRegressor

my_model=ExtraTreesRegressor()

my_model.fit(X_train,y_train)

my_model.feature_importances_

returns feature importance scores",find featur import sklearnensembl import extratree,"['find', 'featur', 'import', 'sklearnensembl', 'import', 'extratree']",finding feature importance  from sklearn.ensemble import extratreesregressor  my_model=extratreesreg
626,631,"Classification report

It is very useful for multiclass classification problem

from sklearn.metrics import classification_report

clf_rpt = classification_report(Y_validation,y_pred)

print(""classification report :"", clf_rpt)

> It gives precision,    recall,  f1-score etc.
",classif report use multiclass classif problem skle,"['classif', 'report', 'use', 'multiclass', 'classif', 'problem', 'skle']",classification report  it is very useful for multiclass classification problem  from sklearn.metrics
627,632,"Ways to improve random forest accuracy

Algorithm Tuning
Add more data
Feature Selection",way improv random forest accuraci algorithm tune a,"['way', 'improv', 'random', 'forest', 'accuraci', 'algorithm', 'tune', 'a']",ways to improve random forest accuracy  algorithm tuning add more data feature selection
628,633,"Basics of Model Explainability

Explainability in machine learning means that we can explain what happens in our model from input to output. It makes models transparent and solves the black box problem. Explainable AI (XAI) is the more formal way to describe this and applies to all artificial intelligence.",basic model explartifici intelligencen explartific,"['basic', 'model', 'explartifici', 'intelligencen', 'explartific']",basics of model explartificial intelligencenability  explartificial intelligencenability in machine 
629,634,"Black Box Model vs. White Box Model

Black-box models, such as deep-learning (deep neural network), boosting, and random forest models, are highly non-linear by nature and are harder to explain in general. With black-box models, users can only observe the input-output relationship.

White-box models are the type of models in which one can clearly explain how they behave, how they produce predictions, and what the influencing variables are.",black box model vs white box model blackbox model ,"['black', 'box', 'model', 'vs', 'white', 'box', 'model', 'blackbox', 'model']","black box model vs. white box model  black-box models, such as deep-learning (deep neural network), "
630,635,"Explainable AI

Explainable AI is about understanding ML models better. How they make decisions, and why. The three most important aspects of model explainability are:

1. Transparency

2. Ability to question

3. Ease of understanding",explartifici intelligencen artifici intellig expla,"['explartifici', 'intelligencen', 'artifici', 'intellig', 'expla']",explartificial intelligencenable artificial intelligence  explartificial intelligencenable artificia
631,636,"Importance of explainability

Explainability connects the data science team and non-technical team, improving knowledge exchange, and giving all stakeholders a better understanding of product requirements and limitations.

Reasons:
1. Accountability
2. Trust
3. Compliance
4. Performance
5. Enhanced control",import explain explain connect data scienc team no,"['import', 'explain', 'explain', 'connect', 'data', 'scienc', 'team', 'no']","importance of explainability  explainability connects the data science team and non-technical team, "
632,637,"A. Scope of explainability

1. Global : This is the overall explanation of model behavior. It shows us a big picture view of the model, and how features in the data collectively affect the result.

2. Local : This tells us about each instance and feature in the data individually (kind of like explaining observations seen at certain points in the model), and how features individually affect the result.",scope explain 1 global overal explan model behavio,"['scope', 'explain', '1', 'global', 'overal', 'explan', 'model', 'behavio']",a. scope of explainability  1. global : this is the overall explanation of model behavior. it shows 
633,638,"B. Approach of explainability

i) Model-Agnostic Approach: It works across all types of models

ii) Model-Specific Approach:  It is tailor-made for a particular class of algorithms

Explainable models

1. Linear models

2. Decision Tree Algorithms

3. Generalized Additive Models (GAM)-it is like linear model, but X is another polynomial function. Example is Splines
",b approach explain modelagnost approach work acros,"['b', 'approach', 'explain', 'modelagnost', 'approach', 'work', 'acros']",b. approach of explainability  i) model-agnostic approach: it works across all types of models  ii) 
634,639,"Techniques or Libraries for Explainability in ML (mainly for black box models)

> LIME
> SHAP
> ELI5",techniqu librari explain machin learn main black b,"['techniqu', 'librari', 'explain', 'machin', 'learn', 'main', 'black', 'b']",techniques or libraries for explainability in machine learning (mainly for black box models)  > loca
635,640,"Local Interpretable Model-Agnostic Explanations (LIME)

It can give us the top most feature weights which are influencing (positively and negatively) the output
!pip install lime

import lime

import lime.lime_tabular

from __future__ import print_function

> This technique lie under model-agnostic approach and scope is local",local interpret modelagnost explan local interpret,"['local', 'interpret', 'modelagnost', 'explan', 'local', 'interpret']",local interpretable model-agnostic explanations (local interpretable model-agnostic explanations)  i
636,641,"Shapley Additive Explanations (SHAP)

The concept is a mathematical solution for a game theory problem – how to share a reward among team members in a cooperative game?

1. TreeExplainer - for the analysis of decision trees
2. DeepExplainer - for the deep learning algorithms (less advance)
3. KernelExplainer - for most of the algorithms

> Shapely (especially of a woman or part of her body) having an attractive or well-proportioned shape.",shapley addit explan shap concept mathemat solut g,"['shapley', 'addit', 'explan', 'shap', 'concept', 'mathemat', 'solut', 'g']",shapley additive explanations (shap)  the concept is a mathematical solution for a game theory probl
637,642,"Implementing SHAP

!pip install shap
import shap
shap.initjs()

shap_values = shap.TreeExplainer(model_rf_final).shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=list(X.columns), plot_type=""bar"")

shap_values = shap.KernelExplainer(model_svr_final.predict, X_train[100:150], link='identity').shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=list(X.columns), plot_type=""bar"")

There are currently four types of Summary Plots in SHAP: dot, bar, violin, and compact dot

plot_type=""dot""

plot_type=""bar""

plot_type=""violin""

plot_type=""compact dot""

Other plots in SHAP

shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0])

for elem in top_features:
shap.dependence_plot(elem, shap_values, X_train)",implement shapley addit explanationsley addit expl,"['implement', 'shapley', 'addit', 'explanationsley', 'addit', 'expl']",implementing shapley additive explanationsley additive explanationsley additive explanationsley addi
638,643,"Explain Like I'm 5 (ELI5)

It is an unified API and very straight forward in giving the explanation.

!pip install eli5

import eli5 as eli

eli.explain_weights(model_rf_final,feature_names=list(X.columns))",explain like im 5 explain like 5 unifi applic prog,"['explain', 'like', 'im', '5', 'explain', 'like', '5', 'unifi', 'applic', 'prog']",explain like im 5 (explain like I am 5)  it is an unified application programming interface and very
639,644,"Other techniques for Explainability in ML

1. Partial Dependence Plots (PDP)
2. Individual Condition Expectations plots (ICE)
3. Leave One Column Out (LOCO)
4. Accumulated Local Effects (ALE)
5. Yellowbrick
6. Lucid
7. Anchors
8. Deep Learning Important Features (DeepLIFT)
9. Layer-wise relevance propagation (LRP)
10. Contrastive Explanations Method (CEM)
11. ProfWeight etc.",techniqu explain machin learn 1 partial depend plo,"['techniqu', 'explain', 'machin', 'learn', '1', 'partial', 'depend', 'plo']",other techniques for explainability in machine learning  1. partial dependence plots (pdp) 2. indivi
640,645,"Basics of KNN

kNN is the first option for non linear classification problems on small non-text dataset (<1lakh experiences)

May be applied for regression problems. Then it takes the mean value of k closest points.

> k-Nearest Neighbors is one of the simplest supervised learning algorithms and it is robust to the noisy training data

> kNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data",basic knearest neighbor knearest neighbor first op,"['basic', 'knearest', 'neighbor', 'knearest', 'neighbor', 'first', 'op']",basics of k-nearest neighbors  k-nearest neighbors is the first option for non linear classification
641,646,"Euclidean Distance

The Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance.

Euclidean distance between two 'n' dimensional experiences or data points=sqrt((X11-X21)^2+(X12-X22)^2+...(X1n-X2n)^2)
Euclidean distance between two n-dimensional experiences treats each feature or dimension as equally important
> Manhattan distance can be used for continuous variables

Other distance metrices for kNN

Tanimoto
Jaccard",euclidean distanc euclidean distanc two point eucl,"['euclidean', 'distanc', 'euclidean', 'distanc', 'two', 'point', 'eucl']",euclidean distance  the euclidean distance between two points in euclidean space is the length of a 
642,647,"Working of kNN

Step-1: Select the number K of the neighbors
Step-2: Calculate the Euclidean distance of K number of neighbors
Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.
Step-4: Among these k neighbors, count the number of the data points in each category.
Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.",work knearest neighbor step1 select number k neigh,"['work', 'knearest', 'neighbor', 'step1', 'select', 'number', 'k', 'neigh']",working of k-nearest neighbors  step-1: select the number k of the neighbors step-2: calculate the e
643,648,"Ways to select the value of k in the kNN Algorithm

There is no particular way to determine the best value for ""k"", so we need to try some values to find the best out of them. The most preferred value for k is 5. 

Best value of k may be found out for best accuracy  score with the help of cross validation

>> The boundary becomes smoother with increasing value of K

>> When we increase value of k the bias increases (simplification of the model)",way select valu k knearest neighbor algorithm part,"['way', 'select', 'valu', 'k', 'knearest', 'neighbor', 'algorithm', 'part']",ways to select the value of k in the k-nearest neighbors algorithm  there is no particular way to de
644,649,"Disadvantages of kNN Algorithm:

> Always needs to determine the value of K which may be complex some time.

> Computationally expensive because of calculating the distance between the data points for all the training samples.

>  kNN is very likely to overfit due to the curse of dimensionality (dimensionality reduction and feature selection shall be used beforing fitting kNN model)

> Accuracy depends on the quality of the data.",disadvantag knearest neighbor algorithm alway need,"['disadvantag', 'knearest', 'neighbor', 'algorithm', 'alway', 'need']",disadvantages of k-nearest neighbors algorithm:  > always needs to determine the value of k which ma
645,650,"Python coding for KNN

from sklearn.neighbors import KNeighborsClassifier

 my_knn = KNeighborsClassifier(n_neighbors=5)

my_knn.fit(X_train,y_train)

my_knn.score(X_test,y_test)

> leaf_size is another important hyperparameter

from sklearn.metrics import confusion_matrix,roc_curve

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test,test_preds)",python code knearest neighbor sklearnneighbor impo,"['python', 'code', 'knearest', 'neighbor', 'sklearnneighbor', 'impo']",python coding for k-nearest neighbors  from sklearn.neighbors import kneighborsclassifier   my_k-nea
646,651,"Receiver operating characteristic and AUC

The ROC is also known as a relative operating characteristic, because it is a comparison of two operating characteristics (TPR and FPR).

AUC means area under the curve. So to get ROC-AUC score we need to define ROC curve first.

AUC is also known as AOC (Area under operating characteristic curve)

The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.

> Thus, a good model should have roc_auc_score > 0.5

",receiv oper characterist area curv receiv oper cha,"['receiv', 'oper', 'characterist', 'area', 'curv', 'receiv', 'oper', 'cha']",receiver operating characteristic and area under the curve  the receiver operating characteristic is
647,652,"Knn for recommender system

from scipy.sparse import csr_matrix

my_df_matrix = csr_matrix(X.values)

from sklearn.neighbors import NearestNeighbors

model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_knn.fit(my_df_matrix)

query_index = np.random.choice(X.shape[0])

distances, indices = model_knn.kneighbors(np.array(X.iloc[query_index,:].values).reshape(1, -1), n_neighbors = 6)",knearest neighbor recommend system scipyspars impo,"['knearest', 'neighbor', 'recommend', 'system', 'scipyspars', 'impo']",k-nearest neighbors for recommender system  from scipy.sparse import csr_matrix  my_df_matrix = csr_
648,653,"Sparse matrix and Dense matrix

> We convert  normal 2D Array with mostly zeros to csr_matrix (sparse matrix)

> Sparse matrices are memory efficient data structures that enable us store large matrices with very few non-zero elements. It does not store the zero values.

>  We can not view a sparse matrix type.

> Dense matrix are normal matrix which stores all the zero and non-zero values

 > CSR stands for compressed sparse row.

 
> So, For converting sparse matrix  to dense matrix 

sparse_matrix.todense()

> NearestNeighbors() model can also take X in the form of dense matrix but using sparse matrix  makes it more efficient in computation",spars matrix dens matrix convert normal 2d array m,"['spars', 'matrix', 'dens', 'matrix', 'convert', 'normal', '2d', 'array', 'm']",sparse matrix and dense matrix  > we convert  normal 2d array with mostly zeros to csr_matrix (spars
649,654,"Basics of Naive Bayes

Naive Bayes is the first option for non linear classification problems on small text dataset (<1lakh experiences)

> No need to convert categorical column to numerical column

> Naive means showing a lack of experience, wisdom, or judgement.",basic naiv bay naiv bay first option non linear cl,"['basic', 'naiv', 'bay', 'naiv', 'bay', 'first', 'option', 'non', 'linear', 'cl']",basics of naive bayes  naive bayes is the first option for non linear classification problems on sma
650,655,"Bayes theorem

Perhaps the easiest Naive Bayes classifier to understand is Gaussian Naive Bayes. 
In this classifier, the assumption is that data for each label is drawn from a simple Gaussian distribution.
> The Naive Bayesian classifier is based on Bayes' theorem

 P(C / X)= P(X / C) *P(C) / P(X) 
Here, 
P(C / X) or P(yes/sunny)= postrior probability,
P(X / C) or P(sunny/yes) = likelihood
P(C) or P(yes)=class prior probability
P(X) or P(sunny)=predictor prior probability
https://www.saedsayad.com/naive_bayesian.htm",bay theorem perhap easiest naiv bay classifi under,"['bay', 'theorem', 'perhap', 'easiest', 'naiv', 'bay', 'classifi', 'under']",bayes theorem  perhaps the easiest naive bayes classifier to understand is gaussian naive bayes.  in
651,656,"Understanding of Naive Bayes

Naive Bayes is a generative model and is very fast and have very few tunable parameters

For Naive Bayes model

>> attributes are equally important.
>> attributes are statistically independent of one another (This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems)
>> attributes can be nominal or numeric
>>  when an attribute value in the testing record has no example in the training set, then the entire posterior probability will be zero.",understand naiv bay naiv bay generat model fast tu,"['understand', 'naiv', 'bay', 'naiv', 'bay', 'generat', 'model', 'fast', 'tu']",understanding of naive bayes  naive bayes is a generative model and is very fast and have very few t
652,657,"Generative model vs Discriminative model
 
1. A Generative Model ‌learns the joint probability distribution p(x,y) but discriminative model ‌learns the conditional probability distribution p(y|x)

2. Both of them predicts the conditional probability

3. Generative models are less accurate than discriminative models 

4. Generative models can work with missing data, and discriminative models generally can’t.

5. Compared with discriminative models, generative models need less data to train.",generat model vs discrimin model 1 generat model ‌,"['generat', 'model', 'vs', 'discrimin', 'model', '1', 'generat', 'model', '\u200c']",generative model vs discriminative model   1. a generative model ‌learns the joint probability distr
653,658,"Text Pre-processing

> The classification algorithms need some sort of numerical feature vector.

> For the conversion of text to numerical value, the simplest method is the bag-of-words approach, where each unique word in a text will be represented by one number.",text preprocess classif algorithm need sort numer ,"['text', 'preprocess', 'classif', 'algorithm', 'need', 'sort', 'numer']",text pre-processing  > the classification algorithms need some sort of numerical feature vector.  > 
654,659,"Steps of Text Pre-processing (used in  Topic modeling)

1. Importing necessary libraries

import nltk

nltk.download('stopwords')

import string

from nltk.corpus import stopwords

2. Writing functions which removes punctuation and stopwords from our data

def text_process(text):
    nopunc =[char for char in text if char not in string.punctuation]
    nopunc=''.join(nopunc)
    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])

3. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens. Tokens are also known as lemmas

message['tokenized_message'] = message['message'].apply(text_process)

4. Vectorization in three steps",step text preprocess use topic model 1 import nece,"['step', 'text', 'preprocess', 'use', 'topic', 'model', '1', 'import', 'nece']",steps of text pre-processing (used in  topic modeling)  1. importing necessary libraries  import nlt
655,660,"One hot encoding

One hot encoding is word representation without any context. It only tells about its presence in the vocabulary. It has nothing to do with the word meaning

Thus, by one hot encoding we are representing a word as point (or a line joining the origin and that point-means vector) in the vector space of vocabulary

One hot encoding is a process by which categorical variables (tokenized message) are converted into a form (matrix with zero/one value) that could be provided to ML algorithms to do a better job in prediction.

In one hot encoding there will be huge number of features (unigram words). So, it has a curse of dimensionality. It also can not count term frequency.

> Solution is CountVectorizer

or TF-IDF vectorizer (better)",one hot encod one hot encod word represent without,"['one', 'hot', 'encod', 'one', 'hot', 'encod', 'word', 'represent', 'without']",one hot encoding  one hot encoding is word representation without any context. it only tells about i
656,661,"Vectorization techniques (three steps)

4.1. Count how many times does a word occur in each message (Known as term frequency,TF)

from sklearn.feature_extraction.text import CountVectorizer

del vectorizer
(del keyword in python is primarily used to delete objects in Python)

vectorizer = CountVectorizer(max_df = 0.9,min_df = 10)
vectorizer_matrix = vectorizer.fit_transform(message['tokenized_message'])

4.2. Weigh the counts, so that frequent tokens get lower weight

4.3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)

> CountVectorizer is also called bag-of-words",vector techniqu three step 41 count mani time word,"['vector', 'techniqu', 'three', 'step', '41', 'count', 'mani', 'time', 'word']",vectorization techniques (three steps)  4.1. count how many times does a word occur in each message 
657,662,"Python coding for Naive Bayes

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import classification_report,confusion_matrix

X=vectorizer_matrix.todense()

y=my_df['label']

> Then split for X_train, X_test, y_train, y_test

classification_model = GaussianNB().fit(X_train,y_train)

Evaluating performance

test_preds = spam_detect_model.predict(X_test)

test_preds[:2]

y_test[:2]

print(confusion_matrix(y_test,test_preds))

print(classification_report(y_test,test_preds))

vectorizer_matrix or tfidf_matrix are sparse matrix. But Naive Bayes model needs dense matrix",python code naiv bay sklearnnaivebay import gaussi,"['python', 'code', 'naiv', 'bay', 'sklearnnaivebay', 'import', 'gaussi']",python coding for naive bayes  from sklearn.naive_bayes import gaussiannb  from sklearn.metrics impo
658,663,Generalization Error is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.,general error measur accur algorithm abl predict o,"['general', 'error', 'measur', 'accur', 'algorithm', 'abl', 'predict', 'o']",generalization error is a measure of how accurately an algorithm is able to predict outcome values f
659,664,"Basics of SVM

Support vector is the last option for linear regression/ classification problems on small dataset (<1lakh experiences) when regularized linear regressions is not working well

SVMs can also efficiently perform as non-linear classification model using the kernel trick. 

> SVM is also used for Outlier Detection and image classification Purposes on small dataset",basic support vector machin support vector last op,"['basic', 'support', 'vector', 'machin', 'support', 'vector', 'last', 'op']",basics of support vector machine  support vector is the last option for linear regression/ classific
660,665,"Kernel function 

Apart from SVM, other kernelizable algorithms are linear regression, PCA, K-means, neural network etc.

The meaning of Kernel is the edible part of a nut, seed, or fruit.

> Thus kernel function of a model softens the decision boundary to fit for non-linear classification

> Kernel function map low dimensional data to high dimensional space

> Kernel is a similarity function",kernel function apart svm kerneliz algorithm linea,"['kernel', 'function', 'apart', 'svm', 'kerneliz', 'algorithm', 'linea']","kernel function   apart from svm, other kernelizable algorithms are linear regression, pca, k-means,"
661,666,"Hinge loss function, hypersurface and hyperplane

The intuition of SVM is rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest point.

> Margin lines are the decision boundaries.

> SVM find a hyperplane to linearly separate the data by using the hinge loss function

> A decision boundary is a hypersurface that partitions the underlying vector space into two sets, one for each class. A general hypersurface in a small dimension space is turned into a hyperplane in a space with much larger dimensions.",hing loss function hypersurfac hyperplan intuit su,"['hing', 'loss', 'function', 'hypersurfac', 'hyperplan', 'intuit', 'su']","hinge loss function, hypersurface and hyperplane  the intuition of support vector machine is rather "
662,667,"Python coding for SVM

Without Karnel (for linear or binary classification)

from sklearn.svm import SVC 

my_model = SVC(gamma=0.23, C=3.20)

my_model.fit(X_train, y_train)

To generate blobs data points with a Gaussian distribution,

from sklearn.datasets.samples_generator import make_blobs

X, y = make_blobs(n_samples=50, centers=2,
random_state=0, cluster_std=0.60)

To generate circular data points,

from sklearn.datasets.samples_generator import make_circles

X, y = make_circles(100, factor=.1, noise=.1)

Kernel SVM

from sklearn.svm import SVC 

my_kernel_model = SVC(kernel='rbf', gamma=0.23, C=3.20)

my_kernel_model.fit(X, y)

> Different kernel functions are linear, nonlinear, polynomial (poly), radial basis function (rbf-it uses Gaussian transformation), and sigmoid.

> For high gamma value the model would consider only the points close to the hyperplane for modeling",python code support vector machin without karnel l,"['python', 'code', 'support', 'vector', 'machin', 'without', 'karnel', 'l']",python coding for support vector machine  without karnel (for linear or binary classification)  from
663,668,"Tuning the SVM: Softening Margins

The hardness of the margin is controlled by a tuning parameter, most often known as  C . For very large  C , the margin is hard, and points cannot lie in it. For smaller  C , the margin is softer, and can grow to encompass some points.

my_model = SVC(kernel='linear', C=0.1).fit(X, y)

The optimal value of the  C  parameter will depend on our dataset, and should be tuned using cv.",tune svm soften margin hard margin control tune pa,"['tune', 'svm', 'soften', 'margin', 'hard', 'margin', 'control', 'tune', 'pa']","tuning the svm: softening margins  the hardness of the margin is controlled by a tuning parameter, m"
664,669,"Advantages of SVM:

Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.

Reason of high speed of SVMs 

1. Quadratic optimization (convex!)
2. They work in the dual, with relatively few points
3. The kernel trick",advantag svm affect point near margin work well hi,"['advantag', 'svm', 'affect', 'point', 'near', 'margin', 'work', 'well', 'hi']","advantages of svm:  because they are affected only by points near the margin, they work well with hi"
665,670,"Disadvantages of SVM:

The results are strongly dependent on a suitable choice for the softening parameter C. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.

If we have the CPU cycles to commit  training and cross-validating an SVM on our data, the method can lead to excellent results.

> We should always be careful about iteration for large dataset, otherwise our model will be computationally expensive

SVM will be effective when:

1. The data is linearly separable
2. The data is clean and ready to use
3. Proper Kernel is selected
4. Proper Kernel Parameters are selected",disadvantag support vector machin result strong de,"['disadvantag', 'support', 'vector', 'machin', 'result', 'strong', 'de']",disadvantages of support vector machine:  the results are strongly dependent on a suitable choice fo
666,671,"Basics of Artificial Neural Network

Artificial Neural Network (ANN) with SGD is the only option for linear Regression/ Classification problems on large dataset (> 1lakh experiences)

ANN's are representative of the human brain. Human nervous system consists of billions of neurons. These neurons collectively process input received from sensory organs, process the information, and decides what to do in reaction to the input.

For simplicity, ANN is called Neural network

Neural network with SGD and kernel approximations is the only option for non-linear Classification problems on large dataset (>1 lakh experiences)",basic artifici neural network artifici neural netw,"['basic', 'artifici', 'neural', 'network', 'artifici', 'neural', 'netw']",basics of artificial neural network  artificial neural network (artificial neural network) with stoc
667,672,"Perceptron, neuron, node, unit

A neural network of single layer with single neuron which gives a single output from multiple inputs (i.e. multiple input features) with the help of activation function is called a perceptron.

Thus,
perceptron = neuron=node=unit 

Neural network can have multiple hidden layers and every hidden layer can have multiple neurons

The Perceptron is a linear classification algorithm. This means that it learns a decision boundary that separates two classes using a line (called a hyperplane) in the feature space.",perceptron neuron node unit neural network singl l,"['perceptron', 'neuron', 'node', 'unit', 'neural', 'network', 'singl', 'l']","perceptron, neuron, node, unit  a neural network of single layer with single neuron which gives a si"
668,673,"Activation function

The activation function maps linear regression function, z(x) (z=y_pred in linear regression) and generates an activation/output value f(z) 

For a classification algorithm, we use sigmoid function

Predict 1: If Activation > 0.0

Predict 0: If Activation <= 0.0

> Non-linear activation function is the kernel approximation",activ function activ function map linear regress f,"['activ', 'function', 'activ', 'function', 'map', 'linear', 'regress', 'f']","activation function  the activation function maps linear regression function, z(x) (z=y_pred in line"
669,674,"Model coefficients

The coefficients of the model are referred to as input weights and are trained using the stochastic gradient descent (SGD) optimization algorithm.",model coeffici coeffici model refer input weight t,"['model', 'coeffici', 'coeffici', 'model', 'refer', 'input', 'weight', 't']",model coefficients  the coefficients of the model are referred to as input weights and are trained u
670,675,"The components of neural network: 

1. Input layer (It includes all the feature values of the data for forward propagation)-This is nothing but inputs not a layer(defined as input_dim in the first hidden layer), 

2. Hidden layers (can have none, one or multiple) (defined with my_model.add method), 
3. Output layer (have 1 node for binary output) (defined with my_model.add method),

4. Neuron with Activation function (defined with Dense function).",compon neural network 1 input layer includ featur ,"['compon', 'neural', 'network', '1', 'input', 'layer', 'includ', 'featur']",the components of neural network:   1. input layer (it includes all the feature values of the data f
671,676,"Backward propagation

We must adjust the weights to make the network fit the training data. The process of making these adjustments is known as backward propagation.",backward propag must adjust weight make network fi,"['backward', 'propag', 'must', 'adjust', 'weight', 'make', 'network', 'fi']",backward propagation  we must adjust the weights to make the network fit the training data. the proc
672,677,"Understanding epochs

An epochs is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. (defined during model training)

",understand epoch epoch term use machin learn indic,"['understand', 'epoch', 'epoch', 'term', 'use', 'machin', 'learn', 'indic']",understanding epochs  an epochs is a term used in machine learning and indicates the number of passe
673,678,"Understanding of batch size

If the batch_size is the whole training dataset then the number of epochs is the number of iterations (batch_size is also defined during model training)",understand batch size batchsiz whole train dataset,"['understand', 'batch', 'size', 'batchsiz', 'whole', 'train', 'dataset']",understanding of batch size  if the batch_size is the whole training dataset then the number of epoc
674,679,"Optimal accuracy in Neural Network

When we find the optimal number of neurons in multiple layers, we get optimal accuracy.

As a first attempt, we can try any one of the following:
> The number of hidden neurons should be between the size of the input layer (no. of features) and the size of the output layer (1 for binary classification).

> The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.

> The number of hidden neurons should be less than twice the size of the input layer. ",optim accuraci neural network find optim number ne,"['optim', 'accuraci', 'neural', 'network', 'find', 'optim', 'number', 'ne']","optimal accuracy in neural network  when we find the optimal number of neurons in multiple layers, w"
675,680,"Applications of Neural Networks:

a. Classification of data
b. Anomaly detection
c. Speech recognition
d. Audio generation
e. Time series analysis
f. Spell checking
g. Character recognition
h. Machine translation (Converts one human language to another)
i. Image processing

> Neural networks have higher computational rates than conventional computers because a lot of the operation is done in parallel",applic neural network classif data b anomali detec,"['applic', 'neural', 'network', 'classif', 'data', 'b', 'anomali', 'detec']",applications of neural networks:  a. classification of data b. anomaly detection c. speech recogniti
676,681,"Steps of ANN:

1. Take inputs
2. Add bias (if required)
3. Assign random weights to input features
4. Run the code for training (applying the learning function) in forward propagation.
5. Find the error in prediction.
6. Update the weight by SGD in back propagation.
7. Repeat the training phase with updated weights.
8. Make predictions.",step ann 1 take input 2 add bias requir 3 assign r,"['step', 'ann', '1', 'take', 'input', '2', 'add', 'bias', 'requir', '3', 'assign', 'r']",steps of ann:  1. take inputs 2. add bias (if required) 3. assign random weights to input features 4
677,682,"Libraries for ANN model

from keras.models import Sequential

from keras.layers import Dense

from tensorflow.keras.optimizers import Adam

from keras.layers import Dropout

from keras import regularizers
",librari artifici neural network model kerasmodel i,"['librari', 'artifici', 'neural', 'network', 'model', 'kerasmodel', 'i']",libraries for artificial neural network model  from keras.models import sequential  from keras.layer
678,683,"Define, Create and Compile ANN model

# define a function to build the keras model
def create_model(a,b):
    # create model
model = Sequential()

  model.add(Dense(a, input_dim=13, activation='sigmoid'))
   
model.add(Dense(b, activation='sigmoid'))
    
model.add(Dense(1, activation='sigmoid'))
   
# compile model
adam = Adam(lr=0.001)    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model

heart_model = create_model(11,10)

print(heart_model.summary())

> 'a' is the no. of neurons in the first hidden layer and 'b' is the no. of neurons in the second hidden layer ",defin creat compil artifici neural network model d,"['defin', 'creat', 'compil', 'artifici', 'neural', 'network', 'model', 'd']","define, create and compile artificial neural network model  # define a function to build the keras m"
679,684,"# fit the ANN model on the dataset

heart_model.fit(X_train, y_train, epochs=150, batch_size=10)",fit artifici neural network model dataset heartmod,"['fit', 'artifici', 'neural', 'network', 'model', 'dataset', 'heartmod']","# fit the artificial neural network model on the dataset  heart_model.fit(x_train, y_train, epochs=1"
680,685,"# evaluate the ANN model

result = heart_model.evaluate(X_test,y_test,verbose=1)

print('Test loss: ', result[0])

print('Test accuracy: ', result[1])

train_pred=model.predict(X_train)>0.5

test_pred=model.predict(X_test)>0.5

print(confusion_matrix(y_train,train_pred))
print(confusion_matrix(y_test,test_pred))",evalu artifici neural network model result heartmo,"['evalu', 'artifici', 'neural', 'network', 'model', 'result', 'heartmo']","# evaluate the artificial neural network model  result = heart_model.evaluate(x_test,y_test,verbose="
681,686,"Plotting roc_curve

fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_pred)
auc_train = auc(fpr_train, tpr_train)
plt.figure
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_train, tpr_train, label='Keras (area = {:.3f})'.format(auc_train))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()

Plotting roc_curve (easy way)

from sklearn import metrics

metrics.plot_roc_curve(my_model, X_train, y_train)  ",plot roccurv fprtrain tprtrain thresholdstrain roc,"['plot', 'roccurv', 'fprtrain', 'tprtrain', 'thresholdstrain', 'roc']","plotting roc_curve  fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_pred) auc_trai"
682,687,"

Plotting precision_recall_curve (easy way)

metrics.plot_precision_recall_curve(my_model, X_train, y_train)",plot precisionrecallcurv easi way metricsplotpreci,"['plot', 'precisionrecallcurv', 'easi', 'way', 'metricsplotpreci']","  plotting precision_recall_curve (easy way)  metrics.plot_precision_recall_curve(my_model, x_train,"
683,688,"Understanding Feature engineering

Feature engineering is a process of transforming the given data into a form which is easier to interpret.

We define feature engineering as creating new features from our existing ones to improve model performance.",understand featur engin featur engin process trans,"['understand', 'featur', 'engin', 'featur', 'engin', 'process', 'trans']",understanding feature engineering  feature engineering is a process of transforming the given data i
684,689,"Importance of Feature Engineering

The intention of feature engineering is to achieve two primary goals:

1. Preparing an input dataset that is compatible with and best fits the machine learning algorithm.

2. Improving the performance of machine learning models",import featur engin intent featur engin achiev two,"['import', 'featur', 'engin', 'intent', 'featur', 'engin', 'achiev', 'two']",importance of feature engineering  the intention of feature engineering is to achieve two primary go
685,690,"Basic EDA

1. Univariate Analysis

Ex:- CDF, PDF, Box plot, Violin plot.

2.  Bivariate analysis

Ex:- Box plot, Scatter Plot,Violin plot, Joint plot.

3. Multivariate Analysis

Ex:- Pair Plot, 3D Scatter Plot.",basic exploratori data analysi 1 univari analysi e,"['basic', 'exploratori', 'data', 'analysi', '1', 'univari', 'analysi', 'e']","basic exploratory data analysis  1. univariate analysis  ex:- cdf, pdf, box plot, violin plot.  2.  "
686,691,"Understanding Outliers

Outliers are actually exceptional or abnormal experience

> Finding out the true outlier experiences is very important

> In univariate or bivariate analysis of box plot, an observation may become outlier. There may be some other feature which is driving this excessive high or low value

> The true outlier experiences are those for which there is no feature in our dataset to catch the pattern

> When the feature is skewed, it can not show the true outlier. It has to be normal distribution to show the true outliers",understand outlier outlier actual except abnorm ex,"['understand', 'outlier', 'outlier', 'actual', 'except', 'abnorm', 'ex']",understanding outliers  outliers are actually exceptional or abnormal experience  > finding out the 
687,692,"Understanding Clustering Algorithm

KMeans is the only option for unsupervised machine learning with small data (< 10K experiences) with known no. of categories or clusters (need to define n_clusters=k)

MiniBatch Kmeans is the only option for unsupervised machine learning with large data (> 10K experiences) with known no. of categories (need to define n_clusters=k)

Variational Bayesian Gaussian mixture model (VB-GMM) is the best option for unsupervised machine learning with small data (<10K experiences) with unknown no. of categories)",understand cluster algorithm kmean option unsuperv,"['understand', 'cluster', 'algorithm', 'kmean', 'option', 'unsuperv']",understanding clustering algorithm  kmeans is the only option for unsupervised machine learning with
688,693,"Python coding for Kmeans

from sklearn.cluster import KMeans

my_model = KMeans(n_clusters=4)

my_model.fit(X)

y_preds = my_model.predict(X)

my_model.cluster_centers_

from sklearn.cluster import MiniBatchKMeans

my_model = MiniBatchKMeans(n_clusters=2, random_state=0, batch_size=6, max_iter=10)

my_model.fit_predict(X_final)

GMM

from sklearn.mixture import GaussianMixture

gaussian_model = GaussianMixture(n_components=2)",python code kmean sklearnclust import kmean mymode,"['python', 'code', 'kmean', 'sklearnclust', 'import', 'kmean', 'mymode']",python coding for kmeans  from sklearn.cluster import kmeans  my_model = kmeans(n_clusters=4)  my_mo
689,694,"Few issues of Kmeans: 

1. Although the expectation– maximization algorithm is guaranteed to improve the result in each step, there is no assurance that it will lead to the global best solution. 
>> Expectation-Maximization approach 
Assign the data points (data point means one observation in the feature space or n dimensional space) to the nearest cluster center in each Expectation step
>> For supervised learning model, we use error minimization algorithm like OLS, gradient descent as we have the actual value. For unsupervised learning model, we use expectation– maximization algorithm

2. Another common challenge with k-means is that we must tell it how many clusters we expect: it cannot learn the number of clusters from the data.

 Ideally, we would not know how many clusters should we have, in the beginning of the algorithm

3. The algorithm will often be ineffective if the clusters have complicated geometries.

It always makes spherical clusters.",issu kmean 1 although expectation– maxim algorithm,"['issu', 'kmean', '1', 'although', 'expectation–', 'maxim', 'algorithm']",few issues of kmeans:   1. although the expectation– maximization algorithm is guaranteed to improve
690,695,"Silhouette score for finding best no. of clusters 

1. Silhouette score tells how well samples are clustered with other samples that are similar to each other. 

from sklearn.metrics import silhouette_samples, silhouette_score

list_of_silhouette_score=[]
for k in range(2,15):
  my_kmeans_model = KMeans(n_clusters=k)
  my_kmeans_model.fit(X)
  y_preds = my_kmeans_model.predict(X)
  list_of_silhouette_score.append(silhouette_score(X,y_preds,  metric='euclidean'))

plt.plot(range(2,15), list_of_silhouette_score,'bo--',linewidth=2, markersize=8)
> Take the no. of cluster for the highest silhouette_score
> Same graph we can create by KElbowVisualizer with metric='silhouette'

from yellowbrick.cluster import SilhouetteVisualizer

my_model=KMeans(n_clusters=3)

visualizer = SilhouetteVisualizer(my_model)

visualizer.fit(X)

  visualizer.poof()

# Draw/show/poof the data",silhouett score find best cluster 1 silhouett scor,"['silhouett', 'score', 'find', 'best', 'cluster', '1', 'silhouett', 'scor']",silhouette score for finding best no. of clusters   1. silhouette score tells how well samples are c
691,696,"Elbow method for finding best no. of clusters 

2. The elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.

sum_of_sq_dist = {}

for k in range(1,15):

    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)
    km = km.fit(X)
    sum_of_sq_dist[k] = km.inertia_
    
sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))

plt.xlabel('Number of Clusters(k)')
plt.ylabel('Sum of Square Distances')
plt.title('Elbow Method For Optimal k')
plt.show()
>> We can also do elbow visualization in yellowbricks",elbow method find best cluster 2 elbow method heur,"['elbow', 'method', 'find', 'best', 'cluster', '2', 'elbow', 'method', 'heur']",elbow method for finding best no. of clusters   2. the elbow method is a heuristic used in determini
692,697,"Plotting Clusters

y_preds = my_model.predict(X)

centers = my_model.cluster_centers_

plt.scatter(X[:, 6], X[:, 7], c=y_preds, s=50, cmap='viridis')

plt.scatter(centers[:, 6], centers[:, 7], c='blue', s=200, alpha=0.5)

> cluster centers have same dimension as the data

> Cluster ploting is not useful for visualizing multidimensional clusters",plot cluster ypred mymodelpredictx center mymodelc,"['plot', 'cluster', 'ypred', 'mymodelpredictx', 'center', 'mymodelc']",plotting clusters  y_preds = my_model.predict(x)  centers = my_model.cluster_centers_  plt.scatter(x
693,698,"Features of KMeans model

> Kmeans is an example of partition clustering method

> Kmeans is most sensitive to outliers 

> Kmeans is sensitive to missing values (Imputation with Expectation Maximization algorithm is the valid iterative strategy for treating missing values) 
>  Kmeans is sensitive to multicollinearity of features 

1) K-means is extremely sensitive to cluster center initializations (cluster seeds)
2) Bad initialization can lead to Poor convergence speed
3) Bad initialization can lead to bad overall clustering",featur kmean model kmean exampl partit cluster met,"['featur', 'kmean', 'model', 'kmean', 'exampl', 'partit', 'cluster', 'met']",features of kmeans model  > kmeans is an example of partition clustering method  > kmeans is most se
694,699,"Other Clustering models

Density-based clustering is a non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together. Example: Density-based spatial clustering of applications with noise (DBSCAN) 

from sklearn.cluster import DBSCAN

dbscan_model = DBSCAN(eps=3, min_samples=2).fit(X)

dbscan_model.labels_

dbscan_model.predict(training_data)",cluster model densitybas cluster nonparametr algor,"['cluster', 'model', 'densitybas', 'cluster', 'nonparametr', 'algor']",other clustering models  density-based clustering is a non-parametric algorithm: given a set of poin
695,700,">> RFM (recency, frequency and monetary) customer segmentation is very commonly used in marketing. It is a heuristic technique not a ml model",rfm recenc frequenc monetari custom segment common,"['rfm', 'recenc', 'frequenc', 'monetari', 'custom', 'segment', 'common']",">> rfm (recency, frequency and monetary) customer segmentation is very commonly used in marketing. i"
696,701,"Basics of Hierarchical clustering

This is also applicable on small dataset (< 10K experiences) like Kmeans

Hierarchical clustering takes away the problem of having to pre-define the number of clusters in Kmeans.

The optimal number of clusters can be obtained by the model itself, practical visualization with the dendrogram

Hierarchical clustering algorithms suffers from the problem of convergence at local optima",basic hierarch cluster also applic small dataset 1,"['basic', 'hierarch', 'cluster', 'also', 'applic', 'small', 'dataset', '1']",basics of hierarchical clustering  this is also applicable on small dataset (< 10k experiences) like
697,702,"Types of hierarchical clustering:

1. Agglomerative hierarchical clustering (Bottom Up Approach)

We assign each data point (sample, observation or experience) to an individual cluster at the begining. 'n' clusters for 'n' observations

Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left.

It is also known as additive hierarchical clustering.

2. Divisive Hierarchical clustering (Top Down Approach)

Also called DIANA (DIvisive ANAlysis) is the inverse of agglomerative clustering

We start with a single cluster and assign all the data points to that cluster.

Now, at each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point.

",type hierarch cluster 1 agglom hierarch cluster bo,"['type', 'hierarch', 'cluster', '1', 'agglom', 'hierarch', 'cluster', 'bo']",types of hierarchical clustering:  1. agglomerative hierarchical clustering (bottom up approach)  we
698,703,"Proximity matrix

This is a mXm (m is the number of observations) square matrix which stores the euclidean distances between all the data points of the experience set.

In each iteration, proximity matrix is updated by considering a cluster as single observation.",proxim matrix mxm number observ squar matrix store,"['proxim', 'matrix', 'mxm', 'number', 'observ', 'squar', 'matrix', 'store']",proximity matrix  this is a mxm (m is the number of observations) square matrix which stores the euc
699,704,"Understanding Linkage

Linkage is the method of determining the euclidean distance between two clusters.

1. Single linkage: Here two closest data points between the clusters is considered.

This approach can separate non-spherical clusters as long as the gap between two clusters is not small.

This approach cannot separate clusters properly if there is noise between clusters.

2. Complete linkage: Here two furthest data points between the clusters is considered.

This approach does well in separating clusters if there is noise between clusters.

3. Average linkage: Average of the distances among all the data points between the clusters is considered.

This approach also does well in separating clusters if there is noise between clusters

4. Ward’s Method

Similar like average linkage except that Ward’s method calculates the sum of the square of the distances.

Ward’s method approach does best in separating clusters if there is noise between clusters.",understand linkag linkag method determin euclidean,"['understand', 'linkag', 'linkag', 'method', 'determin', 'euclidean']",understanding linkage  linkage is the method of determining the euclidean distance between two clust
700,705,"Finding out no. of clusters from visualization

import scipy.cluster.hierarchy as sch

dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))

Find largest vertical distance we can make, without crossing any other horizontal line",find cluster visual import scipyclusterhierarchi s,"['find', 'cluster', 'visual', 'import', 'scipyclusterhierarchi', 's']",finding out no. of clusters from visualization  import scipy.cluster.hierarchy as sch  dendrogram = 
701,706,"Python coding for Hierarchical clustering

from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')

y_hc = hc.fit_predict(X)",python code hierarch cluster sklearnclust import a,"['python', 'code', 'hierarch', 'cluster', 'sklearnclust', 'import', 'a']",python coding for hierarchical clustering  from sklearn.cluster import agglomerativeclustering  hc =
702,707,"Basics of PCA

PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization (in 2D or 3D plot), for noise filtering (does not eleminate the noise, but reduces), for feature extraction and engineering, data compression and much more.

Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction (to get rid of curse of dimensionality) in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset.

> PCA is mostly used for unsupervised learning

> Assumption: There is linear relationship between all variables.

Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.

> PCA's main weakness is that it tends to be highly affected by outliers in the data.

> If we use features of different scales, we get misleading directions.

Thus outlier treatment and feature scaling must be performed prior to PCA",basic princip compon analysi princip compon analys,"['basic', 'princip', 'compon', 'analysi', 'princip', 'compon', 'analys']",basics of principal component analysis  principal component analysis is fundamentally a dimensionali
703,708,"Drawback of PCA

1. Some information is lost, 

2. It can be computationally intensive.

3. Transformed features are often hard to interpret.",drawback princip compon analysi 1 inform lost 2 co,"['drawback', 'princip', 'compon', 'analysi', '1', 'inform', 'lost', '2', 'co']","drawback of principal component analysis  1. some information is lost,   2. it can be computationall"
704,709,"Python Coding of PCA

from sklearn.decomposition import PCA

my_model = PCA(n_components=3)

my_model.fit(X)

X_new = my_model.transform(X)

transform() function reduces dimensions according to the learnings from fit() function

> Now, X_new can be fitted in any ml model

PCA inside Pipeline function

from sklearn.pipeline import Pipeline

my_model = Pipeline([('norm', MinMaxScaler()), ('pca', PCA()), ('m', LogisticRegression())])

my_model.fit(X_train,y_train) or,
my_model.fit_transform(X_train)

y_pred = my_model.predict(X_test)",python code princip compon analysi sklearndecompos,"['python', 'code', 'princip', 'compon', 'analysi', 'sklearndecompos']",python coding of principal component analysis  from sklearn.decomposition import principal component
705,710,"Understanding the important features in PCA

# get the number of components

n_pcs= my_model.components_.shape[0]

# get the index of the most important feature on EACH component

most_important = [np.abs(my_model.components_[i]).argmax() for i in range(n_pcs)]

feature_names = X.columns.tolist()

# get the names

most_important_names = [feature_names[most_important[i]] for i in range(n_pcs)]

list_of_list = [['PC{}'.format(i+1),most_important_names[i],'{}'.format(my_model.explained_variance_[i])] for i in range(n_pcs)]

# build the dataframe

df = pd.DataFrame(list_of_list,columns=['Principle-Components','Feature-Name','Explained-Variance'])",understand import featur princip compon analysi ge,"['understand', 'import', 'featur', 'princip', 'compon', 'analysi', 'ge']",understanding the important features in principal component analysis  # get the number of components
706,711,"Deeper Understanding of PCA

PCA does not change the distribution of datapoints. It only shifts the origin approximately to the center of gravity of the whole data points and selects the spinal cord of the datapoints as the first PC dimension and recalculate the coordinate of each datapoint. When we know the coordinate of any data point, that means we know the projection of the datapoint in all PC dimension. 

PC dimension-1 is the most significant extracted feature.

This is a techniques from the field of linear algebra. This is often called “feature projection” and the algorithms used are referred to as “projection methods.”

PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

This transformation from data axes to principal axes is an affine transformation, which basically means it is composed of three basic transformation: translation,  rotation and uniform scaling

An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation).

> Translation is a geometric transformation that moves every point of a figure, shape or space by the same distance in a given direction. A translation can also be interpreted as the addition of a constant vector to every point, or as shifting the origin of the coordinate system.

> If we don’t rotate (orthogonally) the components, the effect of PCA will diminish and we’ll have to select more number of components to explain variance in the training set.",deeper understand princip compon analysi princip c,"['deeper', 'understand', 'princip', 'compon', 'analysi', 'princip', 'c']",deeper understanding of principal component analysis  principal component analysis does not change t
707,712,"Math behind PCA

Step 1 : Take the whole dataset with dimension  d

Step 2 : Compute the mean of every dimension of the whole dataset.

Step 3 : Compute the covariance matrix (between the features) of the whole dataset

Step 4 : Compute Eigenvectors and corresponding Eigenvalues for the covariance matrix

Step 5 : Sort the eigenvectors by decreasing eigenvalues and choose  k  eigenvectors with the largest eigenvalues to form a  d×k  dimensional matrix  W .

Step 6 : Transform the samples onto the new subspace

> covariance matrix is a square matrix giving the covariance between each pair of elements of a given random vector

> Each Eigenvalue corresponds to a principal component. The highest eigenvalue corresponds to the 1st Principal Component and so on. The length of a principal component is its eigenvalue.
> Thus, if the eigenvalues are roughly equal PCA will perform badly
",math behind princip compon analysi step 1 take who,"['math', 'behind', 'princip', 'compon', 'analysi', 'step', '1', 'take', 'who']",math behind principal component analysis  step 1 : take the whole dataset with dimension  d  step 2 
708,713,"TruncatedSVD 

PCA is TruncatedSVD on centered data (by per-feature mean substraction). If the data is already centered, those two classes will do the same.

In practice TruncatedSVD is useful on large sparse datasets which cannot be centered without making the memory usage explode.

> Truncated meaning shortened in duration or extent",truncatedsvd princip compon analysi truncatedsvd c,"['truncatedsvd', 'princip', 'compon', 'analysi', 'truncatedsvd', 'c']",truncatedsvd   principal component analysis is truncatedsvd on centered data (by per-feature mean su
709,714,"PCA Visualization

A scree plot is a line plot of the principal components vs. eigenvalues. The scree plot is used to determine the number of factors to retain in an exploratory factor analysis or principal components to keep in a principal component analysis",princip compon analysi visual scree plot line plot,"['princip', 'compon', 'analysi', 'visual', 'scree', 'plot', 'line', 'plot']",principal component analysis visualization  a scree plot is a line plot of the principal components 
710,715,"Trace of matrix

The Trace of a Matrix is defined only for a Square Matrix. It is sum of its diagonal elements from the upper left to lower right

Trace of a covariance matrix of shape (n X n) = n, because all the diagonal elements are one

>  The covariance between X1 and X2 indicates how the values of X1 and X2 move relative to each other.
Cov(X1,X1)=1",trace matrix trace matrix defin squar matrix sum d,"['trace', 'matrix', 'trace', 'matrix', 'defin', 'squar', 'matrix', 'sum', 'd']",trace of matrix  the trace of a matrix is defined only for a square matrix. it is sum of its diagona
711,716,"Basics of Anomaly detection 

Anomaly detection is the process of identifying unexpected items or events in data sets, which differ from the norm. And anomaly detection is often applied on unlabeled data which is known as unsupervised anomaly detection.

> Doing PCA partially reduces the anomalies in a dataset

Neural network (RNN, Deep Autoencoders) anomaly detection technique is used for large dataset (> 1lakh experiences)",basic anomali detect anomali detect process identi,"['basic', 'anomali', 'detect', 'anomali', 'detect', 'process', 'identi']",basics of anomaly detection   anomaly detection is the process of identifying unexpected items or ev
712,717,"Assumptions in Anomaly detection 

Anomaly detection has two basic assumptions:

1. Anomalies only occur very rarely in the data.

2. Their features differ from the normal instances significantly.

> Data scaling must be done before performing anomaly detection",assumpt anomali detect anomali detect two basic as,"['assumpt', 'anomali', 'detect', 'anomali', 'detect', 'two', 'basic', 'as']",assumptions in anomaly detection   anomaly detection has two basic assumptions:  1. anomalies only o
713,718,"Finding global outliers-Isolation Forest

Isolation Forest is similar in principle to Random Forest and is built on the basis of decision trees. Isolation Forest, however, identifies anomalies or outliers (for small dataset) rather than profiling normal data points. Here, majority class is 1 and minority class is 0

> It Identifies anomalies as the observations with short average path lengths
> It Splits the data points by randomly selecting a value between the maximum and the minimum of the selected features.

Random Forest is a supervised learning  algorithm and Isolation Forest is an unsupervised learning algorithm (classifier)

A global outlier is a measured sample point that has a very high or a very low value relative to all the values in a dataset.
",find global outliersisol forest isol forest simila,"['find', 'global', 'outliersisol', 'forest', 'isol', 'forest', 'simila']",finding global outliers-isolation forest  isolation forest is similar in principle to random forest 
714,719,"1. Univariate Anomaly Detection

Anomaly detection is carried out on single column of pandas df, like sales column, profit column etc

Visualization of anomalies or outliers through scatter plot

plt.scatter(range(df.shape[0]), np.sort(df['column_name'].values))",1 univari anomali detect anomali detect carri sing,"['1', 'univari', 'anomali', 'detect', 'anomali', 'detect', 'carri', 'sing']","1. univariate anomaly detection  anomaly detection is carried out on single column of pandas df, lik"
715,720,"Python coding for Isolation forest

from sklearn.ensemble import IsolationForest

X=df1['Sales'].values.reshape(-1, 1) # for reshaping in 2D array

clf = IsolationForest(n_estimators=100, contamination=0.01, random_state=0)

clf.fit(X)

df['multivariate_anomaly_score'] = clf.decision_function(X)

df['multivariate_outlier'] = clf.predict(X)

> other hyperparameters of IsolationForest are max_samples
and max_features",python code isol forest sklearnensembl import isol,"['python', 'code', 'isol', 'forest', 'sklearnensembl', 'import', 'isol']",python coding for isolation forest  from sklearn.ensemble import isolationforest  x=df1[sales].value
716,721,"2. Multivariate Anomaly Detection

Most of the analysis that we end up doing are multivariate due to complexity of the world we are living in. 

In multivariate anomaly detection, outlier is a combined unusual score on at least two variables.

X=df[list_of_two_columns]
Other codes are same as univariate

df[(df['outlier_univariate_sales'] == 1) & (df['outlier_univariate_profit'] == 1) & (df['multivariate_outlier'] == -1)] # gives the df of anomalies",2 multivari anomali detect analysi end multivari d,"['2', 'multivari', 'anomali', 'detect', 'analysi', 'end', 'multivari', 'd']",2. multivariate anomaly detection  most of the analysis that we end up doing are multivariate due to
717,722,"Finding local outliers-LOF

from sklearn.neighbors import LocalOutlierFactor

my_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)

other codes as same as isolation forest

 LOF performs well when the density of the data point isn't constant throughout the dataset.

 It gives better results than the global approach to seek out outliers. But, as there's no threshold value of LOF, the choice of a data point as an outlier is user-dependent. 

> Therefore we can run first isolation forest and then LOF to check both

A local outlier is a measured sample point that has a value within the normal range for the entire dataset. Local high or low values",find local outlierslof sklearnneighbor import loca,"['find', 'local', 'outlierslof', 'sklearnneighbor', 'import', 'loca']",finding local outliers-lof  from sklearn.neighbors import localoutlierfactor  my_lof = localoutlierf
718,723,"Visual representation of univariate anomalies

xx = np.linspace(df['Sales'].min(), df['Sales'].max(), len(df)).reshape(-1,1)

anomaly_score = clf.decision_function(xx)

outlier = clf.predict(xx)

plt.figure(figsize=(7,7))
plt.plot(xx, anomaly_score, label='anomaly score')

plt.fill_between(xx.T[0],np.min(anomaly_score),p.max(anomaly_score),where=outlier==-1,color='r',alpha=.4, label='outlier region')",visual represent univari anomali xx nplinspacedfsa,"['visual', 'represent', 'univari', 'anomali', 'xx', 'nplinspacedfsa']","visual representation of univariate anomalies  xx = np.linspace(df[sales].min(), df[sales].max(), le"
719,724,"Deeper Understanding of Anomalies

> Outlier are observations that are distant from the mean or location of a distribution

> Outliers don’t necessarily represent abnormal behavior or behavior generated by a different process

> Anomalies are data patterns that are generated by different processes.

The most common reasons for the outliers are:

> Data errors
> Noise data points
> Hidden patterns in the datasets

Identifying outliers and bad data in our dataset is probably one of the most difficult parts of data cleanup, and it takes time to get right. Even if we have a deep understanding of statistics and how outliers might affect our data, it’s always a topic to explore cautiously.",deeper understand anomali outlier observ distant m,"['deeper', 'understand', 'anomali', 'outlier', 'observ', 'distant', 'm']",deeper understanding of anomalies  > outlier are observations that are distant from the mean or loca
720,725,"Data types of anomaly detection:

> Outliers
> Interquartile range
> Spike
> Level shift",data type anomali detect outlier interquartil rang,"['data', 'type', 'anomali', 'detect', 'outlier', 'interquartil', 'rang']",data types of anomaly detection:  > outliers > interquartile range > spike > level shift
721,726,"Natural Language Understanding (NLU) in four dimensions

Natural language can be oral(speech) or written (text) form

Oral Communication is an informal communication

Written Communication is formal communication

1. Lexical ((Understanding meaning from vocabulary) (understanding the meaning of word by analyzing the structure or parts of words such as stems, root words, prefixes, and suffixes)

> Phoneme-smallest unit of sound (e.g. Ch, Ph etc)
> Morpheme-smallest meaningful unit (e.g. unsual)
> Lexical is a brach of morphology.

2. Syntactic (Understanding meaning from syntax) (understanding the meaning of word/ phrase according to syntax or grammer) 

3. Semantic ((Understanding meaning from context) (understanding the meaning of word/ phrase in relation to context)-used in language model

4. Pragmatic ((Understanding meaning from general use) (understanding the meaning of word/ phrase according to what people mean by the language they use)",natur languag understand nlu four dimens natur lan,"['natur', 'languag', 'understand', 'nlu', 'four', 'dimens', 'natur', 'lan']",natural language understanding (nlu) in four dimensions  natural language can be oral(speech) or wri
722,727,"Important Applications of NLP

1. Question & Answer (Chatbot, Vocebot etc.)
2. Summarizing a paragraph
3. Understanding genuine paragraph  
4. Sentiment analysis etc.

> Modern NLP algorithms are based on machine learning, especially statistical machine
learning.

> Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.",import applic natur languag process 1 question ans,"['import', 'applic', 'natur', 'languag', 'process', '1', 'question', 'ans']","important applications of natural language processing  1. question & answer (chatbot, vocebot etc.) "
723,728,"Feature engineering in NLP

1. Follow 'Steps of Text Pre-processing' as listed in Naive Bayes Classifier
2. During the steps of text pre-processing, along with removing punctuation and stop words, we can perform stemming/ lemmatization (generates the root form of the inflected words) for reducing no. of features/tokens
3. We can also reduce no. of tokens by taking all synonyms as single feature 
4. Removing high frequency words after calculating tf-idf weights",featur engin natur languag process 1 follow step t,"['featur', 'engin', 'natur', 'languag', 'process', '1', 'follow', 'step', 't']",feature engineering in natural language processing  1. follow steps of text pre-processing as listed
724,729,"n-gram in NLP

One word tokens called uni-gram, two words tokens called Bi-grams.

In the fields of computational linguistics and probability, an n-gram is a contiguous (touching) sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.",ngram natur languag process one word token call un,"['ngram', 'natur', 'languag', 'process', 'one', 'word', 'token', 'call', 'un']","n-gram in natural language processing  one word tokens called uni-gram, two words tokens called bi-g"
725,730,"Understanding TF-IDF

> TF-IDF is better technique than CountVectorizer, because in TF-IDF all the document perspective is also considered

tfid_vectorizer = TfidfVectorizer(max_df = 0.9,min_df = 10)

tfidf_matrix=tfid_vectorizer.fit_transform(my_df['tokenized_message'])

dictionary = tfid_vectorizer.vocabulary_.items()

tf-idf weight is product of two terms: the first term is the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm (ln) of the number of the documents in the corpus divided by the number of documents where the specific term appears.

Reason of taking the Term Frequency but inverse of Document Frequency

This is because when a term is appearing in a document multiple times, we want to give more weightage, But when that term is appearing in multiple documents we want to give less weightage. ",understand tfidf tfidf better techniqu countvector,"['understand', 'tfidf', 'tfidf', 'better', 'techniqu', 'countvector']","understanding tf-idf  > tf-idf is better technique than countvectorizer, because in tf-idf all the d"
726,731,"RNN or LSTM 

When the sequence of words matter, we use RNN or LSTM (Long Short Term Memory networks  – are a special kind of RNN). Now a day Transformers are used. Latest is GPT-3 (third generation Generative Pre-trained Transformer)

Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for NLP pre-training developed by Google.",recurr neural network long short term memori seque,"['recurr', 'neural', 'network', 'long', 'short', 'term', 'memori', 'seque']","recurrent neural network or long short term memory   when the sequence of words matter, we use recur"
727,732,"Libraries for NLP

from sklearn import preprocessing

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.model_selection import train_test_split, KFold

from nltk.corpus import stopwords

from nltk.stem.snowball import SnowballStemmer

Famous open source libraries for NLP: Hugging Face, spaCy and nltk are used to run large scale NLP",librari natur languag process sklearn import prepr,"['librari', 'natur', 'languag', 'process', 'sklearn', 'import', 'prepr']",libraries for natural language processing  from sklearn import preprocessing  from sklearn.feature_e
728,733," Right order for a text classification 

1. Text cleaning
2. Text annotation
3. Text to predictors
4. Gradient descent
5. Model tuning
",right order text classif 1 text clean 2 text annot,"['right', 'order', 'text', 'classif', '1', 'text', 'clean', '2', 'text', 'annot']", right order for a text classification   1. text cleaning 2. text annotation 3. text to predictors 4
729,734,"
For language model, conditional probability is rewritten as joint probability

P(A, B) = P(A)* P(B | A)",languag model condit probabl rewritten joint proba,"['languag', 'model', 'condit', 'probabl', 'rewritten', 'joint', 'proba']"," for language model, conditional probability is rewritten as joint probability  p(a, b) = p(a)* p(b "
730,735,"Measuring the complexity of a sentence

> Number of words in a sentence

> Average length of the words in the sentence",measur complex sentenc number word sentenc averag ,"['measur', 'complex', 'sentenc', 'number', 'word', 'sentenc', 'averag']",measuring the complexity of a sentence  > number of words in a sentence  > average length of the wor
731,736,"Basics of Topic modeling

Topic modeling refers to any technique that discovers the hidden semantic structure in a corpus which provides insights into the different themes present in the texts. 

Sentiment analysis is the process of identifying the emotions and opinions expressed in a particular text.

> Corpus means a collection of written texts, especially the entire works of a particular author

Topic modelling refers to the task of identifying topics that best describes a set of documents. These topics will only emerge during the topic modelling process (therefore called latent). And one popular topic modeling technique is known as Latent Dirichlet Allocation (LDA).

> LDA and PCA both does dimensionality reduction",basic topic model topic model refer techniqu disco,"['basic', 'topic', 'model', 'topic', 'model', 'refer', 'techniqu', 'disco']",basics of topic modeling  topic modeling refers to any technique that discovers the hidden semantic 
732,737,"Basic assumptions of all topic models:

1. Each document is a collection of latent(hidden) topics

2. Each topic is a collection of words",basic assumpt topic model 1 document collect laten,"['basic', 'assumpt', 'topic', 'model', '1', 'document', 'collect', 'laten']",basic assumptions of all topic models:  1. each document is a collection of latent(hidden) topics  2
733,738,"Libraries for Topic Modeling

from IPython.display import display

from tqdm import tqdm

from collections import Counter

import ast

import matplotlib.mlab as mlab

from sklearn.feature_extraction.text import CountVectorizer

from textblob import TextBlob

import scipy.stats as stats

from sklearn.decomposition import TruncatedSVD

from sklearn.decomposition import LatentDirichletAllocation

from sklearn.manifold import TSNE

output_notebook()",librari topic model ipythondisplay import display ,"['librari', 'topic', 'model', 'ipythondisplay', 'import', 'display']",libraries for topic modeling  from ipython.display import display  from tqdm import tqdm  from colle
734,739,"Understanding LDA

LDA takes our document-term matrix as input and yield an  n×N  topic matrix as output, where  N  is the number of topic categories (which we supply as a hyperparameter).

> LDA is a generative probabilistic process, designed with the specific goal of uncovering latent topic structure in text corpora.

> Term matrix is not a so called matrix

LDA is a computationally heavy process

So, we take random sample (say 1000 examples) from large text dataset  and then convert it into small_docment_term_matrix by applying countvectorizer/tf-idf vectorizer before passing the text dataset to LDA algorithm

In LDA, Dirichlet Distribution is used.

Any set where the numbers sum up to 1 is called  Dirichlet Distribution. 
e.g. {0.6,0.4} is a dirichlet distribution or a probability simplex (a set where each point represents a probability distribution between a finite number of mutually exclusive events). Here 0.6 and 0.4 are the probability distributions

> A document is a probability distribution of latent topics (alpha) and a topic is a probability distribution of words (beta)

As LDA is generative probabilistic process, we can think LDA as a machine generates document based on random probability distribution settings alpha and beta.

Then the machine finds the best fit of the generated document with the set of documents fitted into by adjusting the parameters alpha and beta.

Then it gives us the topic of the document",understand latent dirichlet alloc latent dirichlet,"['understand', 'latent', 'dirichlet', 'alloc', 'latent', 'dirichlet']",understanding latent dirichlet allocation  latent dirichlet allocation takes our document-term matri
735,740,"Plate Notation

Plate notation is a concise way of visually representing the dependencies among model parameters (alpha and beta).

Large rectangle,M denotes the entire set of documents. Small rectange, N denotes sub-set or sample set of examples. Theta and Phi are the multinomial distribution.",plate notat plate notat concis way visual repres d,"['plate', 'notat', 'plate', 'notat', 'concis', 'way', 'visual', 'repres', 'd']",plate notation  plate notation is a concise way of visually representing the dependencies among mode
736,741,"Python coding of LDA model 

lda = LatentDirichletAllocation()

grid_params={'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}

lda_model = GridSearchCV(lda,param_grid=grid_params)

lda_model.fit(small_document_term_matrix)

best_lda_model = lda_model.best_estimator_",python code latent dirichlet alloc model latent di,"['python', 'code', 'latent', 'dirichlet', 'alloc', 'model', 'latent', 'di']",python coding of latent dirichlet allocation model   latent dirichlet allocation = latentdirichletal
737,742,"Visualization of LDA

!pip install pyLDAvis

import pyLDAvis.sklearn

pyLDAvis.enable_notebook()

lda_panel = pyLDAvis.sklearn.prepare(best_lda_model, small_document_term_matrix,small_count_vectorizer,mds='tsne')
lda_panel",visual latent dirichlet alloc pip instal pylat dir,"['visual', 'latent', 'dirichlet', 'alloc', 'pip', 'instal', 'pylat', 'dir']",visualization of latent dirichlet allocation  !pip install pylatent dirichlet allocationvis  import 
738,743," Cosine Similarity

Machine learning uses Cosine Similarity in applications such as data mining and information retrieval. This allows for a Cosine Similarity measurement to distinguish and compare documents to each other based upon their similarities and overlap of subject matter or context.

Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between the two vectors.

If the ange between the vectors is zero, then cosine similarity is 1, means the vectors (word, expression or sentence) are fully similar. Consine similarity 0 means no match

> By using cosine similarity,we can easily find how two words (word embedding vectors) are similar or opposite to each other.",cosin similar machin learn use cosin similar appli,"['cosin', 'similar', 'machin', 'learn', 'use', 'cosin', 'similar', 'appli']", cosine similarity  machine learning uses cosine similarity in applications such as data mining and 
739,744,"Basics of Recommender System

The objective of a Recommender System is to recommend relevant items for users, based on their preference. Preference and relevance are subjective, and they are generally inferred by items users have consumed previously

Recommender systems have a problem known as user cold-start, in which it is hard to provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.",basic recommend system object recommend system rec,"['basic', 'recommend', 'system', 'object', 'recommend', 'system', 'rec']",basics of recommender system  the objective of a recommender system is to recommend relevant items f
740,745,"Popular Recommender Systems

1. Collaborative Filtering

2. Content-Based Filtering

3. Hybrid Approach",popular recommend system 1 collabor filter 2 conte,"['popular', 'recommend', 'system', '1', 'collabor', 'filter', '2', 'conte']",popular recommender systems  1. collaborative filtering  2. content-based filtering  3. hybrid appro
741,746,"Collaborative Filtering

This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating)",collabor filter method make automat predict filter,"['collabor', 'filter', 'method', 'make', 'automat', 'predict', 'filter']",collaborative filtering  this method makes automatic predictions (filtering) about the interests of 
742,747,"Content-Based Filtering

This method uses only information about the description and attributes of the items users has previously consumed to model user's preferences. 

In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present)

Here we are using a very popular technique in information retrieval (search engines) named TF-IDF.

We model an user profile through which we can find the top tokens and their relevance. 

Thus we can recommend article for that user.

To model the user profile, we take all the item profiles the user has interacted and average them. The average is weighted by the interaction strength, in other words, the articles the user has interacted the most (eg. liked or commented) will have a higher strength in the final user profile.

Here, we compute the cosine similarity between the user profile and all item profiles

from sklearn.metrics.pairwise import cosine_similarity

array_vec_1 = np.array([[12,41,60,11,21]])
array_vec_2 = np.array([[12,41,60,11,14]])

cosine_similarity(array_vec_1 , array_vec_2)",contentbas filter method use inform descript attri,"['contentbas', 'filter', 'method', 'use', 'inform', 'descript', 'attri']",content-based filtering  this method uses only information about the description and attributes of t
743,748,"Hybrid Approach

Recent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective than pure approaches in some cases. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.",hybrid approach recent research demonstr hybrid ap,"['hybrid', 'approach', 'recent', 'research', 'demonstr', 'hybrid', 'ap']","hybrid approach  recent research has demonstrated that a hybrid approach, combining collaborative fi"
744,749,"Implementation strategies of Collaborative Filtering

Collaborative Filtering (CF) has two main implementation strategies:

1. Memory-based: This approach uses the memory of previous users interactions to compute users similarities based on items they've interacted (user-based or  User Neighbourhood-based approach) or compute items similarities based on the users that have interacted with them (item-based approach)

> Finds similar users based on cosine similariry or pearson correlation

2. Model-based: In this approach, models are developed using different machine learning algorithms to recommend items to users. There are many model-based CF algorithms, like Neural Networks, Bayesian Networks, Clustering Techniques, and Latent Factor Models such as Singular Value Decomposition (SVD) and Probabilistic Latent Semantic Analysis.

> Disadvantage of model-based approach: Inference is intracable because of hidden/latent factors",implement strategi collabor filter collabor filter,"['implement', 'strategi', 'collabor', 'filter', 'collabor', 'filter']",implementation strategies of collaborative filtering  collaborative filtering (collaborative filteri
745,750,"Latent factor models

Latent factor models compress user-item matrix into a low-dimensional representation in terms of latent factors.

One advantage of using this approach is that instead of having a high dimensional matrix containing abundant number of missing values we will be dealing with a much smaller matrix in lower-dimensional space.",latent factor model latent factor model compress u,"['latent', 'factor', 'model', 'latent', 'factor', 'model', 'compress', 'u']",latent factor models  latent factor models compress user-item matrix into a low-dimensional represen
746,751,"Matrix Factorization

Like factorization of any number into prime numbers, matrix can also be factorized into set of matrices(U, sigma, Vt). 

This is called singular value dicomposition.

Singular matrix means a matrix with determinant zero

The higher the number of factors, the more precise is the factorization in the original matrix reconstructions. 

> Thus, for model generalization (simplification), we need to decrease the no. of factors",matrix factor like factor number prime number matr,"['matrix', 'factor', 'like', 'factor', 'number', 'prime', 'number', 'matr']","matrix factorization  like factorization of any number into prime numbers, matrix can also be factor"
747,752,"Implementation of SVD

from sklearn.feature_extraction.text import TfidfVectorizer

from scipy.sparse.linalg import svds

Step-1: Data Munging (the procedure for transforming data from erroneous or unusable forms)

We define a dictionary for event strength and map all the user interactions.

For example, a comment in an article indicates a higher interest of the user on the item than a like, or than a simple view.

Step-2:

Then, to model the user interest on a given article, we aggregate all the interactions the user has performed in an item by a weighted sum of interaction type strength and apply a log transformation to smoothen the distribution.

Step-3: 

Train-test splitting of the dataset

Step-4:

Then, we apply pivot method on train df

users_items_pivot_matrix_df = interactions_train_df.pivot(index='personId', columns='contentId',  values='eventStrength')

Step-5: Pivot df to pivot matrix conversion

users_items_pivot_matrix = users_items_pivot_matrix_df.values

Step-6: Performs matrix factorization

U, sigma, Vt = svds(users_items_pivot_matrix, k =15)

where k is the number of matrix factorization

Step-7:

After the factorization, we try to to reconstruct the original matrix by multiplying its factors. The resulting matrix is not sparse any more. It was generated predictions for items the user have not yet interaction, which we will exploit for recommendations.

all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 


Step-8: Writing the class for CFRecommender

class CFRecommender:
    
   def __init__(self, cf_predictions_df, items_df=None):
.
.
return recommendations_df

Step-9: Evaluation

We choose to work with Top-N accuracy metrics, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.",implement singular valu decomposit sklearnfeaturee,"['implement', 'singular', 'valu', 'decomposit', 'sklearnfeaturee']",implementation of singular value decomposition  from sklearn.feature_extraction.text import tfidfvec
748,753,"

Metrices used for evaluation of Recommender systems

The Top-N accuracy metric choosen was Recall@N which evaluates whether the interacted item is among the top N items (hit) in the ranked list of  recommendations for a user.

Other metrices used for evaluation: Top-N Recall, Top-N Precision, MAP@K",metric use evalu recommend system topn accuraci me,"['metric', 'use', 'evalu', 'recommend', 'system', 'topn', 'accuraci', 'me']",  metrices used for evaluation of recommender systems  the top-n accuracy metric choosen was recall@
749,754,"Shortcoming of content-based recommender systems

Users will only get recommendations related to their preferences in their profile, and recommender engine may never recommend any item with other characteristics.",shortcom contentbas recommend system user get reco,"['shortcom', 'contentbas', 'recommend', 'system', 'user', 'get', 'reco']",shortcoming of content-based recommender systems  users will only get recommendations related to the
750,755,"Basics of Time Series

Time series is a series of data points indexed (or listed or graphed) in time order.

Time Series Analysis is to predict any future event

Time interval of time series data can be anything from millisecond level to year level or larger

This is a supervised or sudo supervised algorithm, because here the past data is considered as labels

> Graph of time series is called 
Historigram

> For checking randomness in time series autocorrelation plot is used
",basic time seri time seri seri data point index li,"['basic', 'time', 'seri', 'time', 'seri', 'seri', 'data', 'point', 'index', 'li']",basics of time series  time series is a series of data points indexed (or listed or graphed) in time
751,756,"Components of Time Series

Time series can be decomposed into four components

1. Secular trend or Trend-'T' (the overall movement of a curve)

2. Seasonal variations or Seasonality-'S' (having period)-variation occurring within parts of a year

3. Cyclical fluctuations-'C' (which correspond to periodical but not seasonal variations)-e.g. Prosperity, Recession, and depression in a business

4. Irregular variations or Noise or Error-'I' (residual)",compon time seri time seri decompos four compon 1 ,"['compon', 'time', 'seri', 'time', 'seri', 'decompos', 'four', 'compon', '1']",components of time series  time series can be decomposed into four components  1. secular trend or t
752,757,"Multiplicative Model

The original time series is expressed as the product of trend, seasonal,cyclical and irregular components.

T*S*C*I",multipl model origin time seri express product tre,"['multipl', 'model', 'origin', 'time', 'seri', 'express', 'product', 'tre']","multiplicative model  the original time series is expressed as the product of trend, seasonal,cyclic"
753,758,"Ways to approach a Time Series Prediction Problem

1. Time Series Approach

Time Series Approaches are Naive Forecast, Moving Average (MA),  Weighted average, Exponential smoothing, Double exponential smoothing and Econometric approach

2. Machine Learning Approach",way approach time seri predict problem 1 time seri,"['way', 'approach', 'time', 'seri', 'predict', 'problem', '1', 'time', 'seri']",ways to approach a time series prediction problem  1. time series approach  time series approaches a
754,759,"1. Naive Forecast

Estimating technique in which the last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors (unintended contributor to an incident).",1 naiv forecast estim techniqu last period actual ,"['1', 'naiv', 'forecast', 'estim', 'techniqu', 'last', 'period', 'actual']",1. naive forecast  estimating technique in which the last periods actuals are used as this periods f
755,760,"2. Moving Average (MA)

A simple moving average (SMA) is a prediction that takes the k historical points. 

This is one step advancement of Naive forecast.

MA may be used to smoothen a time series. (level of smoothening depends on the value of k)

> The moving average is free from the influences of seasonal and irregular variations

> Simple average method for finding out seasonal indices is good when the time-series has no trend and cyclic variation",2 move averag simpl move averag sma predict take k,"['2', 'move', 'averag', 'simpl', 'move', 'averag', 'sma', 'predict', 'take', 'k']",2. moving average (ma)  a simple moving average (sma) is a prediction that takes the k historical po
756,761,"3. Weighted average is a simple modification to the moving average. The weights sum up to 1 with larger weights assigned to more recent observations.

",3 weight averag simpl modif move averag weight sum,"['3', 'weight', 'averag', 'simpl', 'modif', 'move', 'averag', 'weight', 'sum']",3. weighted average is a simple modification to the moving average. the weights sum up to 1 with lar
757,762,"4. Exponential smoothing

 Instead of weighting the last  k  values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time.

Smoothening parameter alpha is used here instead of weight, w

Like Naive forecast, MA and weighted average, it also predicts one data point in the future.

When we try to predict multiple data points (say, forecasting for horizon, h = 8) in future, general tendency of our model is the accuracy of our prediction decreases with time. Lag 0 prediction accuracy will be very high and lag n prediction accuracy will be very low.",4 exponenti smooth instead weight last k valu time,"['4', 'exponenti', 'smooth', 'instead', 'weight', 'last', 'k', 'valu', 'time']","4. exponential smoothing   instead of weighting the last  k  values of the time series, we start wei"
758,763,"5. Double exponential smoothing (Holt’s linear trend method)

Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level or intercept and one for the trend)

Here, we use two smoothening paramaters, alpha and beta.",5 doubl exponenti smooth holt linear trend method ,"['5', 'doubl', 'exponenti', 'smooth', 'holt', 'linear', 'trend', 'method']",5. double exponential smoothing (holt’s linear trend method)  holt (1957) extended simple exponentia
759,764,"Statistical technique in Time Series

To apply any statistical technique (1 to 6) to forecast a time series, the main assumption is that the time seires is stationary.

Stationarity: If a process is stationary, that means it does not change its statistical properties over time, namely its mean, variance and auto-correlation (means correlating current value with previous value). 

Apart from visual inspection of time series to check stationarity, Dickey-Fuller test is there for stationarity check",statist techniqu time seri appli statist techniqu ,"['statist', 'techniqu', 'time', 'seri', 'appli', 'statist', 'techniqu']",statistical technique in time series  to apply any statistical technique (1 to 6) to forecast a time
760,765,"6. Econometric approach

Realworld time seires are non-stationary in nature. 

So, we apply different transformation to make it stationary.",6 econometr approach realworld time seir nonstatio,"['6', 'econometr', 'approach', 'realworld', 'time', 'seir', 'nonstatio']","6. econometric approach  realworld time seires are non-stationary in nature.   so, we apply differen"
761,766,"Understanding ARIMA

Autoregressive Integrated Moving Average model

ARIMA - Parameters

p: Trend autoregression order 
d: Trend difference order.
q: Trend moving average order.

> A statistical model is autoregressive if it predicts future values based on past values.",understand arima autoregress integr move averag mo,"['understand', 'arima', 'autoregress', 'integr', 'move', 'averag', 'mo']",understanding arima  autoregressive integrated moving average model  arima - parameters  p: trend au
762,767,"Understanding SARIMA

Seasonal Autoregressive Integrated Moving Average model

It is used when there is seasonal component in the time series.

SARIMA - Parameters

P: Seasonal autoregressive order.
D: Seasonal difference order.
Q: Seasonal moving average order.",understand sarima season autoregress integr move a,"['understand', 'sarima', 'season', 'autoregress', 'integr', 'move', 'a']",understanding sarima  seasonal autoregressive integrated moving average model  it is used when there
763,768,"Libraries for Time Series Analysis

from dateutil.relativedelta import relativedelta

from scipy.optimize import minimize

import statsmodels.formula.api as smf

import statsmodels.tsa.api as smt

import statsmodels.api as sm

import scipy.stats as scs

from itertools import product  
                 
from tqdm import tqdm_notebook",librari time seri analysi dateutilrelativedelta im,"['librari', 'time', 'seri', 'analysi', 'dateutilrelativedelta', 'im']",libraries for time series analysis  from dateutil.relativedelta import relativedelta  from scipy.opt
764,769,"Machine learning approach in Time series

Other than time stamp as index column and dependent variable (y) column, there can be external factors columns to help prediction of y. 

To solve time series prediction problem by machine learning approach, we need to convert the two column ts dataset into a supervised learning dataset where there will be dependent variables or features (X's).

We can use any supervised ml algorithm after creating the features in ts

'S' related features can be created from time stamp by extracting year, month etc.

Autoregression (AR) related features can be lag 1, lag 2, lag 3 and other lag values of y.

'I' related features by taking the differences between lag values

'MA' related features can be MA3, MA4 etc. 

External or exogenous factors, Target encoding, Forecasts from other models and many more ideas can be features",machin learn approach time seri time stamp index c,"['machin', 'learn', 'approach', 'time', 'seri', 'time', 'stamp', 'index', 'c']",machine learning approach in time series  other than time stamp as index column and dependent variab
765,770,"Downside of SARIMA

The biggest downside of SARIMA is we need to manually check if the data is stationary or not for applying SARIMA. But in reallife we need to attempt multiple time series, in the order of thousand to solve one ts problem. So, manual checking of stationarity is not feassible.

Thus, the machine learning approach came into picture.",downsid sarima biggest downsid sarima need manual ,"['downsid', 'sarima', 'biggest', 'downsid', 'sarima', 'need', 'manual']",downside of sarima  the biggest downside of sarima is we need to manually check if the data is stati
766,771,"Understanding Leakage

In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time.

Leakage means 'will I know the value of the feature during prediction'",understand leakag statist machin learn leakag also,"['understand', 'leakag', 'statist', 'machin', 'learn', 'leakag', 'also']","understanding leakage  in statistics and machine learning, leakage (also known as data leakage or ta"
767,772,"Prophet or Facebook Prophet

It is an open-source library for univariate (one variable) time series forecasting developed by Facebook.

> It converts a time series problem into machine learning problem

> In PROPHET, we can specify holidays and many more

https://facebook.github.io/prophet/docs/quick_start.html#python-api

from fbprophet import Prophet

model = Prophet()

model.fit(pjme_train.reset_index()     .rename(columns={'Datetime':'ds',
  'PJME_MW':'y'}))

> For seasonal smooth pattern generally we use prophet

> For non-seasonal smooth pattern we generally use ml algorithm",prophet facebook prophet opensourc librari univari,"['prophet', 'facebook', 'prophet', 'opensourc', 'librari', 'univari']",prophet or facebook prophet  it is an open-source library for univariate (one variable) time series 
768,773,"Cross Validation in Time Series

We can not randomly shuffle k-folds cross validation on ts, because here, y is highly dependent on sequential time.

So, here cross validation shall be done by k-folds sequentially. This is called temporal cross-validation.

",cross valid time seri random shuffl kfold cross va,"['cross', 'valid', 'time', 'seri', 'random', 'shuffl', 'kfold', 'cross', 'va']","cross validation in time series  we can not randomly shuffle k-folds cross validation on ts, because"
769,774,"Demand pattern classification for choosing the right model in time series

1. Intermittent- The demand history shows very little variation in demand quantity but a high variation in the interval between two demands
2. Smooth-The demand is very regular in time and in quantity
3. Lumpy- The demand is characterized by a large variation in quantity and in time
4. Erratic- The demand has regular occurrences in time with high quantity variations",demand pattern classif choos right model time seri,"['demand', 'pattern', 'classif', 'choos', 'right', 'model', 'time', 'seri']",demand pattern classification for choosing the right model in time series  1. intermittent- the dema
770,775,"Product forecastability

To determine a product forecastability, we apply two coefficients:

Average Demand Interval (ADI). It measures the demand regularity in time by computing the average interval between two demands.

Square of the Coefficient of Variation (CV²). It measures the variation in quantities.

1. Smooth demand (ADI < 1.32 and CV² < 0.49)

2. Intermittent demand (ADI >= 1.32 and CV² < 0.49)

3. Erratic demand (ADI < 1.32 and CV² >= 0.49)

4. Lumpy demand (ADI >= 1.32 and CV² >= 0.49)",product forecast determin product forecast appli t,"['product', 'forecast', 'determin', 'product', 'forecast', 'appli', 't']","product forecastability  to determine a product forecastability, we apply two coefficients:  average"
771,776,"Basics of case interview

A case interview is a real client problem that we must solve in about 15-30 minutes during a job interview.

> Expecting answer or decision from the trained and tested data scientist learning model",basic case interview case interview real client pr,"['basic', 'case', 'interview', 'case', 'interview', 'real', 'client', 'pr']",basics of case interview  a case interview is a real client problem that we must solve in about 15-3
772,777,"Categorization of case studies

1. Estimate the size of something

2. Fix something bad 


3. Take a strategic decision

4. Improve something 
",categor case studi 1 estim size someth 2 fix somet,"['categor', 'case', 'studi', '1', 'estim', 'size', 'someth', '2', 'fix', 'somet']",categorization of case studies  1. estimate the size of something  2. fix something bad    3. take a
773,778,"Guesstimate (Problem Solving Approach)

1. Clarify the problem statement

 > Need to ask all the questions to make a clear problem statement

> Choose between supply side or demand side approach 
(if applicable)

> Confirm assumptions if any

2. Break down into sub-problems 

> Check for bottleneck if any (in case of demand side approach)

> Prepare a diagram considering the dependent variable and all independent variables)

3. Solve sub-problems 

> Understand the filters if any (Geography, gender, life expectancy, income level etc.)

> guesstimate the values of all independent variables

> Take logical simple numbers for easy calculation

> Avoid zero error using exponents

4. Consolidate our findings 
(Create a learning function)",guesstim problem solv approach 1 clarifi problem s,"['guesstim', 'problem', 'solv', 'approach', '1', 'clarifi', 'problem', 's']",guesstimate (problem solving approach)  1. clarify the problem statement   > need to ask all the que
774,779,"Meaning of Diagnosis  

As in what is the true definition of Diagnosis cannot be a few signs and symptoms but needs a computer to analyse various relation and metrics together.",mean diagnosi true definit diagnosi cannot sign sy,"['mean', 'diagnosi', 'true', 'definit', 'diagnosi', 'cannot', 'sign', 'sy']",meaning of diagnosis    as in what is the true definition of diagnosis cannot be a few signs and sym
775,780,"ML in Healthcare

The implementation of EHR i.e. Electronic health records started around 2010

Adoption of Machine learning in other domains like finance, retail etc is easier as compared to Healthcare

1. Use of Computer Vision 

i.Medical Imaging

ii.Nuclear Medicine

2. Use of NLP 

i.Physician Notes

ii.Nurse Notes

iii.Speech to text 

iv.Billing Denials annotation

3. Use of Tabular data based machine learning

i.Prediction of Length of Stay

ii.Readmission of a Patient

iii.Early detection of Disease",machin learn healthcar implement ehr ie electron h,"['machin', 'learn', 'healthcar', 'implement', 'ehr', 'ie', 'electron', 'h']",machine learning in healthcare  the implementation of ehr i.e. electronic health records started aro
776,781,"Geometric Mean Length of Stay

- A standard LOS (Length of Stay) metric set by the CMS (Centers for Medicare & Medicaid Services)",geometr mean length stay standard length stay leng,"['geometr', 'mean', 'length', 'stay', 'standard', 'length', 'stay', 'leng']",geometric mean length of stay  - a standard length of stay (length of stay) metric set by the cms (c
777,782,"Features for the prediction of LOS

1. Demography related features

i.Patient Age
ii.Gender
iii.Marital Status
iv.Ethnicity 

2. Vitals related features

i.Heart Rate
ii.Blood Pressure
iii.Oxygen level iv.Temperature

3. Utilization related features

i.Surgery Hours
ii.Number of Past visits to Hospitals
iii.Number of Surgeries

4. Lab report related features

i.Labs_Creatinine
ii.Lab_Platelet Count iii.Lab_Magnesium iv.Lab_Phosporus 

5. Diagnosis related features

i.Arthritis
ii.Heart failure iii.Diabetes mellitus 
iv.Acute kidney failure ",featur predict length stay 1 demographi relat feat,"['featur', 'predict', 'length', 'stay', '1', 'demographi', 'relat', 'feat']",features for the prediction of length of stay  1. demography related features  i.patient age ii.gend
778,783,"Understanding Fraud

“Fraud” is an intended action or set of actions by users to cause harm to Business as Usual

1. Fraud in the context of Credit Risk

It is different from “Loan Default”, which is a genuine inability of the customer to complete a certain action or abide by certain rules

2. Fraud in the context of E-commerce

It is a genuine attempt by the customer to cause financial losses to the company by unnecessarily increasing their purchasing costs, forward and reverse logistics costs etc",understand fraud “fraud” intend action set action ,"['understand', 'fraud', '“fraud”', 'intend', 'action', 'set', 'action']",understanding fraud  “fraud” is an intended action or set of actions by users to cause harm to busin
779,784,"WAYS TO CAPTURE FRAUDULENT BEHAVIOURS

Bank Account Opening

During account opening, typing speed, swipe patterns, and every click of the mouse tells a story – one of cybercriminal activity or genuine user behavior. BioCatch pulls together advanced behavioral insights to empower organizations with increased visibility into risk. Even when we have never seen a user before, BioCatch quickly spots trusted behaviors to create a smooth customer journey during account opening.",way captur fraudul behaviour bank account open acc,"['way', 'captur', 'fraudul', 'behaviour', 'bank', 'account', 'open', 'acc']","ways to capture fraudulent behaviours  bank account opening  during account opening, typing speed, s"
780,785,"Social Engineering Scams

Four ways behavioral biometrics can uncover a real time social engineering scam:

> Length of session: Sessions are longer and behaviors such as aimless moouse movements are common indicating a person is waiting for instructions

> Segmented typing: These patterns indicate dictation such as a cybercriminal reading off an account number to transfer funds to

> Hesitation: Longer pauses before performing simple intuitive actions such as clicking on the submit button

> Displacement: Continuous movement of the phone suggests the user is picking the phone up to take instructionsand placing it back down to perform the actions instructed by the cybecriminals",social engin scam four way behavior biometr uncov ,"['social', 'engin', 'scam', 'four', 'way', 'behavior', 'biometr', 'uncov']",social engineering scams  four ways behavioral biometrics can uncover a real time social engineering
781,786,"Synthetic Minority Over-sampling Technique

Classification for Modeling for frauds will require SMOTE as incidence rate is pretty low

SMOTE: Synthetic Minority Over-sampling Technique

An approach to the construction of classifiers from imbalanced datasets",synthet minor oversampl techniqu classif model fra,"['synthet', 'minor', 'oversampl', 'techniqu', 'classif', 'model', 'fra']",synthetic minority over-sampling technique  classification for modeling for frauds will require synt
782,787,"C4-Curious Case of Customer Credit

Egger to know the whole analysis for customer credit in some banking sector",c4curious case custom credit egger know whole anal,"['c4curious', 'case', 'custom', 'credit', 'egger', 'know', 'whole', 'anal']",c4-curious case of customer credit  egger to know the whole analysis for customer credit in some ban
783,788,"Overall Objective of Credit Risk

Wishes to optimise the portfolio by reducing the Risk

Solutions

1. Predictive Analytics

2. Prescriptive Analytics
",overal object credit risk wish optimis portfolio r,"['overal', 'object', 'credit', 'risk', 'wish', 'optimis', 'portfolio', 'r']",overall objective of credit risk  wishes to optimise the portfolio by reducing the risk  solutions  
784,789,"Predictive Analytics of Credit Risk

Model that would predict loan default

 a.Classification Problem

b.Recall to be given more importance

Features of Predictive Analytics

1.Loan amount, terms
2.Customer default history
a.3 month, 6 month, 9 month
b.Customers outstanding debts/obligations etc

3.Customer demography
a.Age
b.Salary
c.Disposable income
d.PPP etc

4.City level features etc",predict analyt credit risk model would predict loa,"['predict', 'analyt', 'credit', 'risk', 'model', 'would', 'predict', 'loa']",predictive analytics of credit risk  model that would predict loan default   a.classification proble
785,790,"Prescriptive Analytics helps us draw up specific recommendations by deeply looking into the ""what"" and ""why"" of a potential future outcome.

It utilizes complicated mathematical algorithms, artificial intelligence and machine learning.
",prescript analyt help us draw specif recommend dee,"['prescript', 'analyt', 'help', 'us', 'draw', 'specif', 'recommend', 'dee']","prescriptive analytics helps us draw up specific recommendations by deeply looking into the ""what"" a"
786,791,"Metrics that help in evaluating our model’s accuracy

1. Confusion Matrix
2. F1 Score
3. Gain and Lift Charts
4. Kolmogorov Smirnov Chart (KS score)
5. ROC-AUC
6. Log Loss
7. Gini Coefficient
8. Concordant – Discordant Ratio
9. Root Mean Squared Error",metric help evalu model accuraci 1 confus matrix 2,"['metric', 'help', 'evalu', 'model', 'accuraci', '1', 'confus', 'matrix', '2']",metrics that help in evaluating our model’s accuracy  1. confusion matrix 2. f1 score 3. gain and li
787,792,"Model Health using KS Scores

> Used for classification models. 

> KS is a measure of the degree of separation between the positive and negative distributions.

> The Sooner the Highest KS value occurs, the better the segregation of the goods and the bads

> Gains Chart study coupled with Confusion matrix study will govern final answer
",model health use ks score use classif model ks mea,"['model', 'health', 'use', 'ks', 'score', 'use', 'classif', 'model', 'ks', 'mea']",model health using ks scores  > used for classification models.   > ks is a measure of the degree of
788,793,"Decision Making using Risk Bins

Depending on risk appetite and customer acquisition targets, we can choose the bins post which we wish to stop booking customers",decis make use risk bin depend risk appetit custom,"['decis', 'make', 'use', 'risk', 'bin', 'depend', 'risk', 'appetit', 'custom']","decision making using risk bins  depending on risk appetite and customer acquisition targets, we can"
789,794,"C4 -Curious Case of Customer Contacts 

Egger to know the whole analysis for customer contacts in some E-commerce sector",c4 curious case custom contact egger know whole an,"['c4', 'curious', 'case', 'custom', 'contact', 'egger', 'know', 'whole', 'an']",c4 -curious case of customer contacts   egger to know the whole analysis for customer contacts in so
790,795,"Important Features in E-commerce

1.Issue Category Wise analysis
2.Product type wise analysis
3.Customer demographic wise analysis
a.Area
b.Purchase history
c.Mode of payment

4.Price of product
5.Courier Mode
6.Time of order placement
7.Seller profile
a.Seller rating
b.Seller return history

8.Time taken to place the order
9.Return Logistics Partner wise
",import featur ecommerc 1issu categori wise analysi,"['import', 'featur', 'ecommerc', '1issu', 'categori', 'wise', 'analysi']",important features in e-commerce  1.issue category wise analysis 2.product type wise analysis 3.cust
791,796,"Predictive Analytics in  E-commerce

1.Model that would predict a contact on a ticket and help solve it before contact happens

a.Classification Problem

b.Recall to be given more importance

2.Model that predicts number of contacts to streamline workforce

a.Regression problem
b.Help in WFM",predict analyt ecommerc 1model would predict conta,"['predict', 'analyt', 'ecommerc', '1model', 'would', 'predict', 'conta']",predictive analytics in  e-commerce  1.model that would predict a contact on a ticket and help solve
792,797,"Understanding UNIX Operating System

UNIX (also referred to as UNICS) is UNiplexed Information Computing System


● UNIX Operating System was first developed in the 1960s at AT&T Labs.

● Multi-user, multi-tasking system for servers, desktops and laptops.

● Most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X.",understand unix oper system unix also refer unic u,"['understand', 'unix', 'oper', 'system', 'unix', 'also', 'refer', 'unic', 'u']",understanding unix operating system  unix (also referred to as unics) is uniplexed information compu
793,798,"Kernel and shell of UNIX

Unix Operating System is made up of three parts: the kernel, the shell and the programs.

●Kernel is the main OS program to access the storage (local or remote)

● Shell is the application program for the users to access the storage through kernel.

● Program means all the data or installed application softwares.",kernel shell unix unix oper system made three part,"['kernel', 'shell', 'unix', 'unix', 'oper', 'system', 'made', 'three', 'part']","kernel and shell of unix  unix operating system is made up of three parts: the kernel, the shell and"
794,799,"Understanding LINUX Operating System

LINUX stands for Lovable Intellect Not Using XP. Linux was developed by Linus Torvalds and named after him. 

Linux is an open-source and community-developed operating system for computers, servers, mainframes, mobile devices, and embedded devices.

Linux offers great speed and security, on the other hand, Windows offers great ease of use",understand linux oper system linux stand lovabl in,"['understand', 'linux', 'oper', 'system', 'linux', 'stand', 'lovabl', 'in']",understanding linux operating system  linux stands for lovable intellect not using xp. linux was dev
795,800,"Time Complexity and  Space Complexity

Performance parameters of any algorithm or application (be it Local or cloud) are as follows:

1. Time complexity or Speed (Time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the input.)

2. Space Complexity (Space complexity of an algorithm quantifies the amount of space or memory taken by an algorithm to run as a function of the length of the input)
3. Ease of use
4. Reliability
5. Security

Time and space complexity depends on lots of things like hardware, operating system, processors, etc. However, we don't consider any of these factors while analyzing the algorithm. We will only consider the time taken and space comsumed by an algorithm.

> Always we need to reduce the iterations in our algorithm to to reduce to run time. We have to think about the loop which is running behind any simple function or method

> We will always take worst case scienario to define the performance of any algorithm

> As a first step of reducing the iterations, we should sort the list or array. Then, we can ignore any part of the list as per our requirement. This will help us to cut the linkage of algorithm's speed with the length of input (length of input may be very large)

> Eficient alternative to loop is writing function for repitivite work. Within the function, we should try to avoid loop and get things done by conditional operators

> When we call same function within itself, it is called recursion",time complex space complex perform paramet algorit,"['time', 'complex', 'space', 'complex', 'perform', 'paramet', 'algorit']",time complexity and  space complexity  performance parameters of any algorithm or application (be it
796,801,"Ways to connect to an EC2 instance

AWS means Amazon Web Services

AWS free tier EC2 is a free Cloud computer

One EC2 instance is like one remote computer (virtual machine) running generally on Linux and on which we can install whatever software we want.

EC means Elastic Compute. So, we can resize this remote computer as per our requirement.

There are many ways to connect to an EC2 instance

1.  Putty

2. Git Bash

3. AWS console panel

4. Command prompt

5. Windows powershell or any other terminal

> windows PowerShell is a more advanced version of cmd (command prompt). It is not only an interface but also a scripting language that is used to carry out administrative tasks more easily.",way connect ec2 instanc aw mean amazon web servic ,"['way', 'connect', 'ec2', 'instanc', 'aw', 'mean', 'amazon', 'web', 'servic']",ways to connect to an ec2 instance  aws means amazon web services  aws free tier ec2 is a free cloud
797,802,"Security of Remote Computer

Since multi-user OS have several users accessing the system resources simultaneously, it's very important for the system administrators to implement security features within the system. 

These features could include account separation (by login through key pairs, a identity file), user groups, roles, and permissions.",secur remot comput sinc multius oper system sever ,"['secur', 'remot', 'comput', 'sinc', 'multius', 'oper', 'system', 'sever']",security of remote computer  since multi-user operating system have several users accessing the syst
798,803,"Git Bash Understanding

Git is a revision control software (RCS) (created by the inventor of linux) for tracking changes in any set of files. Means it is a distributed version control system (DVCS).

Git Bash is a command line Windows application to navigate through local or remote computers having windows or linux environment. Through git bash, we can also enter into python environment to write codes.  

Bash means Bourne Again Shell. 

Git Bash is a command line environment used for creating and interacting with Git repository",git bash understand git revis control softwar rcs ,"['git', 'bash', 'understand', 'git', 'revis', 'control', 'softwar', 'rcs']",git bash understanding  git is a revision control software (rcs) (created by the inventor of linux) 
799,804,"Introduction to Repository

Repository means the storage location or the folder where code to be stored.

GitHub is a Git repository hosting service. It is usually used for coordinating work among programmers collaboratively developing source code during software development.",introduct repositori repositori mean storag locat ,"['introduct', 'repositori', 'repositori', 'mean', 'storag', 'locat']",introduction to repository  repository means the storage location or the folder where code to be sto
800,805,"
Steps to connect to AWS EC2 instance with git bash

>  From AWS console, create an EC2 instance and let the machine run
> Create key pairs 
> Copy the key pair in our local machine
> Git bash entry into the directory of local machine where the key pair is saved
> Login through Git bash
ssh -i 'key_pair_name' user_name@public_IP_DNS

> ssh- means Secure Shell
> i- means identity",step connect aw ec2 instanc git bash aw consol cre,"['step', 'connect', 'aw', 'ec2', 'instanc', 'git', 'bash', 'aw', 'consol', 'cre']"," steps to connect to aws ec2 instance with git bash  >  from aws console, create an ec2 instance and"
801,806,"Understanding IP address

An IP address is an unique address that identifies a device on the internet or a local network. IP stands for ""Internet Protocol,"" 

EC2 instance public IP address changes when we start stop an instance. Elastic IP's if attached to the instance, it does not change. So, people using the IP address will not get error related to IP address. But there is a charge for elastic IP's when the instance is stopped.",understand ip address ip address uniqu address ide,"['understand', 'ip', 'address', 'ip', 'address', 'uniqu', 'address', 'ide']",understanding ip address  an ip address is an unique address that identifies a device on the interne
802,807,"Basic Bash or Linux commands

> Entering into the comp. memory

cd folder_name -to enter into a directory

cd / -to enter into the root directory (cd /f/ -to enter intro f drive)
> To view the list of folders in the directory

ls shows the list of files and folders name

ls -l  shows long details of directory

ls -lh shows long details in more human readable forms

ls -al shows all folders including hidden folders

mkdir folder_name  -to create directory
touch file_name -to create empty file
rm file_name -to remove file
rm -r folder_name (rmdir folder_name may be used for emty directory)
mv old_name new_name -for renaming file/directory
mv file_name /directory - moving a file/directory
ln  -symbolic link (shortcut to the file / folder)
> Few other commands
wget web_address -to download any file from website

top/htop -to see current process uses for all the users

clear- to clear all the commands in git bash

> Reading Files (.txt, .csv or .py)
cat file_name -to print the file contents on the screen

vi file_name -to open the file in vi editor (:q to exit)

nano file_name -to open the file in nano editor

head / tail file_name -to look at top / bottom lines of the file
> Searching Files
grep -to search text in files / search file in the folder structure
> Reading directory path
pwd -to print the path of working directory

> To Compress
To zip single file

gzip file_name 

To zip multiple files
tar -cvf new_file_name.tar file_name1 file_name2
> To reduce the size further 
tar -cvzf file_name.tar.gz
> To Decompress 
gzip -d file_name 

tar -xvf tar_file_name

> file file_name -gives details of file type

>  up and down arrow to go to the command history

> after typing few letters of file name press tab, git bash will automatically type the file name

> Copying files
cp file_name directory_name- to copy in that folder
ftp -to copy file using FTP

> Environment Variables
export -to define the new environment variable
env -to print all environment variables
> command --help to see required argument for that command

ctrl+c for canceling or aborting any command

git clone 'http_address' -to download the code from github repository  in the current directory

bash bash_file_name.sh -to execute the whole code in that file

man command_name -to see the manual for any command for learning purpose (press q to escape the manual)-This command can only be used after logging in into a unix machine

uname to display the operating system name

uname -r to display the os version

du -gives information about how much disk space (data usage) each file in the current directory uses",basic bash linux command enter comp memori cd fold,"['basic', 'bash', 'linux', 'command', 'enter', 'comp', 'memori', 'cd', 'fold']",basic bash or linux commands  > entering into the comp. memory  cd folder_name -to enter into a dire
803,808,"Few UNIX commands during project setup

Writing into a new file

cat > my_file.csv
Ram, 80
Sham,30
Jadu,40
Ctrl+d

By '>' we, can redirect the output of any command to a file

who > names.txt saves all the users details connected to the server

wc -l < name.txt counts the no. of lines in the file

Pipe character

(pipe command or pipeline command means doing multiple operation in a single command)

who | wc -l

we can use multiple pipes in a single command

To copy file or folder from local to local

cp or cp -r

To copy file from local to remote computer

scp -i 'key_file_name' /local_file_path username@public_IP_DNS:/remote_directory_path

To copy folder from local to remote computer

scp -i 'key_file_name' -r /local_file_path username@public_IP_DNS:/remote_directory_path

To see the python version in the server (for colab)

import sys

sys.version",unix command project setup write new file cat myfi,"['unix', 'command', 'project', 'setup', 'write', 'new', 'file', 'cat', 'myfi']","few unix commands during project setup  writing into a new file  cat > my_file.csv ram, 80 sham,30 j"
804,809,"File Permission Handling for security

> File permission are of three types read (r), write (w), execute (x) and it is divided among three groups
> Check left most side of the long list (ls -l)
> The first (1) character represents file or directory (_ for file, d for directory)
Group-1:  The first three (2-4) character represents the permission for the file owner (users,u)
Group-2: Second group of three character (5-7) represents permission for the group (g)
Group-3: The last group of three character (8-10) for everyone else (o)

Change file Permission
(change file mod bits)

chmod u+x o+wx file_name 
or 
chmod 102 file_name
-Three numbers represents three group permission

0- No permission
1- Execute permission
2- Write permission
3-Execute and write permissioin
4- read permission etc.
5-read and execute
6-read and write
7-read+write+execute


",file permiss handl secur file permiss three type r,"['file', 'permiss', 'handl', 'secur', 'file', 'permiss', 'three', 'type', 'r']","file permission handling for security  > file permission are of three types read (r), write (w), exe"
805,810,"UNIX command in colab notebook with ! Mark

Example: !man cd
> Google colab is a cloud computing coding software hosted on Google cloud server. It has no direct access to our local drives.
> However, Colab provides various options to connect to almost any cloud storage we can imagine. 
> We can either clone an entire GitHub repository to our Colab environment or access individual files from their link",unix command colab notebook mark exampl man cd goo,"['unix', 'command', 'colab', 'notebook', 'mark', 'exampl', 'man', 'cd', 'goo']",unix command in colab notebook with ! mark  example: !man cd > google colab is a cloud computing cod
806,811,"Creating a Simple Module

> We can save a single module_name.py file in the session and call the module/function inside the module with import statement

Suppose add() is defined in a module adder.py. 

To import and use the add() function

from adder import add
result = add(2, 3)

or
import adder
result = adder.add(2, 3)

** If we just create a blank __init__.py file within a folder and keep the module_name.py files in that folder, the folder becomes a python library (folder name=library name).

** The library has to be in the same directory (session) where it is called with import statement",creat simpl modul save singl modulenamepi file ses,"['creat', 'simpl', 'modul', 'save', 'singl', 'modulenamepi', 'file', 'ses']",creating a simple module  > we can save a single module_name.py file in the session and call the mod
807,812,"Basics of modular programming

Python Modules
A module allows us to logically organise our python code. For example, 

1. UI
2. Training
3. API

This is called modular programming.

> Module is a single python file with set of codes that we can import in python

Modular Programming Application Layout 

> It means packaging or foldering procedure
> Main folder or directory is called a python library
> Subdirectories are called python packages.
> All the .py files either inside the main directory or subdirectories are called python modules (module_name.py).

> Layouts are One-Off Script, Installable Package and App with Internal Packages",basic modular program python modul modul allow us ,"['basic', 'modular', 'program', 'python', 'modul', 'modul', 'allow', 'us']",basics of modular programming  python modules a module allows us to logically organise our python co
808,813,"One-Off Script Layout

> Here is only one directory with the application_name. This directory includes following files:

1. .gitignore file (available in github. This ensures that certain file types are never committed to the local Git repository)

2. application_name.py file (this script houses all the main codes)
3. LICENSE file (available in github) 
4. README.md file (a markdown document with purpose and usage of the application)

5. requirements.txt file (lists down outside python dependencies)
6. setup.py file (This script houses all the installation realted codes. This is the most crucial files in the folder directory.)
7. tests.py file (This script houses all the test related codes)",oneoff script layout one directori applicationnam ,"['oneoff', 'script', 'layout', 'one', 'directori', 'applicationnam']",one-off script layout  > here is only one directory with the application_name. this directory includ
809,814,"Installable Package Layout

The only difference here is that our application code is now all held in  application_name subdirectory. This subdirectory includes following files:
1. application_name.py file (this script houses all the main codes)
2. __init__.py file (It sets up how packages or functions will be imported into our other files)
3. helpers.py file

Tests are written in a separate subdirectory",instal packag layout differ applic code held appli,"['instal', 'packag', 'layout', 'differ', 'applic', 'code', 'held', 'appli']",installable package layout  the only difference here is that our application code is now all held in
810,815,"App with Internal Packages Layout

Here the subdirectories are:
1. bin (houses executable file if any.  If our package is a pure python one, there is nothing to put here)
2. docs (houses all the.md files)
3. application_name
4. data
5. tests (module_1_test.py, module_2_test.py etc.)

> Sub-packages may be formed if necessary

> src- This is the folder that contains our python package codes (module_1.py, module_2.py etc.).",app intern packag layout subdirectori 1 bin hous e,"['app', 'intern', 'packag', 'layout', 'subdirectori', '1', 'bin', 'hous', 'e']",app with internal packages layout  here the subdirectories are: 1. bin (houses executable file if an
811,816,"Data Science Project Layout

Data Science Project Layouts can vary depending on what all
components it contains. Some of the components are:

a. Data Engineering Pipelines (airflow / celery / API / SQL)
b. Model Training Pipeline (CLI)
c. Model Serving (API)
d. Model Validation (CLI)
e. Visualization
f. Report Generation",data scienc project layout data scienc project lay,"['data', 'scienc', 'project', 'layout', 'data', 'scienc', 'project', 'lay']",data science project layout  data science project layouts can vary depending on what all components 
812,817,"Using cookiecutter for packaging

We can either manually do the packaging or can do the same with few libraries like cookiecutter

pip install cookiecutter (can be used both for CLI and colab)

from cookiecutter.main import cookiecutter (for colab)

> Navigate to a directory where we want to create the new project, then run following in CLI

cookiecutter gh:claws/cookiecutter-python-project (we can run this command in colab with ! mark)

> Full form of CLI is command line interface",use cookiecutt packag either manual packag librari,"['use', 'cookiecutt', 'packag', 'either', 'manual', 'packag', 'librari']",using cookiecutter for packaging  we can either manually do the packaging or can do the same with fe
813,818,"stdin, stdout and stderr

All applications have three unique streams that connect them to the outside world. These are referred to as Standard Input, or stdin; Standard Output, or stdout; and Standard Error, or stderr.

1. Standard input is the default mechanism for getting input into an interactive program. This is typically a direct link to the keyboard when running directly in a terminal, and isn’t connected to anything otherwise.

2. Standard output is the default mechanism for writing output from a program. This is typically a link to the output terminal but is often buffered for performance reasons. This can be especially important when the program is running over a slow network link.

3. The standard error is an alternative mechanism for writing output from a program for errors. ",stdin stdout stderr applic three uniqu stream conn,"['stdin', 'stdout', 'stderr', 'applic', 'three', 'uniqu', 'stream', 'conn']","stdin, stdout and stderr  all applications have three unique streams that connect them to the outsid"
814,819,"Reading a static data file from inside a Python package

import pkgutil   # provides binary data
from io import StringIO # for binary to high level data conversion

bytes_data = pkgutil.get_data(__name__, ""my_data.csv"")

s=str(bytes_data,'utf-8')
data = StringIO(s) 
my_df_clean=pd.read_csv(data)",read static data file insid python packag import p,"['read', 'static', 'data', 'file', 'insid', 'python', 'packag', 'import', 'p']",reading a static data file from inside a python package  import pkgutil   # provides binary data fro
815,820,"Code for Creating python library 

After basic packaging (creating README.txt, LICENCE.txt, setup.py and __init__.py files), run the following code in powershell/anaconda prompt/gitbash in the project file where setup.py file is present, not in the package file where __init__.py file is present, for final packaging or creating distributable version (wheel file) of python model

 > Always mension the version of dependencies in the setup.py file install_requires

pip install wheel (required during first run)

python setup.py sdist bdist_wheel

This will create a dist folder with .whl file for uploading in pypi. Then upload as follows

 python -m pip install --upgrade twine (required during first run)

twine upload dist/* (for pypi.org)

twine upload --repository testpypi dist/* (shall be used for testing)

>  In case hanging problem, try any other terminal

> To include data files in the source distribution create MANIFEST.in

recursive-include package_name *.csv  (inside the file)",code creat python librari basic packag creat readm,"['code', 'creat', 'python', 'librari', 'basic', 'packag', 'creat', 'readm']","code for creating python library   after basic packaging (creating readme.txt, licence.txt, setup.py"
816,821,"Testing during packaging of our own python library

Test-1: Initial packaging without setup.py file
> create a folder 'package_name' in the session with init.py file
> upload data_file (.csv,.xlsx etc.) file in the session (if needed by the code)
> !pip install dependencies
> from my_package import my_function
> call the my_function

Test-2: Installation through setup.py file
> create a folder 'package_name' in the session with init.py file
> upload README, LICENSE and setup.py file in the session
> upload data_file (.csv,.xlsx etc.) file in the session (if needed by the code)
> !pip install my_package
> from my_package import my_function
> call the my_function

Test-3: Installation through wheel file
Must contain init.py file and data_file (.csv,.xlsx etc.)
> upload the wheel file in the session
> Checking files inside the source distribution file
import tarfile
tar = tarfile.open('my_package-0.0.1.tar.gz')
tar.getnames() 
> Checking files inside final distribution file or wheel file
import pprint
from zipfile import ZipFile

path = 'my_package-0.0.1-py3-none-any.whl'
names = ZipFile(path).namelist()
pprint.pprint(names)
> !pip install my_package-0.0.1-py3-none-any.whl
> from my_package import my_function
> call the my_function

Test-4: Installation through testpypi
> !pip install -i https://test.pypi.org/simple/ my_package
> from my_package import my_function
> call the my_function

Test-5: Installation through pypi (final testing)
> !pip install my_package
> from my_package import my_function
> call the my_function",test packag python librari test1 initi packag with,"['test', 'packag', 'python', 'librari', 'test1', 'initi', 'packag', 'with']",testing during packaging of our own python library  test-1: initial packaging without setup.py file 
817,822,"CVCS and DVCS

Version control allows we to keep track of our work and helps us to easily explore the changes we have made, be it data, coding scripts, notes, etc.

Manual version control with time stamps in the folder is prone to error. That is why DVCS and CVCS which keeps the track of file history.

> CVCS is old technology. Example: SVN (SubVersioN)

> DVCS is most popular because of empowering offline work by cloning the repository

> There aren’t really revision numbers in DVCS

> Every repo has its own revision numbers depending on the changes.

> We can tag releases with meaningful names.

Git bash+Github together make a DVCS.",central version control system distribut version c,"['central', 'version', 'control', 'system', 'distribut', 'version', 'c']",central version control system and distributed version control system  version control allows we to 
818,823,"Introduction to GitHub

Having a GitHub repo makes it easy for us to keep track of collaborative and personal projects - all files necessary for certain analysis can be held together and people can add in their code, graphs, etc. as the projects develop.

We can review other people’s code, add comments to certain lines or the overall document, and suggest changes. 

For collaborative projects, GitHub allows we to assign tasks to different users, making it clear who is responsible for which part of the analysis. ",introduct github github repo make easi us keep tra,"['introduct', 'github', 'github', 'repo', 'make', 'easi', 'us', 'keep', 'tra']",introduction to github  having a github repo makes it easy for us to keep track of collaborative and
819,824,"GitHub workflow

The GitHub workflow can be summarised by the “commit-pull-push” mantra.

1. Commit-
Once we’ve saved our files, we need to commit them - this means the changes we have made to files in our repo will be saved as a version of the repo
2. Pull (git pull)-
Now, before we send our changes to Github, we need to pull, i.e. make sure we are completely up to date with the latest version
3. Push (git push)-
Once we are up to date, we can push our changes ",github workflow github workflow summaris “commitpu,"['github', 'workflow', 'github', 'workflow', 'summaris', '“commitpu']",github workflow  the github workflow can be summarised by the “commit-pull-push” mantra.  1. commit-
820,825,"Steps of creating local git repository

1. mkdir my_project
2. cd my_project
3. git init (initializes a hidden git repository which tracks all changes)
4. Copy all the project files here
5. git add files_or_folder_name
6. git status
7. git config --global user.name 'user name'
8. git config --global user.email 'emailid'
9. git commit file_name -m 'my_message'
10. git log -to see the log of the history (git log -n to show limited number of commits)

Git repository never forgets its history. So, we can restore any deleted file or older version file.

git reset commitID
",step creat local git repositori 1 mkdir myproject ,"['step', 'creat', 'local', 'git', 'repositori', '1', 'mkdir', 'myproject']",steps of creating local git repository  1. mkdir my_project 2. cd my_project 3. git init (initialize
821,826,"Concept of Branch

Master branch (for releasing)- ready to deploy codes

Branch (developer/feature branch)- experimentals codes

> Git commands for creating, activating and entering into the branch

git branch branch_name

git checkout branch_name

git branch",concept branch master branch releas readi deploy c,"['concept', 'branch', 'master', 'branch', 'releas', 'readi', 'deploy', 'c']",concept of branch  master branch (for releasing)- ready to deploy codes  branch (developer/feature b
822,827,"Synchronising local git repository with Github (remote)

git remote add origin github_repository_address

git push --set-upstream origin master",synchronis local git repositori github remot git r,"['synchronis', 'local', 'git', 'repositori', 'github', 'remot', 'git', 'r']",synchronising local git repository with github (remote)  git remote add origin github_repository_add
823,828,"Merge Conflict

When multiple people want to commit same line of code with different changes in the same workflow, then merge conflict occurs.",merg conflict multipl peopl want commit line code ,"['merg', 'conflict', 'multipl', 'peopl', 'want', 'commit', 'line', 'code']",merge conflict  when multiple people want to commit same line of code with different changes in the 
824,829,"Continuous Integration/ Continuous Deployment or Delivery (CI/CD)

Through github Actions we can build, test and deploy our codes continuously or regularly.

> Concept of master branch and branch introduced for this purpose",continu integr continu deploy deliveri cicd github,"['continu', 'integr', 'continu', 'deploy', 'deliveri', 'cicd', 'github']",continuous integration/ continuous deployment or delivery (ci/cd)  through github actions we can bui
825,830,"Understanding API

> An API (Application Programming Interface) is a set of functions that allows applications to access data
and interact with external software components, operating systems, or microservices.

> Application means the production deployment of software development. There are two types of Apps: Mobile Apps and Web Apps

> API delivers a user request to a system and sends the system’s response back to a user. Thus, API makes a web page dynamic (web app)

Popular Examples

• YouTube API
• Twitter API
• Facebook API
• Microsoft 365 Graph API",understand applic program interfac applic program ,"['understand', 'applic', 'program', 'interfac', 'applic', 'program']",understanding application programming interface  > an application programming interface (application
826,831,"Types of web pages

1. Static webpage (can be created using only html or html+css). It is also called website
2. Dynamic webpage (can be created using html+ flask/JavaScript). This is also called web app
> A website provides visual and text content that users can view and read.  Web application is designed for interaction with end users.

HTML-Definitions of tags that are added to web documents to control their appearance
HTTP- The rules governing the conversion between a web client and a web server

> Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.",type web page 1 static webpag creat use html htmlc,"['type', 'web', 'page', '1', 'static', 'webpag', 'creat', 'use', 'html', 'htmlc']",types of web pages  1. static webpage (can be created using only html or html+css). it is also calle
827,832,"Machine Learning Services

All Machine Learning Services are available using API Interface:

○ Azure Cognitive Services

○ Google Cloud AI and Machine 
Learning Services

○ AWS AI Services",machin learn servic machin learn servic avartifici,"['machin', 'learn', 'servic', 'machin', 'learn', 'servic', 'avartifici']",machine learning services  all machine learning services are avartificial intelligencelable using ap
828,833,"Types of API (Package for creating Apps)

SOAP API -Simple Object Access Protocol, XML based protocol, heavy weight

REST API- Representational State Transfer, HTTP based protocol, light weight",type applic program interfac packag creat app soap,"['type', 'applic', 'program', 'interfac', 'packag', 'creat', 'app', 'soap']",types of application programming interface (package for creating apps)  soap application programming
829,834,"Basics of Flask

Flask is a python based framework (microframework) to build web application. 

> It is a very light weight, REST API
> It is based on Werkzeug and Jinja2
> Flask supports database powered application (RDBMS)
Django is a similar web api python package like flask. But Flask is easy to learn.
",basic flask flask python base framework microframe,"['basic', 'flask', 'flask', 'python', 'base', 'framework', 'microframe']",basics of flask  flask is a python based framework (microframework) to build web application.   > it
830,835,"Benefits of using the Flask framework

1. It supports Unicode.
2. It has an inbuilt development server.
3. It has a tiny API and can be quickly learned by a web developer.
4. It has vast third-party extensions",benefit use flask framework 1 support unicod 2 inb,"['benefit', 'use', 'flask', 'framework', '1', 'support', 'unicod', '2', 'inb']",benefits of using the flask framework  1. it supports unicode. 2. it has an inbuilt development serv
831,836,"Popular HTTP Requests

1. GET – Gathers information (Pulling all Coupon Codes)

2. PUT –  Updates pieces of data (Updating Product pricing)

3. POST – Creates (Creating a new Product Category)

4. DELETE – (Deleting a blog post)

We can send api request through postman like app or directly through python interpreter with following command:

import requests

response=requests.get('https://api.github.com')

response.text

-Requests library  allows us to send HTTP/1.1 requests using Python
",popular http request 1 get – gather inform pull co,"['popular', 'http', 'request', '1', 'get', '–', 'gather', 'inform', 'pull', 'co']",popular http requests  1. get – gathers information (pulling all coupon codes)  2. put –  updates pi
832,837,"Use of Virtual Environment

A virtual environment is a tool that helps to keep dependencies required by different projects separate by creating isolated python virtual environments for them. 

As a best practice, create a virtual environment for any new project and install packages (dependencies) as required through pip install command

",use virtual environ virtual environ tool help keep,"['use', 'virtual', 'environ', 'virtual', 'environ', 'tool', 'help', 'keep']",use of virtual environment  a virtual environment is a tool that helps to keep dependencies required
833,838," Creating a virtual environment

Creating a virtual environment in local machine (through git bash)

> To create a virtual environment, python must be installed in that computer.
> make a project directory and enter into that directory

python -m venv env

> Activate the environment
source env/Scripts/activate

Creating a virtual environment in EC2 instance (through git bash)
> where python3 is installed, 
> make a project directory and enter into that directory

python3 -m venv env

> Then activate the environment by sourcing the activate file in the  directory.
source env/bin/activate",creat virtual environ creat virtual environ local ,"['creat', 'virtual', 'environ', 'creat', 'virtual', 'environ', 'local']", creating a virtual environment  creating a virtual environment in local machine (through git bash) 
834,840,"Understanding pip install

pip install package_name

pip stands for ""preferred installer program"".

This is the Python Package Installer (also used for uninstalling)

Usually, pip is automatically installed if we are working in a virtual environment.

pip show package_name (to see the version of package installed)

> colab notebook is a python virtual environment, so we can use pip without creating a virtual environment

If the project directory has setup.py file, that means we can install the package.

1. For installing a static package, as a tester,

pip install

2. For installing an editable package, as a developer (keep the  setup.py and README.txt, LICENCE.txt files in the session),
pip install -e .
> This will install the dependencies
or 
Direct installation from wheel file

pip install package_name-0.0.1-py3-none-any.whl",understand pip instal pip instal packagenam pip st,"['understand', 'pip', 'instal', 'pip', 'instal', 'packagenam', 'pip', 'st']","understanding pip install  pip install package_name  pip stands for ""preferred installer program"".  "
835,841,"Creating a Web App in Flask

> after activating virtual environment, enter into the project directory

!pip install flask
> write the code for the Web App in app.py file and save in the project directory, 

from flask import Flask, redirect, url_for, render_template, request
app=Flask(__name__)
@app.route('/')
def home_page():
    return 'prediction=200'
app.run() # gives the link for the webapp
> We can integrate html pages in the above coding. Then we need to return render_template('my_file.html')

> url_for -used for dynamic url generation

if total_score>=50:
res='success'
else:
res='fail'
return redirect(url_for(res,score=total_score))

> Creating a dynamic button

@app.route('/submit', method= ['POST','GET'])

Adding mailing feature in the Flask Application

pip install Flask-Mail

> Flask default port is 5000

> Flask default host is a localhost (127.0.0.1)

> FastAPI default port is 8000",creat web app flask activ virtual environ enter pr,"['creat', 'web', 'app', 'flask', 'activ', 'virtual', 'environ', 'enter', 'pr']","creating a web app in flask  > after activating virtual environment, enter into the project director"
836,842,"Jinja techniques

> Jinja tag is written as 
{% ... %} any statements
{{  }} expressions to print output
{#...#} for internal comments

> Jinja techniques are used to integrate dynamic coding within html",jinja techniqu jinja tag written statement express,"['jinja', 'techniqu', 'jinja', 'tag', 'written', 'statement', 'express']",jinja techniques  > jinja tag is written as  {% ... %} any statements {{  }} expressions to print ou
837,843,"Ways to a edit script code in Colab notebook (alternative to vs code)

> Make a copy the script file in google drive

> Mount drive with colab

> !cat 'file_path'

> copy the output code and paste in a new colab file and start editing

> Then, download the .py file

> Colab notebook is a development server, we should not use it in making app.

VS Code desktop app is useful for offline editing of python files",way edit script code colab notebook altern vs code,"['way', 'edit', 'script', 'code', 'colab', 'notebook', 'altern', 'vs', 'code']",ways to a edit script code in colab notebook (alternative to vs code)  > make a copy the script file
838,844,"Common file formats used in Data Science:

.csv
.txt
.json
.xlsx
.sas
.sql
.pickle
.dta
.h5
.xml
.html
.zip
.pdf
.docx
.jpeg, .png, .gif etc.",common file format use data scienc csv txt json xl,"['common', 'file', 'format', 'use', 'data', 'scienc', 'csv', 'txt', 'json', 'xl']",common file formats used in data science:  .csv .txt .json .xlsx .sas .sql .pickle .dta .h5 .xml .ht
839,845,"Flaskr as a basic blog application

Flaskr is a basic blog application inside Flask package. Here, users are be able to register, log in, create posts, and edit or delete their own posts (basic five actions).

Flaskr directory has following files and sub-directories:
1. flask app object defined in __init__.py

2. table intilization in db.py

3. table structure is defined in schema.sql

4. user authentication is defined in auth.py

5. creating, editing or deleting a post is defined in blog.py

6. template sub-directory contains all the .html template files

7. static directory contains all the static .css files",flaskr basic blog applic flaskr basic blog applic ,"['flaskr', 'basic', 'blog', 'applic', 'flaskr', 'basic', 'blog', 'applic']","flaskr as a basic blog application  flaskr is a basic blog application inside flask package. here, u"
840,846,"Hyper Text Markup Language

The Hyper Text Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by  Cascading Style Sheets (CSS) and JavaScript.
We can create static html page as follows:
> we can simply use notepad (or vs code) and write the code and save the file with file_name.html or we can copy the html source code and paste it in notepad/ vs code and start editing

Template inheritance allows us to build a base “skeleton” template that contains all the common elements of our site and defines blocks that child templates can override.",hyper text markup languag hyper text markup langua,"['hyper', 'text', 'markup', 'languag', 'hyper', 'text', 'markup', 'langua']","hyper text markup language  the hyper text markup language, or html is the standard markup language "
841,847,"Understanding FastAPI

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.

FastAPI is similar to Flask but FastAPI is really fast and very usefull for ml engineers to present their work in a working way through ml app.

> after activating virtual environment, enter into the project directory

pip install fastapi[all]
pip install uvicorn[standard]
> write the code for the Web App in app.py file and save in the project directory, 
example python code:

from fastapi import FastAPI
app=FastAPI()
@app.get('/')
async def root():
    return 'prediction=200'
uvicorn app(is the file name):app(is the object name) --reload

> we can deploy our application on Heroku/Amazon Web Services/Microsoft Azure",understand fastapi fastapi modern fast highperform,"['understand', 'fastapi', 'fastapi', 'modern', 'fast', 'highperform']","understanding fastapi  fastapi is a modern, fast (high-performance), web framework for building apis"
842,848,"WSGI vs ASGI

WSGI stands for Web Server Gateway Interface. It is synchronous. Fask, Django uses WSGI

ASGI stands for Asynchronous Server Gateway Interface. It is asynchronous. FastAPI use ASGI




",web server gateway interfac vs asynchron server ga,"['web', 'server', 'gateway', 'interfac', 'vs', 'asynchron', 'server', 'ga']",web server gateway interface vs asynchronous server gateway interface  web server gateway interface 
843,849,"Features of FastAPI

● Interactive API Docs (Swagger UI)

url/docs

● Data Type Validation

> url stands for Uniform Resource Locator",featur fastappl program interfac ● interact applic,"['featur', 'fastappl', 'program', 'interfac', '●', 'interact', 'applic']",features of fastapplication programming interface  ● interactive application programming interface d
844,850,"Pydantic Data Model

from pydantic import BaseModel

class Item(BaseModel)

name:str
description: Optional[str] = None

price: float

tax: Optional[float] = None

> pydantic guarantees the types and constraints of the output model, not the input data",pydant data model pydant import basemodel class it,"['pydant', 'data', 'model', 'pydant', 'import', 'basemodel', 'class', 'it']",pydantic data model  from pydantic import basemodel  class item(basemodel)  name:str description: op
845,851,"FastAPI Working with SQL (RDBMS)

> FastAPI works with any database and any style of library to talk to the database.
> A common pattern is to use an ""ORM"": an ""object-relational
mapping"" library. E.g. SQLAlchemy",fastapi work structur queri languag rdbms fastapi ,"['fastapi', 'work', 'structur', 'queri', 'languag', 'rdbms', 'fastapi']",fastapi working with structured query language (rdbms)  > fastapi works with any database and any st
846,852,"VS Code for python coding

> Open VS Code and then open a folder (newly created)  in any location of the computer
> Then inside that workspace (folder), create a file with .py extension
> Write or copy paste the code
> Create and activate a virtual environment in that folder and pip install all the required packages through gitbash
> from command palette> select python interpreter> select recommended virtual env
> right click on the code and click Run python file in terminal
>> From view tab> terminal (Ctrl+`), we can open the CLI, here we can write all the git/terminal commands

> Different extensions are used in vs code to give us multiple extra features. We can also build vs code extension and contribute",vs code python code open vs code open folder newli,"['vs', 'code', 'python', 'code', 'open', 'vs', 'code', 'open', 'folder', 'newli']",vs code for python coding  > open vs code and then open a folder (newly created)  in any location of
847,853,"Ways to use python in local machine through git bash (when anaconda installed)

conda init bash 
(in anaconda prompt)

python --version
(checking the version of python)

ipython

exit()

",way use python local machin git bash anaconda inst,"['way', 'use', 'python', 'local', 'machin', 'git', 'bash', 'anaconda', 'inst']",ways to use python in local machine through git bash (when anaconda installed)  conda init bash  (in
848,854,"Docker Basics

> Docker file helps in packaging our project as a container (packages our project like setup.py)

> This is standardized packaging for software and dependencies

> Docker is the company which is pioneer in container technology. So container and docker is used interchangbly.

> This is very light weight

> This is the most popular packaging technique

> Solomon Hykes developed Docker in 2013",docker basic docker file help packag project conta,"['docker', 'basic', 'docker', 'file', 'help', 'packag', 'project', 'conta']",docker basics  > docker file helps in packaging our project as a container (packages our project lik
849,855,"Image or Docker Image

It is a read only binary file which defines all the step for running the app including instruction for installling  packages and libraries as a dependency of the application. 

> Image has the task to create a container.

> We can create multiple containers from the same image

> Docker file is a text document containing few lines of codes (command line) for creating a docker image
Docker file-> Docker image-> Docker Container-> Installation of Packages and libraries
",imag docker imag read binari file defin step run a,"['imag', 'docker', 'imag', 'read', 'binari', 'file', 'defin', 'step', 'run', 'a']",image or docker image  it is a read only binary file which defines all the step for running the app 
850,856,"Container or Docker Container

It is the running instance of the docker image

Container is similar as virtual machine (instance) but very different from VM.

Instance is heavy weight but docker is very light weight.

In one heavy instance with high RAM, high processor or core, high storage and docker installed, we can run multiple applications in multiple dockers.

All the dockers share same resources (i.e computational resources like OS kernel, RAM, CPU and Hard disk)

> Thus docker creates boundaries in the instance for separating multiple applications",contain docker contain run instanc docker imag con,"['contain', 'docker', 'contain', 'run', 'instanc', 'docker', 'imag', 'con']",container or docker container  it is the running instance of the docker image  container is similar 
851,857,"Introduction to dockerhub

Just like github there is dockerhub for shipping docker image for the application

The easiest way to make our images available for use by others inside or outside our organization is to use a Docker registry, such as Docker Hub, or by running our own private registry. 

> Docker registry uses port 5000

> Docker Hub allows only one private repository

> Docker engine is the docker software (an open source containerisation technology)",introduct dockerhub like github dockerhub ship doc,"['introduct', 'dockerhub', 'like', 'github', 'dockerhub', 'ship', 'doc']",introduction to dockerhub  just like github there is dockerhub for shipping docker image for the app
852,858,"Ways to install docker in EC2 instance

docker --version (before installing, check if the docker is already installed)



",way instal docker ec2 instanc docker version insta,"['way', 'instal', 'docker', 'ec2', 'instanc', 'docker', 'version', 'insta']","ways to install docker in ec2 instance  docker --version (before installing, check if the docker is "
853,859,"Basic Docker Commands

$ docker pull python:3.6
$ docker images
$ docker run –d –p 5000:5000 –name node node:latest
$ sudo docker run -i -t alpine /bin/bash (for running images as container)
$ docker ps -a
$ docker stop container_id (or docker kill)
$ docker rm container_ id (for removing the containers)
$ docker rmi image_id (for removing the images)
$ docker build –t node:2.0 .
$ docker push node:2.0
$ docker ps (to see all running container in Docker)
$ docker --help

> A node is a device or data point in a larger network.

Multiple nodes means that there are multiple virtual machines.",basic docker command docker pull python36 docker i,"['basic', 'docker', 'command', 'docker', 'pull', 'python36', 'docker', 'i']",basic docker commands  $ docker pull python:3.6 $ docker images $ docker run –d –p 5000:5000 –name n
854,860,"Docker or Container volume

Volumes mount a directory on the host into the container at a specific location

> Mount local source code into a running container",docker contain volum volum mount directori host co,"['docker', 'contain', 'volum', 'volum', 'mount', 'directori', 'host', 'co']",docker or container volume  volumes mount a directory on the host into the container at a specific l
855,861,"Docker Bridge Networking and Port Mapping

 docker container run -p 8080 (host_port):80(container_port)",docker bridg network port map docker contain run p,"['docker', 'bridg', 'network', 'port', 'map', 'docker', 'contain', 'run', 'p']",docker bridge networking and port mapping   docker container run -p 8080 (host_port):80(container_po
856,862,"Docker Compose

It is a Multi Container Applications",docker compos multi contain applic,"['docker', 'compos', 'multi', 'contain', 'applic']",docker compose  it is a multi container applications
857,863,"Docker swarm

Docker swarm allows the user to manage multiple containers deployed across multiple host machines (native clustering). ",docker swarm docker swarm allow user manag multipl,"['docker', 'swarm', 'docker', 'swarm', 'allow', 'user', 'manag', 'multipl']",docker swarm  docker swarm allows the user to manage multiple containers deployed across multiple ho
858,864,"Understanding Microservices

Microservices are used when the application is huge (generally for e-commerce application)

Earlier there were monolithic applications implemented using a single development stack

Microservices is the architecture of an entire software project. ML application is only one part of that main application (main service). 

> The knowledge of microservices is important for ML engineer, not for data scientist

In microservices architecture, all the part applications are loosely coupled, so their deployment schedule can be independent.

Loosely coupled means, all the part applications are individually complete application and they interact with each other through api service

All the micro services don't need to share the same technology stack, libraries, or frameworks",understand microservic microservic use applic huge,"['understand', 'microservic', 'microservic', 'use', 'applic', 'huge']",understanding microservices  microservices are used when the application is huge (generally for e-co
859,865,"API Gateway

All the part applications do not directly interact with each other, they interact with each other through a coordinator, called API Gateway",applic program interfac gateway part applic direct,"['applic', 'program', 'interfac', 'gateway', 'part', 'applic', 'direct']",application programming interface gateway  all the part applications do not directly interact with e
860,866,"Development of Microservices

> Use domain analysis to define
our microservice boundaries. 

In this stage we list down the requirement of api's for every part application

Domain means the part applications or microservices

> Design service includes Scheduler service",develop microservic use domain analysi defin micro,"['develop', 'microservic', 'use', 'domain', 'analysi', 'defin', 'micro']",development of microservices  > use domain analysis to define our microservice boundaries.   in this
861,867,"Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It uses Docker

It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.",kubernet also known k8s opensourc system autom dep,"['kubernet', 'also', 'known', 'k8s', 'opensourc', 'system', 'autom', 'dep']","kubernetes, also known as k8s, is an open-source system for automating deployment, scaling, and mana"
862,868,"Creating a Streamlit file

Streamlit is a python library

The fastest way to build Web App

python commands
> We must create an account in ngrok
 
!pip install streamlit
!pip install pyngrok

from pyngrok import ngrok

!streamlit run file_name.py

pub_url=ngrok.connect(port='8501')

pub_url # will provide an url for the demo app

> ngrok is a cross-platform application that enables developers to expose a local development server to the Internet for end user with minimal effort.

streamlit is for demo development not for api development.

Not only for demo but streamlit is also very useful for our ml model validation

Using streamlit we can deploy any machine learning model and any python project with ease and without worrying about the frontend
",creat streamachin learningit file streamachin lear,"['creat', 'streamachin', 'learningit', 'file', 'streamachin', 'lear']",creating a streamachine learningit file  streamachine learningit is a python library  the fastest wa
863,869,"Creating a file for streamlit inside the session

%%writefile file_name.py

import streamlit as st

st.title('The Taste of My Mango')

def mango(name):
  result=name+' is awesome'
  return result",creat file streamlit insid session writefil filena,"['creat', 'file', 'streamlit', 'insid', 'session', 'writefil', 'filena']",creating a file for streamlit inside the session  %%writefile file_name.py  import streamlit as st  
864,870,"Computer Networking

In computer networking, a port is a communication endpoint. At the software level, within an operating system, a port is a logical construct that identifies a specific process or a type of network service. A port is identified for each transport protocol and address combination by a 16-bit unsigned number, known as the port number. 

> A port number is always associated with an IP address of a host and the type of transport protocol used for communication

192.168.0.1:80 (Socket address)
192.168.0.1 (IP address)
80 (port number)",comput network comput network port communic endpoi,"['comput', 'network', 'comput', 'network', 'port', 'communic', 'endpoi']","computer networking  in computer networking, a port is a communication endpoint. at the software lev"
865,871,"4 Phases to ML Lifecycle

Phase 1 is Project Planning and Project Setup

Phase 2 is Data Collection and Data Labeling

Phase 3 is Model Training, Testing and Model Debugging (model explainability check)

Phase 4 is Model Deployment (API>Docker>Cloud) and Model Real Time Testing",4 phase machin learn lifecycl phase 1 project plan,"['4', 'phase', 'machin', 'learn', 'lifecycl', 'phase', '1', 'project', 'plan']",4 phases to machine learning lifecycle  phase 1 is project planning and project setup  phase 2 is da
866,872,"Challenges with ML during development

> Development, training and deployment environment can be different
> Tools, libraries and dependencies can complicate deployment
> Tracking and analyzing experiment can become tedious to handle
> Difficult to reproduce experiment as input data changes
> ML Code end up in a spaghetti jungle",challeng machin learn develop develop train deploy,"['challeng', 'machin', 'learn', 'develop', 'develop', 'train', 'deploy']","challenges with machine learning during development  > development, training and deployment environm"
867,873,"Challenges with ML in production

> Live data is not equal to training data
> Feature engineering pipeline must match between training and serving infrastructure
> Seamlessly scale up and scale down deployed model
> Continuous training and champion challenger model deployment
> Different technology landscape between development and deployment (check versions of the libraries)",challeng machin learn product live data equal trai,"['challeng', 'machin', 'learn', 'product', 'live', 'data', 'equal', 'trai']",challenges with machine learning in production  > live data is not equal to training data > feature 
868,874,"Data drift and Model drift

In model deployment, there can be data drift or model drift issues. 

Data drift is the change in model input data that leads to model performance degradation

Model drift refers to the degradation of model performance due to changes in data and relationships between input and output variables.

So, drift analysis must be done.

",data drift model drift model deploy data drift mod,"['data', 'drift', 'model', 'drift', 'model', 'deploy', 'data', 'drift', 'mod']","data drift and model drift  in model deployment, there can be data drift or model drift issues.   da"
869,875,"Machine learning engineering (MLE) is the process of using software engineering principles, and analytical and data science knowledge, and combining both of those in order to take an ML model that’s created and making it available for use by the product or the consumers.
",machin learn engin machin learning process use sof,"['machin', 'learn', 'engin', 'machin', 'learning', 'process', 'use', 'sof']",machine learning engineering (machine learninge) is the process of using software engineering princi
870,876,"Cloud computing advantages

Azure fundamental is basically a cloud computing fundamental

● Reliability: Depending on the service-level agreement that we choose, our cloud-based applications can provide a continuous user experience with no apparent downtime even when things go wrong.
● Scalability: Applications in the cloud can be scaled in two ways, while taking advantage of autoscaling:
>> Vertically: Computing capacity can be increased by adding RAM or CPUs to a virtual machine.
>> Horizontally: Computing capacity can be increased by adding instances of a resource, such as adding more virtual machines.

● Elasticity: Cloud-based applications can be configured to always have the resources they need.
● Agility: Cloud-based resources can be deployed and configured quickly as our application requirements change.
● Geo-distribution: Applications and data can be deployed to regional datacenters around the globe, so our customers always have the best performance in their region.

● Disaster recovery: By taking advantage of cloud-based backup services, data replication, and geo-distribution, we can deploy our applications with the confidence that comes from knowing that our data is safe in the event that disaster should occur",cloud comput advantag azur fundament basic cloud c,"['cloud', 'comput', 'advantag', 'azur', 'fundament', 'basic', 'cloud', 'c']",cloud computing advantages  azure fundamental is basically a cloud computing fundamental  ● reliabil
871,877,"Cloud service models

IaaS-Infrastructure as a Service (Provides storage, computational power and networking (like DNS, Azure VMs and storage services)

PaaS-Platform as a Service (along with IaaS services, it provides OS and runtime)-This is the most popular one

SaaS-Software as a Service. (along with PaaS services, it provides applications as needed. Example is Office 365)",cloud servic model iaasinfrastructur servic provid,"['cloud', 'servic', 'model', 'iaasinfrastructur', 'servic', 'provid']","cloud service models  iaas-infrastructure as a service (provides storage, computational power and ne"
872,878,"A runtime system refers to the collection of software and hardware resources that enable a software program to be executed on a computer system. 

Ex. Docker is a runtime system",runtim system refer collect softwar hardwar resour,"['runtim', 'system', 'refer', 'collect', 'softwar', 'hardwar', 'resour']",a runtime system refers to the collection of software and hardware resources that enable a software 
873,879,"CPU vs GPU

The main difference between CPU and GPU architecture is that a CPU is designed to handle a wide-range of tasks quickly, a GPU (Graphics processing unit) is designed to quickly render high-resolution images and video concurrently.

CPU reads Cache Memory data before reading RAM.

RAM is a hardware element where the data being currently used is stored. It is a volatile memory.",central process unit vs graphic process unit main ,"['central', 'process', 'unit', 'vs', 'graphic', 'process', 'unit', 'main']",central processing unit vs graphics processing unit  the main difference between central processing 
874,880,"Azure machine learning platform 

(end to end service means it provides all services staring from development to deployment)

-It is getting very popular day by day because it helps us to build, train and deploy our
ML models faster and foster team collaboration.",azur machin learn platform end end servic mean pro,"['azur', 'machin', 'learn', 'platform', 'end', 'end', 'servic', 'mean', 'pro']",azure machine learning platform   (end to end service means it provides all services staring from de
875,881,"Azure ML Workspace and Azure ML piplines

> In azureml every run is called an Experiment

> Environment file includes the list of all the dependent libraries and packages for the deployment of our application
> from the environment file azure creates the docker image

> Azure Resource Manager and Azure Portal are used to manage resources.",azur machin learn workspac azur machin learn pipli,"['azur', 'machin', 'learn', 'workspac', 'azur', 'machin', 'learn', 'pipli']",azure machine learning workspace and azure machine learning piplines  > in azuremachine learning eve
876,882,"Connecting to azure ml

We can access azureml through web interface or through python coding by using azureml web api

> !pip install azureml-sdk (enables us to connect to azureml from python)
> import azureml.core

> from azureml.core import Environment, Workspace, Experiment, ScriptRunConfig

> SDK: Software Development Kit

experiment_name = 'my_experiment'

ws = Workspace.from_config()

experiment = Experiment(workspace=ws, name=experiment_name)

myenv = Environment.get(workspace=ws, name=experiment_name)

src = ScriptRunConfig(source_directory=project_folder, script='train.py',
compute_target=my_compute_target, environment=myenv)",connect azur machin learn access azuremachin learn,"['connect', 'azur', 'machin', 'learn', 'access', 'azuremachin', 'learn']",connecting to azure machine learning  we can access azuremachine learning through web interface or t
877,883,Blob storage-Blob stands for Binary Large Object,blob storageblob stand binari larg object,"['blob', 'storageblob', 'stand', 'binari', 'larg', 'object']",blob storage-blob stands for binary large object
878,884,"import os

The OS module in Python provides functions for interacting with the operating system. OS comes under Python's standard utility modules.",import oper system oper system modul python provid,"['import', 'oper', 'system', 'oper', 'system', 'modul', 'python', 'provid']",import operating system  the operating system module in python provides functions for interacting wi
879,885,"Different cloud deployment models

1. Private cloud: Privately owned and manged with restricted access

2. Public Cloud: Service provider owned and managed.

3. Hybrid cloud: combination of private and public cloud",differ cloud deploy model 1 privat cloud privat ow,"['differ', 'cloud', 'deploy', 'model', '1', 'privat', 'cloud', 'privat', 'ow']",different cloud deployment models  1. private cloud: privately owned and manged with restricted acce
880,886,"ML Lifecycle management

MLOps is one of the field in ML lifecycle, mainly a part of ML engineering.

MLOps in simple term is DevOps for Machine Learning

MLOps reduces technical debt across machine learning models

ML Lifecycle (MLOps) management tools  are MLFlow, Azure ML, AWS Sagemaker and Google Cloud ML

> Technical debt (or design debt or code debt) is reflects the implied cost of additional rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer.",machin learn lifecycl manag machin learningop one ,"['machin', 'learn', 'lifecycl', 'manag', 'machin', 'learningop', 'one']",machine learning lifecycle management  machine learningops is one of the field in machine learning l
881,887,"MLOps and DevOps

When we only use github we can not compare metrices between different versions of our model. But when we use github along with ML lifecycle management tools, we can compare metrices

DevOps meaning Software development and IT operation

Software development process includes Plan, Code, Build (an executable program that can be used to carry out proper machine level outputs. Building is also called programming) and Test

IT operation process includes Release, Deploy, Operate and Monitor

DevOps breaks the wall of confusion between the development team and operation team by automating the process of delivery or deployment through CI/CD",machin learn lifecycl devop use github compar metr,"['machin', 'learn', 'lifecycl', 'devop', 'use', 'github', 'compar', 'metr']",machine learning lifecycle and devops  when we only use github we can not compare metrices between d
882,888,"Agile software development refers to a group of software development methodologies based on iterative development, where requirements and solutions evolve through collaboration between self-organizing cross-functional teams.

MLOps enables the application of agile principles to machine learning projects.

> Agile is a process of colaboration and DevOps is an automation of colaboration",agil softwar develop refer group softwar develop m,"['agil', 'softwar', 'develop', 'refer', 'group', 'softwar', 'develop', 'm']",agile software development refers to a group of software development methodologies based on iterativ
883,889,"Key components of MLOps

1. Machine learning

2.  DevOps (IT)

3. Data Engineering",key compon ml lifecycl 1 machin learn 2 devop 3 da,"['key', 'compon', 'ml', 'lifecycl', '1', 'machin', 'learn', '2', 'devop', '3', 'da']",key components of ml lifecycle  1. machine learning  2.  devops (it)  3. data engineering
884,890,"Advantages of MLOps

MLOps is more vibrant than DevOps (ML + DevOps = MLOps)

1. Data/Schema versioning apart from code versioning
2. Experiment tracking (Model hyperparameters, Data
Distribution, Model performance, feature importance etc)
3. Model artifacts versioning
4. Monitor continuously for data and model drift
5. Continuous re-training of model
6. Capture sensitivity of key features to target 

> MLOps is simpler than DevOps",advantag ml lifecycl ml lifecycl vibrant devop ml ,"['advantag', 'ml', 'lifecycl', 'ml', 'lifecycl', 'vibrant', 'devop', 'ml']",advantages of ml lifecycle  ml lifecycle is more vibrant than devops (ml + devops = ml lifecycle)  1
885,891,"Key outcomes of MLOps

1. Model Packaging and Validation

2. Model Reproducibility

3. Model deployment 

4. Model Explainability,
Monitoring and Re-training",key outcom ml lifecycl 1 model packag valid 2 mode,"['key', 'outcom', 'ml', 'lifecycl', '1', 'model', 'packag', 'valid', '2', 'mode']",key outcomes of ml lifecycle  1. model packaging and validation  2. model reproducibility  3. model 
886,892,"Model registry and Metadata 

Model registry is for model versioning

(There are three kinds of versioning: Data versioning, model versioning and source code versioning)

ML metadata store is for tracking experiments

> Metadata is a set of data that describes and gives information about other data.",model registri metadata model registri model versi,"['model', 'registri', 'metadata', 'model', 'registri', 'model', 'versi']",model registry and metadata   model registry is for model versioning  (there are three kinds of vers
887,893,"MLFLow Basics

MLFLow starts from modeling to deployment.

>> It is an open source platform for the machine learning lifecycle. 

>> It has four components Tracking (experiment, code, data, config, results) Projects (packaging), Models (deployment) and Model registry

>> We can log different metrices using mlflow library and each run of our ml project is saved as individual version. So, we can compare the metrices of different version.

!pip install mlflow

import mlflow
import mlflow.sklearn
import logging

logging.basicConfig(level=logging.WARN)

logger = logging.getLogger(__name__)

 mlflow.log_metric(""rmse"", rmse)",machin learningflow basic machin learningflow star,"['machin', 'learningflow', 'basic', 'machin', 'learningflow', 'star']",machine learningflow basics  machine learningflow starts from modeling to deployment.  >> it is an o
888,894,"Apache Spark (PySpark)

Apache Spark is an open-source unified analytics engine for large-scale data processing which requires multiple virtual machines. 

It is very popular analytics tool. 

Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm

> Spark was built on the top of Hadoop MapReduce module and adds superb speed

Hadoop is an open-source software framework for storing and processing big data in a distributed computing environment. The core of Hadoop consists of a storage part “HDFS” (Hadoop Distributed File System) and a processing part “MapReduce”. ",apach spark pyspark apach spark opensourc unifi an,"['apach', 'spark', 'pyspark', 'apach', 'spark', 'opensourc', 'unifi', 'an']",apache spark (pyspark)  apache spark is an open-source unified analytics engine for large-scale data
889,895,"Advantages of Spark

1. Speed: Spark is 100 times faster than MapReduce (because spark uses RAM)

2. Ease of use: allows to quickly write applications in Java, Scala, R and Python

3. Advanced analytics: support sql quaries, streaming data and advanced analytics",advantag spark 1 speed spark 100 time faster mapre,"['advantag', 'spark', '1', 'speed', 'spark', '100', 'time', 'faster', 'mapre']",advantages of spark  1. speed: spark is 100 times faster than mapreduce (because spark uses ram)  2.
890,896,"Spark Modules

1. Spark Core: It is the underlying general execution engine. It provides an RDD (Resilient Distributed Dataset) and in-memory (RAM) computing capabilities.

2. Spark SQL and DataFrame

3. Streaming: 

4. Mllib: MLlib is a scalable machine learning library

Spark Core is responsible for:

● Memory management and fault recovery
● Scheduling, distributing and monitoring jobs on a cluster
● Interacting with storage systems
",spark modul 1 spark core under general execut engi,"['spark', 'modul', '1', 'spark', 'core', 'under', 'general', 'execut', 'engi']",spark modules  1. spark core: it is the underlying general execution engine. it provides an resilien
891,897,"RDD operations

A Cluster manager breaks the main csv files into multiple RDD blocks for parallel processing in multiple nodes. Then different transformation occurs on RDD's and generates new RDD's. Then through actions we extract our required datasets

> Here the concept of driver node and worker node (can run application code in the cluster) comes in.

> RDD is an immutable and fault-tolerant system",resili distribut dataset oper cluster manag break ,"['resili', 'distribut', 'dataset', 'oper', 'cluster', 'manag', 'break']",resilient distributed dataset operations  a cluster manager breaks the main csv files into multiple 
892,898,"Transformations: map,
filter, join, union, and so on

Actions: collect, reduce, count,
first, and so on

",transform map filter join union action collect red,"['transform', 'map', 'filter', 'join', 'union', 'action', 'collect', 'red']","transformations: map, filter, join, union, and so on  actions: collect, reduce, count, first, and so"
893,899,"Anatomy of Spark Application

• Driver

• Application Master

• Spark Context (This is the object for entry point of a PySpark program. This object allows us to connect to a Spark cluster and create RDDs)

• Resource Manager (Cluster Manager)

• Executors",anatomi spark applic • driver • applic master • sp,"['anatomi', 'spark', 'applic', '•', 'driver', '•', 'applic', 'master', '•', 'sp']",anatomy of spark application  • driver  • application master  • spark context (this is the object fo
894,900,"Spark application coding

spark dataframe functions and methods are very similar as pandas data frame (but they are distributed and immutable)

import pyspark

my_spark_df=spark.read.csv('file_path'

my_spark_df.show(5)

> We can also start a spark session in colab notebook with few lines of codes

Git command for linking the Python API to Spark Core and initializing SparkContext

PySpark Shell",spark applic code spark datafram function method s,"['spark', 'applic', 'code', 'spark', 'datafram', 'function', 'method', 's']",spark application coding  spark dataframe functions and methods are very similar as pandas data fram
895,901,"Directed Acyclic Graph (DAG)

When a task is defined with simple pandas df like commands, spark builds a logical flow of operations that can be represented as directed acyclic graph (DAG) where node represents a RDD partition and the edge represents a data transformation.

> We can check the DAG for job details till the deepest level in a spark web interface

> Task is a defined unit of work within a DAG; it is represented as a node in the DAG graph, and it is written in Python.

> DAG is a collection of all the tasks we want to run, organized in a way that reflects their relationships and dependencies.

> In Graph view, we can visualize each and every step of our workflow with their dependencies and their current status",direct acycl graph direct acycl graph task defin s,"['direct', 'acycl', 'graph', 'direct', 'acycl', 'graph', 'task', 'defin', 's']",directed acyclic graph (directed acyclic graph)  when a task is defined with simple pandas df like c
896,902,"Azure Databricks is a data analytics platform optimized for the Microsoft Azure cloud services platform. Azure Databricks offers three environments for developing data intensive applications: Databricks SQL, Databricks Data Science & Engineering, and Databricks Machine Learning.

> Databricks was built on top of Spark and adds High reliability ",azur databrick data analyt platform optim microsof,"['azur', 'databrick', 'data', 'analyt', 'platform', 'optim', 'microsof']",azure databricks is a data analytics platform optimized for the microsoft azure cloud services platf
897,903,"Spark application deploy modes

There are two deploy modes that can be used to launch Spark applications on Apache Hadoop YARN - client mode and cluster mode",spark applic deploy mode two deploy mode use launc,"['spark', 'applic', 'deploy', 'mode', 'two', 'deploy', 'mode', 'use', 'launc']",spark application deploy modes  there are two deploy modes that can be used to launch spark applicat
898,904,"Apache Airflow

Platform to programmatically author, schedule and monitor
workflows

Luigi, Jenkins, AWS Step Functions, and Pachyderm are the most popular alternatives and competitors to Airflow.

> Airbnb created Apache Airflow",apach airflow platform programmat author schedul m,"['apach', 'airflow', 'platform', 'programmat', 'author', 'schedul', 'm']","apache airflow  platform to programmatically author, schedule and monitor workflows  luigi, jenkins,"
899,905,"Introduction to Jenkins

Jenkins is an open source automation server. It helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration (CI) and continuous deployment or delivery (CD).

Jenkins creates workflows using Declarative Pipelines, which are similar to GitHub Actions workflow files. ",introduct jenkin jenkin open sourc autom server he,"['introduct', 'jenkin', 'jenkin', 'open', 'sourc', 'autom', 'server', 'he']",introduction to jenkins  jenkins is an open source automation server. it helps automate the parts of
900,906,"Use of Apache Airflow

1. Manage scheduling and running jobs and data pipelines

2. Provides mechanisms for tracking the state of jobs and recovering from failure

3. Ensures jobs are ordered correctly based on dependencies

>> The strongest point of airflow is it can trigger any workflow including spark",use apach airflow 1 manag schedul run job data pip,"['use', 'apach', 'airflow', '1', 'manag', 'schedul', 'run', 'job', 'data', 'pip']",use of apache airflow  1. manage scheduling and running jobs and data pipelines  2. provides mechani
901,907,"Airflow Components

Airflow Components are Web server, Scheduler, Executor and Metadata database",airflow compon airflow compon web server schedul e,"['airflow', 'compon', 'airflow', 'compon', 'web', 'server', 'schedul', 'e']","airflow components  airflow components are web server, scheduler, executor and metadata database"
902,908,"
● Web Server and Scheduler: The Airflow web server and Scheduler are separate processes run (in this
case) on the local machine and interact with the database mentioned above.

Scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete (scheduling the execution of DAGs). 

● airflow.cfg is the Airflow configuration file which is accessed by the Web Server, Scheduler, and
Workers.

● DAGs refers to the DAG files containing Python code, representing the data pipelines to be run by Airflow. The location of these files is specified in the Airflow configuration file, but they need to be accessible by the Web Server, Scheduler, and Workers.

> By Airflow we can create variables where we can store and retrieve data at runtime in the multiple DAGS

from airflow.models import DAG",● web server schedul airflow web server schedul se,"['●', 'web', 'server', 'schedul', 'airflow', 'web', 'server', 'schedul', 'se']", ● web server and scheduler: the airflow web server and scheduler are separate processes run (in thi
903,909,"● Metadata Database: Airflow uses a SQL database to store metadata about the data pipelines being run.
Postgres is extremely popular with Airflow. Alternate databases supported with Airflow include MySQL.",● metadata databas airflow use structur queri lang,"['●', 'metadata', 'databas', 'airflow', 'use', 'structur', 'queri', 'lang']",● metadata database: airflow uses a structured query language database to store metadata about the d
904,910," Executor, Worker and Operator

● The Executor is shown separately in architecture but runs within the Scheduler.

Executors are the mechanism by which task instances get run. E.g. SequentialExecutor, Celery Executor, Kubernetes Executor

● The Worker(s) are separate processes which also interact with the other components of the Airflow architecture and the metadata repository

● Operators determine what actually gets done by a task. E.g. Bash Operator, Python Operator

> Azure Kubernetes Service (AKS) and few others offers serverless capabilities

> By Task duration we will be able to compare the duration of our tasks run at different time intervals",executor worker oper ● executor shown separ archit,"['executor', 'worker', 'oper', '●', 'executor', 'shown', 'separ', 'archit']"," executor, worker and operator  ● the executor is shown separately in architecture but runs within t"
905,911,"Applications of Airflow

Apache Airflow can be used to schedule:

● ETL pipelines that extract data from multiple sources and run Spark jobs or any other data
transformations
● Training machine learning models
● Report generation
● Backups and similar DevOps operations",applic airflow apach airflow use schedul ● etl pip,"['applic', 'airflow', 'apach', 'airflow', 'use', 'schedul', '●', 'etl', 'pip']",applications of airflow  apache airflow can be used to schedule:  ● etl pipelines that extract data 
906,912,"Understanding Celery

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing
operations with the tools required to maintain such a system.

Celery instance / Celery application or just celery app used as the entry-point for everything we want to do in
Celery, like creating tasks and managing workers, it must be possible for other modules to import it.",understand celeri celeri simpl flexibl reliabl dis,"['understand', 'celeri', 'celeri', 'simpl', 'flexibl', 'reliabl', 'dis']","understanding celery  celery is a simple, flexible, and reliable distributed system to process vast "
907,913,"Basic of Deep Neural Networks

Neural Network/Deep learning models are supervised learning models. Means we need labeled data for it's functioning.

A small neural network can also beat the performance of any other ML model. That is why deep learning is gaining its popularity, but it is a blackbox model.

> To get better performance we must have large neural networks and more data.

> Neuron applies some forms of non-linearity on the inputs
> The hidden layers of the neural networks actually does some feature engineering inside (learns different weights), about which we don't know. We only get the output
> So, it is called a blackbox model means, there is no interpretability
> Deep learning is a type of machine learning based on artificial neural networks in which multiple layers of processing are used to extract progressively higher level features from data.

In deep learning, at a very basic level, we convert an unstructured data (text, audio, image) into structured data (matrix)",basic deep neural network neural networkdeep learn,"['basic', 'deep', 'neural', 'network', 'neural', 'networkdeep', 'learn']",basic of deep neural networks  neural network/deep learning models are supervised learning models. m
908,914,"SNN, CNN and RNN

Types of Neural Network

1. Standard Neural Network (used for normal classification and regression problems on large dataset (>1 lakh experiences))

2. CNN-Convolutional Neural Network (used for image related inputs)

3. RNN-Recurrent Neural Network (used for sequencial input like text, audio)

4. Custom NN (for solving complex problems, like autonomous driving)",sneural network convolut neural network recurr neu,"['sneural', 'network', 'convolut', 'neural', 'network', 'recurr', 'neu']","sneural network, convolutional neural network and recurrent neural network  types of neural network "
909,915,"Channels of RGB image

An RGB image has three channels: red, green, and blue. RGB channels roughly follow the color receptors in the human eye, and are used in computer displays and image scanners.

R channel gives us a matrix with the intensity of red in every pexel.

G channel gives us a matrix with the intensity of green in every pexel.

B channel gives us a matrix with the intensity of blue in every pexel.

Seven colours of Rainbow:

Red, Orange, Yellow, Green, Blue, Indigo and Violet.

Red, Green and Blue are additive primary colours because they can make all other colors, even yellow. When mixed together, red, green and blue lights make white. ",channel rgb imag rgb imag three channel red green ,"['channel', 'rgb', 'imag', 'rgb', 'imag', 'three', 'channel', 'red', 'green']","channels of rgb image  an rgb image has three channels: red, green, and blue. rgb channels roughly f"
910,916,"Deeper understanding of deep neural network

We can think a neural network as ensembles of linear regression with kernel and does feature engineering in multiple layers. 

As we initialize the weights of the features randomly, every node of the first layer generates different activations means every node is generating a new feature with the combination of input features with feature importance (higher weights). This process goes on in multiple layers for generating higher level feature. 

In backpropagation feature importances (weights) are adjusted to get the desired label.

Regularization can be used to train models that generalize better on unseen data, by preventing the algorithm from overfitting the training dataset.",deeper understand deep neural network think neural,"['deeper', 'understand', 'deep', 'neural', 'network', 'think', 'neural']",deeper understanding of deep neural network  we can think a neural network as ensembles of linear re
911,918,"Loss function and Cost Function in DNN

when Loss function is denoted by L(ŷ,y) and 'm' is the number of observations

Cost Function, J(w,b)=1/m∑L(ŷ(i),y(i)), i=1,m

'w' is the weight and 'b' is the intercept

Partial derivative of J, dJ/dw is referred as 'dw' and dJ/db is referred as 'db'

Loss function for logistic regression

L(ŷ,y)= -[ylogŷ-(1-y)log(1-ŷ)]

Hyperparameter tuning for neural network is an iterative process, for minimizing the cost fuction(loss or error)",loss function cost function deep neural network lo,"['loss', 'function', 'cost', 'function', 'deep', 'neural', 'network', 'lo']","loss function and cost function in deep neural network  when loss function is denoted by l(ŷ,y) and "
912,919,"Coputation graph

A computational graph is defined as a directed graph where the nodes correspond to mathematical operations.

> We can create the computation graph for linear regression or logistic regression or deep learning model

> feed_dict feeds external data into computational graphs",coput graph comput graph defin direct graph node c,"['coput', 'graph', 'comput', 'graph', 'defin', 'direct', 'graph', 'node', 'c']",coputation graph  a computational graph is defined as a directed graph where the nodes correspond to
913,920,"Calculation behind gradient descent

> Calculation of 'z' values, 'a' values and cost function in forward propagation considering random weights
> Calculation of da, dz, dw and db in back propagation 
> Calculation of new weight of w and b for learning rate alpha and the gradient of cost function dw and db. Then another forward propagation begins
> Process continues till we reach global minima for the cost function (cost function means the average error in prediction for all the observations)
> In every step weight is updated by w= w -α*dw
> Gradient at a given layer is the product of all gradients at the previous layers.

> Weight between input and hidden layer have a constant input in each epoch of training a Deep Learning model

> In FeedForward ANN information flow is unidirectional

> Effect of false minima is reduced by stochastic update of weights

> For a non-continuous objective during optimization in deep neural net Subgradient method is used",calcul behind gradient descent calcul z valu valu ,"['calcul', 'behind', 'gradient', 'descent', 'calcul', 'z', 'valu', 'valu']","calculation behind gradient descent  > calculation of z values, a values and cost function in forwar"
914,921,"Understanding vectorization

> Z=W-transpose.X+b 
> CPU or GPU has the ability of parallel processing (which is called SIMD, single instruction multiple data). Numpy utilizes this ability of the computer
> for Z calculation in non-vectorized form, we use explicit for loop (Explicit describes something that is very clear), which is series calculation and very slow. So, we alway need to avoid for loops.
>> During numpy dot product of two vectors W-transpose and X, SIMD is utilized making the caculation very fast
>> X is a matrix (or vector) in which each row is one feature and each column is one training example. (For other ml algorithm it is transposed)",understand vector zwtransposexb central process un,"['understand', 'vector', 'zwtransposexb', 'central', 'process', 'un']",understanding vectorization  > z=w-transpose.x+b  > central processing unit or graphics processing u
915,922,"Vectorized implementation of forward propagation for layer l

Z [l]=W [l]A [l - 1]+b [l]

A [l]=g [l] (Z [l])

> Input layer is called the activation zero. A[0]=X.

> First hidden layer is the first layer and output layer is the final layer

> A neural network with 1 input layer and 2 hidden layers and 1 output layer have total 3 layers

> Shape of X is (nx,m) where nx= total no. of features and m= total no. of observations",vector implement forward propag layer l z lw la l ,"['vector', 'implement', 'forward', 'propag', 'layer', 'l', 'z', 'lw', 'la', 'l']",vectorized implementation of forward propagation for layer l  z [l]=w [l]a [l - 1]+b [l]  a [l]=g [l
916,923,"Notations in DNN

> When we represent a single observation, we use small letter notation (z,w,x)

> When we represent the entire dataset, we use capital letter notation (Z,W,X). So, Z,W and X are all vectors  not single numbers

> For multiple observations, Z,W,X are all matrices",notat deep neural network repres singl observ use ,"['notat', 'deep', 'neural', 'network', 'repres', 'singl', 'observ', 'use']","notations in deep neural network  > when we represent a single observation, we use small letter nota"
917,924,"SIMD units refer to hardware components that perform the same operation on multiple data operands concurrently. Typically, a SIMD unit receives as input two vectors (each one with a set of operands), performs the same operation on both sets of operands (one operand from each vector), and outputs a vector.",simd unit refer hardwar compon perform oper multip,"['simd', 'unit', 'refer', 'hardwar', 'compon', 'perform', 'oper', 'multip']",simd units refer to hardware components that perform the same operation on multiple data operands co
918,925," Vector dot product

If vector,
a.shape=(2,3) and b.shape=(3,1)

Then,  
> vector addition, a+b will give error because atleast one dimension (row/column ) is not matching

> vector addition with transpose, a+b.T will give result and shape of resultant vector will be (2,3)

> vector dot product, np.dot(a,b) will give result and the shape of the resultant vector will be (2,1). Vector dot product gives the result of matrix multiplication because dot products are done between the rows of the first matrix and the columns of the second matrix.",vector dot product vector ashape23 bshape31 vector,"['vector', 'dot', 'product', 'vector', 'ashape23', 'bshape31', 'vector']"," vector dot product  if vector, a.shape=(2,3) and b.shape=(3,1)  then,   > vector addition, a+b will"
919,926,"Element-wise matrix multiplication

a = np.random.randn(3, 3)
b = np.random.randn(3, 1)
c = a * b
c.shape=(3,3)

Why is an RNN used for machine translation?

> It can be trained as a supervised learning problem.

> It is applicable when the input/output is a sequence (e.g., a sequence of words).

",elementwis matrix multipl nprandomrandn3 3 b npran,"['elementwis', 'matrix', 'multipl', 'nprandomrandn3', '3', 'b', 'npran']","element-wise matrix multiplication  a = np.random.randn(3, 3) b = np.random.randn(3, 1) c = a * b c."
920,927,"Notation for multiple layers

In the superscript of Z, W, X or b if we write in parenthesis, it denotes the observation no.

Z(1) means the Z value for the first observation or training example

In the superscript of Z, W, X or b if we write in square bracket, it denotes the layer no.

Z[1] means the Z value in the first layer

Neurons are also called nodes or units

In the subscript we denote the node number",notat multipl layer superscript z w x b write pare,"['notat', 'multipl', 'layer', 'superscript', 'z', 'w', 'x', 'b', 'write', 'pare']","notation for multiple layers  in the superscript of z, w, x or b if we write in parenthesis, it deno"
921,928,"Activation functions

> In general activation function is denoted by 'g'
1. sigmoid function
if z is large in positive side then σ(z) or g(z)≈1

if z is large in negative side then σ(z) or g(z)≈0
*** a=g(z), g(z) is called the 'a' value
2. Tanh function
This activation function is similar as sigmoid, but the range is -1 to 1
3. ReLU Leaky ReLU function
Rectified Linear Unit (ReLU) is function where the output is non-linear for a certain period and then becomes linear.

ReLU is better activation function than Sigmoid and Tanh, because learnig becomes very slow for larger values of z as the gradient vanishes (flatens) in the case of sigmoid and tanh

But tanh function is better than sigmoid function, because it centralizes the data and it is useful for second layer learning. (Hyperbolic Tangent activation function also centralizes the data)

Sigmoid function is only used for binary classification problems

The range of ReLU is 0 to z

The range of Leaky ReLU is -0.01z to z

In different layer we can use different activation functions",activ function general activ function denot g 1 si,"['activ', 'function', 'general', 'activ', 'function', 'denot', 'g', '1', 'si']",activation functions  > in general activation function is denoted by g 1. sigmoid function if z is l
922,929,"Linear and nonlinear activation function

We apply linear activation function (similar to not applying any activation) in neural network for regression problems

Non linear activation functions (Sigmoid, Tanh, ReLU or Leaky ReLU) are called kernel approximation or kernel functions.",linear nonlinear activ function appli linear activ,"['linear', 'nonlinear', 'activ', 'function', 'appli', 'linear', 'activ']",linear and nonlinear activation function  we apply linear activation function (similar to not applyi
923,930,"Keras and TensorFlow

Keras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. 

TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs.  Keras is built in Python which makes it way more user-friendly than TensorFlow.

Keras is a deep learning wrapper on TensorFlow

Wrapper methods measure the “usefulness” of features based on the classifier performance
>> TensorFlow architecture works in 3 parts (Data pre-processing, Model building and Train and estimate the model)",kera tensorflow kera neural network librari tensor,"['kera', 'tensorflow', 'kera', 'neural', 'network', 'librari', 'tensor']",keras and tensorflow  keras is a neural network library while tensorflow is the open-source library 
924,931,"Problem of zero initialization

> When all the intial weights and biases are considered as zero, all the nodes in a particular layer will learn the same function even after multiple iterations of gradient descent. This is same as taking single node.
>> That is why random weights are initialized but very small values. This breaks the symmetry and allows different nodes to learn independently of each other 
>> Also for other ml model if all the weights are initiallized as same, after many iterations the model learns same weight for all the features. This is the “symmetry”",problem zero initi intial weight bias consid zero ,"['problem', 'zero', 'initi', 'intial', 'weight', 'bias', 'consid', 'zero']","problem of zero initialization  > when all the intial weights and biases are considered as zero, all"
925,932,"Initializing parameters for the model

> suppose we store the values of n[l] in layer_dims=[nx,4,3,1]

for i in range(1,len(layer_dims)):

parameter['W'+str(i)]= np.random.randn(layer_dims[i],layer_dims[i-1])*0.01

parameter['b'+str(i)]= np.random.randn(layer_dims[i],1)*0.01

# The above code is for understanding the concept not for running",initi paramet model suppos store valu nl layerdims,"['initi', 'paramet', 'model', 'suppos', 'store', 'valu', 'nl', 'layerdims']","initializing parameters for the model  > suppose we store the values of n[l] in layer_dims=[nx,4,3,1"
926,933," Deep Learning capabilities:

1. Classification Only (C)

2. Classification with Memory (CM)

3. Classification with Knowledge (CK)

4. Classification with Imperfect Knowledge (CIK)

5. Collaborative Classification with Imperfect Knowledge (CCIK)-ensembles of weak learners

> As range of hidden layers boom, model capability will increase",deep learn capabl 1 classif c 2 classif memori cm ,"['deep', 'learn', 'capabl', '1', 'classif', 'c', '2', 'classif', 'memori', 'cm']", deep learning capabilities:  1. classification only (c)  2. classification with memory (cm)  3. cla
927,934,"Mathematics behind 4L neural network for multiple observation

L= layers no.

g[l]= activation function of layer l

> Multiple layer neural network is called deep neural network

parameters (which the model learns) for a neural network

W[1], b[1], W[2], b[2],…. W[l], b[l]",mathemat behind 4l neural network multipl observ l,"['mathemat', 'behind', '4l', 'neural', 'network', 'multipl', 'observ', 'l']",mathematics behind 4l neural network for multiple observation  l= layers no.  g[l]= activation funct
928,935,"Layer dimension notation in Neural Network

n[l] means no. of nodes in l th layer
n[0]= no. of nodes in the input layer (=no. of features in the dataset)
n[1]=no. of nodes in the first hidden layer",layer dimens notat neural network nl mean node l t,"['layer', 'dimens', 'notat', 'neural', 'network', 'nl', 'mean', 'node', 'l', 't']",layer dimension notation in neural network  n[l] means no. of nodes in l th layer n[0]= no. of nodes
929,936,"Weight notation in Neural Network

For a two layer network, say there are three input nodes (X1,X2 and X3) and one hidden layer with two nodes, h1[1] and h2[1] and one output layer, h1[2]

> then the weights learned by h1 will be w11,w21,w31 and weights learned by h2 will be w12,w22,w32 for the same set of inputs

Initialized weight vector, W= [[w11,w12], [w21,w22], [w31,w32]] or [list_of_weight_for_input_variable_1, list_of_weight_for_input_variable_2, list_of_weight_for_input_variable_3,]

Thus, the shape of W is (3,2)

Weight notation in vectorized form

Weight vector for the first layer W[1]=W.T= [[w11,w21,w31],[w12,w22,w32]]

Thus, the shape of W[1] is (2,3) or shape of W[l] is (n[l], n[l-1])",weight notat neural network two layer network say ,"['weight', 'notat', 'neural', 'network', 'two', 'layer', 'network', 'say']","weight notation in neural network  for a two layer network, say there are three input nodes (x1,x2 a"
930,937,"Bias notation in vectorized form in Neural Network

b[1]= [[b1],[b2]]

shape of b[l] is (n[l],1)

for all the training example, m, shape of b[l] is (n[l],m)

> Shape of Z and A will be same as shape of b",bias notat vector form neural network b1 b1b2 shap,"['bias', 'notat', 'vector', 'form', 'neural', 'network', 'b1', 'b1b2', 'shap']","bias notation in vectorized form in neural network  b[1]= [[b1],[b2]]  shape of b[l] is (n[l],1)  fo"
931,938,"Network notation for i th experience

For a two layer network, say there are three input nodes (X1,X2 and X3) and one hidden layer with two nodes, h1[1] and h2[1] and one output layer, h1[2]

X1(i)--a1[1]--
X2(i)--a2[1]--a1[2]--y(i)
x3(i)--",network notat th experi two layer network say thre,"['network', 'notat', 'th', 'experi', 'two', 'layer', 'network', 'say', 'thre']","network notation for i th experience  for a two layer network, say there are three input nodes (x1,x"
932,939,"Forward propagation and backpropagation

During forward propagation, in the forward function for layer II we need to know what is the activation function in a layer.

During backpropagation, the corresponding backward function also needs to know what is the activation function for layer II, since the gradient depends on it.

We use catche to pass variables computed during forward propagation to corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives",forward propag backpropag forward propag forward f,"['forward', 'propag', 'backpropag', 'forward', 'propag', 'forward', 'f']","forward propagation and backpropagation  during forward propagation, in the forward function for lay"
933,940,"""shallow"" neural network is a term used to describe NN that usually have only one hidden layer (Standard NN) as opposed to deep NN which have several hidden layers, often of various types.

If we need a large shallow network (large no. of nodes or logic gates) to compute any function, using deep NN will be exponentially smaller.",shallow neural network term use describ neural net,"['shallow', 'neural', 'network', 'term', 'use', 'describ', 'neural', 'net']","""shallow"" neural network is a term used to describe neural network that usually have only one hidden"
934,941,"Basics of Improving Deep Neural Networks

We can split the dataset into three parts: Training, Development and Testing (fully untouched one, used as unseen data) as an improvement method

In case of big data, there is no need to follow 70-30 train-test split (old way of spliting data). We may take 98 % in training, 1% in development and 1% in testing, because here 1% is also a huge number and testing data does not contribute in learning, it is only for evaluation.

> But for human learning model, testing data also contribute in learning when we perform error analysis. This is the case like reinforcement learning model

> Always make sure that our train,dev and test have similar distribution of data",basic improv deep neural network split dataset thr,"['basic', 'improv', 'deep', 'neural', 'network', 'split', 'dataset', 'thr']","basics of improving deep neural networks  we can split the dataset into three parts: training, devel"
935,942,"Basic 'recipe' for all machine learning models

> Build our first simple model or system quickly and then slowly keep on increasing complexity

> Then we should use Bias/Variance analysis and error analysis to prioritize next step 

1. check the model has high bias or not (comparing trainig error with expected error) and take action.
2. Check the model has high variance or not (comparing training and dev error) and take action

In deep learning we do not concentrate on bias variance trade off. But our model should not have 
1. High bias, low variance
2. Low bias, high variance
3. High bias, high variance (also possible)

Our optimal model must have
1. low bias and low variance (optimum)",basic recip machin learn model build first simpl m,"['basic', 'recip', 'machin', 'learn', 'model', 'build', 'first', 'simpl', 'm']",basic recipe for all machine learning models  > build our first simple model or system quickly and t
936,943,"DNN model complexity

Big neural network may make our model overfitting (so, we can easily control model complexity by adjusting the layers and nodes. We can also reduce model complexity of big neural network by applying regularization)

Less data with large no. of dimensions can also make our model overfitting

For normal machine learning models, after a threshold limit of data, model performance saturate. 

But for deep neural network there is no limit of data. More data means more accuracy.

Regularization prevent overfitting in neural network

For extreamly large lambda, most of the weights will become zero and the model will become very simple.

For classification problem,

Every neuron in a neural network learns a classification boundary.

More the classification boundary becomes non-linear, more the model becomes complex. ",deep neural network model complex big neural netwo,"['deep', 'neural', 'network', 'model', 'complex', 'big', 'neural', 'netwo']","deep neural network model complexity  big neural network may make our model overfitting (so, we can "
937,944,"Dropout regularization

Apart from L1 and L2 regularization, there is another Dropout regularization (here we randomly make some node zero to make the model simpler)

> Weight decay is a regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration",dropout regular apart l1 l2 regular anoth dropout ,"['dropout', 'regular', 'apart', 'l1', 'l2', 'regular', 'anoth', 'dropout']","dropout regularization  apart from l1 and l2 regularization, there is another dropout regularization"
938,945,"Implementing dropout ('inverted dropout'): 

Here in a certain layer (l=3), we define keep prob=0.8 Means, with probability 0.2 the nodes will become zero in layer 3. So, if in layer 3 there were 10 nodes, 2 nodes will be deleted randomly

> There is no implementation of dropout during test, it is only implemented during training

> Generally used in computer vision (CNN architecture)

> Dropout can not be used in input and output layers",implement dropout invert dropout certain layer l3 ,"['implement', 'dropout', 'invert', 'dropout', 'certain', 'layer', 'l3']","implementing dropout (inverted dropout):   here in a certain layer (l=3), we define keep prob=0.8 me"
939,946,"Data Augmentation

When there is less data and more data can not be made available (generally in case of images), we generate more data by twisting, rotating or zooming the orginal iamge. This is an useful technique to reduce variance",data augment less data data made avail general cas,"['data', 'augment', 'less', 'data', 'data', 'made', 'avail', 'general', 'cas']",data augmentation  when there is less data and more data can not be made available (generally in cas
940,947,"Early stopping technique to stop overfitting

When the cost function for train and dev set diverge after certain no. of iterations, then we need to stop training at that iteration.

> This is not a very good technique to deal with overfitting",earli stop techniqu stop overfit cost function tra,"['earli', 'stop', 'techniqu', 'stop', 'overfit', 'cost', 'function', 'tra']",early stopping technique to stop overfitting  when the cost function for train and dev set diverge a
941,948,"Importance of normalized inputs

When cost function is drawn from normalized inputs, the curve becomes symmetric and gradient descent reaches minima faster",import normal input cost function drawn normal inp,"['import', 'normal', 'input', 'cost', 'function', 'drawn', 'normal', 'inp']","importance of normalized inputs  when cost function is drawn from normalized inputs, the curve becom"
942,949,"Vanishing/exploding gradients

If there is very large no. of layers, 
Then lower (<1) weights becomes very very small or vanishes when multiplied in every layer to reach final activation and thus the gradient also vanishes.(curve of y or J becomes parallel to x or weight axis). Means with the change in feature value there is no change in prediction. 

And higher (>1) weights becomes very very large or explodes when multiplied in every layer to reach final activation and thus the gradient also explodes. (curve of y or J becomes perpendicular to x or weight axis)",vanishingexplod gradient larg layer lower 1 weight,"['vanishingexplod', 'gradient', 'larg', 'layer', 'lower', '1', 'weight']","vanishing/exploding gradients  if there is very large no. of layers,  then lower (<1) weights become"
943,950,"Batch size and iteration

Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.

Iteration is one time processing for forward and backward for a batch of images (say one batch is defined as 16, then 16 images are processed in one iteration). Epoch is once all images are processed one time individually of forward and backward to the network, then that is one epoch.",batch size iter batch size term use machin learn r,"['batch', 'size', 'iter', 'batch', 'size', 'term', 'use', 'machin', 'learn', 'r']",batch size and iteration  batch size is a term used in machine learning and refers to the number of 
944,951,"Optimization Algorithms

1. Mini-Batch gradient descent

2. Gradient Descent with momentum

3. RMSprop

4. Adam Optimization Algorithm 

5. Learning rate decay


",optim algorithm 1 minibatch gradient descent 2 gra,"['optim', 'algorithm', '1', 'minibatch', 'gradient', 'descent', '2', 'gra']",optimization algorithms  1. mini-batch gradient descent  2. gradient descent with momentum  3. rmspr
945,952,"1. Mini-Batch gradient descent

For extremely large dataset, instead of passing the entire dataset at once, we can pass the entire dataset in small batches or mini batches.

This will solve the memory issue (RAM) for this extremely large computation

> In case of batch gradient descent, the cost function smoothly decreases with increase in no. of iterations 

> In case of mini-batch gradient descent, the cost function decreases with oscillation with increase in no. of iterations (because here iterations are performed with mini batches. Thus for some iteration cost function increases slightly and for some iteration decreases slightly)

> Average of the training samples produces stable error gradients and convergence.",1 minibatch gradient descent extrem larg dataset i,"['1', 'minibatch', 'gradient', 'descent', 'extrem', 'larg', 'dataset', 'i']","1. mini-batch gradient descent  for extremely large dataset, instead of passing the entire dataset a"
946,953,"> Notation of Mini-batch

In the superscript of Z, W, X or b if we write in curly braces, it denotes the mini-batch no.

Z{1} means the Z value for the first mini-batch",notat minibatch superscript z w x b write cur brac,"['notat', 'minibatch', 'superscript', 'z', 'w', 'x', 'b', 'write', 'cur', 'brac']","> notation of mini-batch  in the superscript of z, w, x or b if we write in curly braces, it denotes"
947,954,"> Choosing our mini-batch size

If mini-batch size=m: Batch gradient descent (we face memory shortage issue)-(here m is the total no. of observations)

If mini-batch size=1: stochastic gradient descent (here we loose the speed of vectorization)

Thus, choosing the mini-batch size in between 1 and m is important

> For <2k data points (for images or audio), we can use mini-batch size=m

> For larger dataset, we typically use mini-batch size=64(2^6), 128(2^7), 256(2^8), 512(2^9) because parallelization of neural network is best when the memory is used optimally",choos minibatch size minibatch sizem batch gradien,"['choos', 'minibatch', 'size', 'minibatch', 'sizem', 'batch', 'gradien']",> choosing our mini-batch size  if mini-batch size=m: batch gradient descent (we face memory shortag
948,955,"2. Gradient Descent with momentum

> To speed up our algorithm to reach the global minima, we use weighted gradient in the weight update equation, in place of using simple gradients (dw & db)

Hyperparameter =α

Additional hyperparameter=  β # It controls the amount of history (momentum) 

> With increase in the value of momemtum (β), learning becomes fast to reach the global minima for the cost function
> If β =0, it is like simple gradient descent with lowest learning speed.",2 gradient descent momentum speed algorithm reach ,"['2', 'gradient', 'descent', 'momentum', 'speed', 'algorithm', 'reach']","2. gradient descent with momentum  > to speed up our algorithm to reach the global minima, we use we"
949,956,"Exponentially weighted averages

weighted avg. of 't' th observation= β*weighted avg. of (t-1) th observation+ (1-β)*value of 't' th observation

-We can smoothen the curve of weighted avg. by taking higer values of β (this can be utilized for noisy data)

> Higher β means giving more imporatance to the past

> Increasing β will shift the weighted avg. line slightly to the right.

Bias correction in exponentially weighted averages

We use a bias correction factor to the weighted avg. to handle the bias in the initial period.",exponenti weight averag weight avg th observ βweig,"['exponenti', 'weight', 'averag', 'weight', 'avg', 'th', 'observ', 'βweig']",exponentially weighted averages  weighted avg. of t th observation= β*weighted avg. of (t-1) th obse
950,957,"
3. RMSprop

> To speed up our algorithm to reach the global minima, we use RMS gradient in the weight update equation, in place of using simple gradients (dw & db)

Hyperparameter =β≈0.9",3 rmsprop speed algorithm reach global minima use ,"['3', 'rmsprop', 'speed', 'algorithm', 'reach', 'global', 'minima', 'use']"," 3. rmsprop  > to speed up our algorithm to reach the global minima, we use rms gradient in the weig"
951,958,"4. Adam Optimization Algorithm 

It is a combination of Gradient Descent with momentum (AdaGrad) and RMSprop

Adam stands for Adaptive moment estimation

Hyperparameter =β1≈0.9,β2≈0.999,ε=10^-8

> Here, batch gradient descent or mini batch gradient descent can be used.",4 adam optim algorithm combin gradient descent mom,"['4', 'adam', 'optim', 'algorithm', 'combin', 'gradient', 'descent', 'mom']",4. adam optimization algorithm   it is a combination of gradient descent with momentum (adagrad) and
952,959,"5. Learning rate decay

> SGD wonder arround the global minima, it never reaches global minima.
> To solve this issue, if the learning rate (step size) decays with progress, SGD will wonder arround very close vicinity of the global minima.
> We can define any function of alpha so that its value decreases with increase in epoch",5 learn rate decay stochast gradient descent wonde,"['5', 'learn', 'rate', 'decay', 'stochast', 'gradient', 'descent', 'wonde']","5. learning rate decay  > stochastic gradient descent wonder arround the global minima, it never rea"
953,960,"Problem of local optima

In neural network, it is unlikely to get stuck in a bad local optima. 

There are milions of dimensions (weights) in neural network, so it is impossible to get all the partial derivates as zero at one point giviing a bad local optima.

> But problem of plateau can make the learning slow",problem local optima neural network unlik get stuc,"['problem', 'local', 'optima', 'neural', 'network', 'unlik', 'get', 'stuc']","problem of local optima  in neural network, it is unlikely to get stuck in a bad local optima.   the"
954,961,"Hyperparameters tuning in DNN 

There is a long list of hyperparameters in neural network to get the best set of weights

> To get the best values of hyperparameters we should not use grid search CV (takes all the combinations of X's), we should use random search CV (takes combination of X's randomly) for neural network. We also use Bayesian optimisation (it is usually employed to optimize expensive-to-evaluate functions. It takes combinations of X's intelligently) for XGBoost, Random Forest

Tuning scale varies from hyperparameter to hyperparameter and it is not uniform for most of the hyperparameters. Scale may be logarithmic or exponential.

> Retest the values of hyperparameters occationally

Hyperparameter tuning in pandas is like babysitting one model

Hyperparameter tuning in Caviar is like training many models in parallel

> During hyperparameter search, whether we try to babysit one model (“Panda” strategy) or train a lot of models in parallel (“Caviar”) is largely determined by the amount of computational power we can access",hyperparamet tune deep neural network long list hy,"['hyperparamet', 'tune', 'deep', 'neural', 'network', 'long', 'list', 'hy']",hyperparameters tuning in deep neural network   there is a long list of hyperparameters in neural ne
955,962,"Batch normalization

> This is actually a standardization technique
> We know the normalizing of inputs for a layer is to make the learning faster. 
> So the idea is, we can also normalize the outputs of a layer which are entering to the next layer through activation, to make the learning fastest.
>  Thus the 'z' values for a layer are normalized to get z-tilda for that layer and then through activation function it enters the next layer
> Here we introduce two more learnable parameters γ (gamma) and β

> γ and β can be learned using Mini-batch gradient descent, Gradient descent with momentum, RMSprop or Adam, not just with gradient descent.

> γ and β set the mean and variance of the linear variable z[l] of a given layer.

> Batch norm has a slight regularization effect on the model

> In the normalization formula, we use epsilon (ε) to avoid division by zero

z_norm = (z – μ) /√(σ^2 - ε) # z is the output for i th observation

After training a neural network with Batch Norm, at test time, to evaluate the neural network on a new example we should perform the needed normalizations, use μ and σ^2 estimated using an exponentially weighted average across mini-batches seen during training",batch normal actual standard techniqu know normal ,"['batch', 'normal', 'actual', 'standard', 'techniqu', 'know', 'normal']",batch normalization  > this is actually a standardization technique > we know the normalizing of inp
956,963,"Multi-class classification
(softmax classifer)

> So far we have discussed about binary classification of audio or iamges through deep neural network 

> For multi-class classification, softmax activation function works well",multiclass classif softmax classif far discuss bin,"['multiclass', 'classif', 'softmax', 'classif', 'far', 'discuss', 'bin']",multi-class classification (softmax classifer)  > so far we have discussed about binary classificati
957,964,"Deep learning programming framework

Caffe/Caffe2
CNTK
DL4J
Keras (simplest one)
Lasagne
mxnet
PaddlePaddle
TensorFlow
Theano
Torch",deep learn program framework caffecaffe2 cntk dl4j,"['deep', 'learn', 'program', 'framework', 'caffecaffe2', 'cntk', 'dl4j']",deep learning programming framework  caffe/caffe2 cntk dl4j keras (simplest one) lasagne mxnet paddl
958,965,"Python coding for DNN

import numpy as np
import tensorflow as tf
# define all the tf.Variables and cost function
train = tf.train.GradientDescentOptimizer(0.01).minimize(my_cost_function)
init = tf.global_variables_initializer()
session = tf.Session()
# for initializing the weights
session.run(init)
print(session.run(w))
# for starting the first iteration
session.run(train)
print(session.run(w))
# run a for loop for 'n' no. of iteration

Alternate session initialization

with tf.Session() as session:
session.run(init)
print(session.run(w))",python code deep neural network import numpi np im,"['python', 'code', 'deep', 'neural', 'network', 'import', 'numpi', 'np', 'im']",python coding for deep neural network  import numpy as np import tensorflow as tf # define all the t
959,966,"Different vector operations in tensor flow

for matrix multiplication tf.matmul(m1, m2)

for matrix addition tf.add(m1,m2)

Thus Linear Regression equation can be represented as follows:

out=tf.add(tf.matmul(X,W), b)

for reducing along rows  tf.reduce_sum(x, 0) # means we are squeezing from bottom and top so that two separate rows become one row.",differ vector oper tensor flow matrix multipl tfma,"['differ', 'vector', 'oper', 'tensor', 'flow', 'matrix', 'multipl', 'tfma']","different vector operations in tensor flow  for matrix multiplication tf.matmul(m1, m2)  for matrix "
960,967,"Cross entropy loss function (for classification problem), 

C= Σ y(i)*log(y_pred(i)) can be written as follows:

C= tf.reduce_sum(Y*tf.log(out))

",cross entropi loss function classif problem c σ yi,"['cross', 'entropi', 'loss', 'function', 'classif', 'problem', 'c', 'σ', 'yi']","cross entropy loss function (for classification problem),   c= σ y(i)*log(y_pred(i)) can be written "
961,968,"Computational graph 

> Variables in TensorFlow are also known as tensor objects

> To perform caculations in
TensorFlow we launch the computational graph in a
session

> Calculations can be done in parallel in computational graph",comput graph variabl tensorflow also known tensor ,"['comput', 'graph', 'variabl', 'tensorflow', 'also', 'known', 'tensor']",computational graph   > variables in tensorflow are also known as tensor objects  > to perform cacul
962,969,"Basics Structuring ML project

Structuring ML project means, having clear strategies for creating a good deep learning model

As there are too many parameters to tune a neural network, a clear strategy must be decided.",basic structur machin learn project structur machi,"['basic', 'structur', 'machin', 'learn', 'project', 'structur', 'machi']","basics structuring machine learning project  structuring machine learning project means, having clea"
963,970,"Orthogonalization Basics

Orthogonalization is a system design property that ensures that modification of an instruction or an algorithm component (hyperparameter) does not create or propagate side effects to other system components (hyperparameter)",orthogon basic orthogon system design properti ens,"['orthogon', 'basic', 'orthogon', 'system', 'design', 'properti', 'ens']",orthogonalization basics  orthogonalization is a system design property that ensures that modificati
964,971," Fundamental assumptions of supervised learning

1. We can fit training set well on cost function
2. The training set performance generalizes pretty well to the dev/test set.

",fundament assumpt supervis learn 1 fit train set w,"['fundament', 'assumpt', 'supervis', 'learn', '1', 'fit', 'train', 'set', 'w']", fundamental assumptions of supervised learning  1. we can fit training set well on cost function 2.
965,972,"Setting up our goal

1. Single number evaluation metric

> for example, if a metric shows multiple numbers of evaluation like precision and recall for different models, it will be difficult for us to decide the best model. But if the metric shows single number evaluation like F1 score, we can decide the best model esily.

2. Satisficing and optimizing metrics

When the requirement of the model is not suitable for single number evaluation metric, then we shall create one optimizing meric and N-1 satisficing merics meeting different criterias.",set goal 1 singl number evalu metric exampl metric,"['set', 'goal', '1', 'singl', 'number', 'evalu', 'metric', 'exampl', 'metric']","setting up our goal  1. single number evaluation metric  > for example, if a metric shows multiple n"
966,973,"Train/dev/test distribution

> We should mix and shuffle randomly all the data coming from different regions before train/dev/test distribution.

> Choose a dev set and test set to reflect data we expect to get in the future and consider important to do well on.

Size of Dev set

> Size of the dev set to be big enough to detect differences in algorithm/models we are trying out and select the best model.

Size of test set

> Size of the test set to be big enough to give high confidence in the overall performance of our selected model.

> If our model takes weeks to get trained, then it is worthy to buy faster computers that could speed up our teams’ iteration speed and thus our team’s productivity.

> If we have a large training set, then we should mix the data comming from different distributions (e.g from internet and from front facing camera) and split for train/dev/test set",traindevtest distribut mix shuffl random data come,"['traindevtest', 'distribut', 'mix', 'shuffl', 'random', 'data', 'come']",train/dev/test distribution  > we should mix and shuffle randomly all the data coming from different
967,974,"Changing dev/test sets and metrics

> If there is any special criteria (say, in no way the model should classify pornographic image as cat) for choosing the best model, then the weightage of that criteria must be included in our evaluation metric (the weightage may also be included in the cost function for the improvement in model performance) ",chang devtest set metric special criteria say way ,"['chang', 'devtest', 'set', 'metric', 'special', 'criteria', 'say', 'way']","changing dev/test sets and metrics  > if there is any special criteria (say, in no way the model sho"
968,975,"Bias/Variance Analysis

Comparing to human level performance

> For structured data, our model generally takes less time to reach human level performance

> For the natural perception  (audio or images), it takes lot of time to surpass human level performance and reach bayes optimal error level.(There is very less difference between human error and bayes error)",biasvari analysi compar human level perform struct,"['biasvari', 'analysi', 'compar', 'human', 'level', 'perform', 'struct']","bias/variance analysis  comparing to human level performance  > for structured data, our model gener"
969,976,"Human level error and avoidable bias

Human level error does not indicate to the performance of a single human, it indicates to the performance of whole human kind or team of expert human beings.

> Thus human level error is the closest proxy for bayes error.

> Difference between training error and human level error is the avoidable bias.

> Difference between training error and dev error is the variance

Problems where ML  significantly surpasses human-level performance

-Online advertising
-Product recommendations
-Logistics (predicting transit time)
-Loan approvals",human level error avoid bias human level error ind,"['human', 'level', 'error', 'avoid', 'bias', 'human', 'level', 'error', 'ind']",human level error and avoidable bias  human level error does not indicate to the performance of a si
970,977,"Error Analysis

> Look at dev examples (which our model misclassified) to get ideas to improve performance

> Evaluate multiple ideas in parallel preparing a table

> If there is incorrectly labeled data (substantial percentage), clean up incorrectly labeled data 

> If we correct the incorrectly labeled data on the dev set, then we should also correct the incorrectly labeled data in the test set, so that the dev and test sets continue to come from the same distribution.",error analysi look dev exampl model misclassifi ge,"['error', 'analysi', 'look', 'dev', 'exampl', 'model', 'misclassifi', 'ge']",error analysis  > look at dev examples (which our model misclassified) to get ideas to improve perfo
971,978,"Mislabeled data

Systematic mislabeled data/example in the training set, hamper our model performance. So, we must pay attention.

But if there is randomly mislabeled data/example, they do not impack our model much.",mislabel data systemat mislabel dataexampl train s,"['mislabel', 'data', 'systemat', 'mislabel', 'dataexampl', 'train', 's']","mislabeled data  systematic mislabeled data/example in the training set, hamper our model performanc"
972,979,"Mismatched training and dev/test data (if the training and dev data are from different distribution)

> In this case, how can we identify how much error is due to variance and how much error is due to mismatched data

> Training-dev set: same distribution as training set, but not used for training
> Then compare the errors for all the sets for clear understanding

> We have a large data-mismatch problem when our model does a lot better on the training-dev set than on the dev set",mismatch train devtest data train dev data differ ,"['mismatch', 'train', 'devtest', 'data', 'train', 'dev', 'data', 'differ']",mismatched training and dev/test data (if the training and dev data are from different distribution)
973,980,"Learning from multiple tasks
 
1. Transfer learning

2. Multi-task learning",learn multipl task 1 transfer learn 2 multitask le,"['learn', 'multipl', 'task', '1', 'transfer', 'learn', '2', 'multitask', 'le']",learning from multiple tasks   1. transfer learning  2. multi-task learning
974,981,"1. Transfer learning 

(here learned weights for one task is utilized for classifying another task) Eg. LeNet-5
When transfer learning makes sense?
> Task A&B has same input X
> We have lot more data for Task A than Task B
> Lowe level features from A could be helpful for learning B

(Low-level features are minor details of the image, like lines or dots, that can be picked up by.  High-level features are built on top of low-level features to detect objects and larger shapes in the image)

For transfer learning in CNN, people always borrow the entire 20+ layers networks and only train the last output layer to customize towards their use cases. This is not the case for LSTM with different vocabularies. But widely used in (NLP) LSTM with similar vocabularies 

Transfer learning from CNN to RNN is not possible

A pre-trained model is nothing but a deep learning model someone else built and trained on some data and can be used to solve simialr problem. 

Transfer Learning is a machine learning technique where we use a pre-trained neural network to solve a problem that is similar to the problem the network was originally trained to solve. We can use partial network and edit the final layer to fit our objectives.

Many pre-trained models are available online.",1 transfer learn learn weight one task util classi,"['1', 'transfer', 'learn', 'learn', 'weight', 'one', 'task', 'util', 'classi']",1. transfer learning   (here learned weights for one task is utilized for classifying another task) 
975,982,"2. Multi-task learning

> For identifying multiple objects from a picture, we can utilize multiple neural network with binary classification.

> Else, we can utilize single neural network with multi-task learning. Here the final layer will be have no. of nodes equal to no. of tasks

>  It is not particularly feasible to build one dataset to rule them all that is fully labeled with every category we would ever need.

use of multi-task neural network 

> Training on a set of tasks that could benefit from having shared lower-level features.

> Usually: Amount of data we have for each task is quite similar",2 multitask learn identifi multipl object pictur u,"['2', 'multitask', 'learn', 'identifi', 'multipl', 'object', 'pictur', 'u']","2. multi-task learning  > for identifying multiple objects from a picture, we can utilize multiple n"
976,983,"End to end deep learning

> The biggest advantage of deep learning is that, if we have big enough data, there is no need of feature engineering and lots of human efforts to develop a model, a neural network can do every thing to achieve the goal. This is called end to end deep learning.

Applying end-to-end deep learning

Key question: Do we have sufficient data to learn a function of the complexity need to map x to y?",end end deep learn biggest advantag deep learn big,"['end', 'end', 'deep', 'learn', 'biggest', 'advantag', 'deep', 'learn', 'big']","end to end deep learning  > the biggest advantage of deep learning is that, if we have big enough da"
977,984,"Pros and cons of end to end deep learning

Pros: 
> Let the data speak
> Less hand designing of components needed

Cons:
> May need large amount of data
> Excludes potentially useful hand-designed components which may be useful to improve model performance",pros con end end deep learn pros let data speak le,"['pros', 'con', 'end', 'end', 'deep', 'learn', 'pros', 'let', 'data', 'speak', 'le']",pros and cons of end to end deep learning  pros:  > let the data speak > less hand designing of comp
978,985,"Difference between Multi-class and multi-task learning

Multi-class learning is the terminology used when we predict a single label for each input, but each label is a single element from a set of possible labels. Multi-task learning is when we have different problems that need to be solved simultaneously (multiple labels for each input)",differ multiclass multitask learn multiclass learn,"['differ', 'multiclass', 'multitask', 'learn', 'multiclass', 'learn']",difference between multi-class and multi-task learning  multi-class learning is the terminology used
979,986,"Testing the model for the entire dataset

This would cause the dev and test set distributions to become different. This is a bad idea because we’re not aiming where we want to hit

If a teacher is teaching mathematics to a child. She teaches him some examples such as 1+1=2, 2+2=4 and 1+3 = 4. If she takes a test of that child and asks him only 1+1 or 2+2 or 1+3, he will give the exact answer. Based on this test she can not tell if the child has learned mathematics or not.",test model entir dataset would caus dev test set d,"['test', 'model', 'entir', 'dataset', 'would', 'caus', 'dev', 'test', 'set', 'd']",testing the model for the entire dataset  this would cause the dev and test set distributions to bec
980,987,"Perceptual task 

Perceptual tasks consist of studies aimed at distinguishing various biomarkers of perception including visual-spatial attention, auditory attention and olfactory attention

Here, perceptual task means computer vision through CNN

> CNN is used in
1. Image classification
2. Text classification
3. Object Detection

>> A Convolution matches or surpasses the output of an individual neuron to a visual stimuli. 

> Convolution means a thing that is complex and difficult to follow

> For black and white screen, there is only one channel which denotes the intensity of white

> When a high resolution image is used in a neural network, for RGB channels, what a huge number of feature inputs (3300*4200*3) it will be for one  observation, that is why for simplification CNN came into picture. 

We should always try to use famous architectures like LeNet-5, AlexNet, VGG, ResNet, Inception etc. in our model instead of building it from the scratch.",perceptu task perceptu task consist studi aim dist,"['perceptu', 'task', 'perceptu', 'task', 'consist', 'studi', 'aim', 'dist']",perceptual task   perceptual tasks consist of studies aimed at distinguishing various biomarkers of 
981,988,"Basics of Pixel 

Pixel  means a minute area of illumination on a display screen, one of many from which an image is composed.

The smallest resolution Windows supports is 640x480 pixels (meaning 640 dots horizontally by 480 vertically). Better video cards and monitors are capable of much higher resolutions. The standard resolution used today is 1024(W)x768(H).

Say computer monitor display area is 12'(W)x7'(H).

Then ppi (pixels per inch) in width= 1024/12=85

But 72 ppi is the standard

 A 12-megapixel camera (3300 x 4200 pixel), for example, can produce images with more than 12 million total pixels.

Considering 300 ppi for the best quality print, maximum size of the print will be
W=3300/300=11'
H=4200/300=14'

Early digital camera: 100x100 pixels (0.01 megapixels)",basic pixel pixel mean minut area illumin display ,"['basic', 'pixel', 'pixel', 'mean', 'minut', 'area', 'illumin', 'display']","basics of pixel   pixel  means a minute area of illumination on a display screen, one of many from w"
982,989,"Convolution on Black-and-white Image

For Edge detection, we use filters (a 3x3 matrix) to perform convolution operation with the picture intensity matrix:
>> vertical edge detection filter
>> Horizontal edge detection filter
>> There are different types of vertical and horizontal fileters like Sobel filter, Scharr filter etc.

Here, pixel locations are the feature names like cell reference in excel. 
Thus, total no. of pixel= total no. of feature= total no. of inputs
All the values of one picture intensity matrix are the feature values for one observation 

In CNN, we use filter but the weights are not defined. We leave it to the model to learn by itself.",convolut blackandwhit imag edg detect use filter 3,"['convolut', 'blackandwhit', 'imag', 'edg', 'detect', 'use', 'filter', '3']","convolution on black-and-white image  for edge detection, we use filters (a 3x3 matrix) to perform c"
983,990,"Convolution operation

> convolution operator is *
Say, picture intensity matrix is I with size 4X4 and filter size is 2X2 
I*F=resultant matrix

Then, the first element of the resultant matrix will be
Z11= I11*F11+I12*F12+I21*F21+I22*F22

>> Here, I11,I12,I21,I22 are the raw feature values(raw x's) and F11,F12,F21,F22 are the learnable weights(w's), Z11,Z12,Z13.. are the refined feature values (refined x's)

Here the equation of Z is similar like linear regression equation. It  is only a refinement of raw data (picture intensity matrix) to refined data (output matrix)

> Convolution operation can be compared with PCA for better understanding of important features",convolut oper convolut oper say pictur intens matr,"['convolut', 'oper', 'convolut', 'oper', 'say', 'pictur', 'intens', 'matr']","convolution operation  > convolution operator is * say, picture intensity matrix is i with size 4x4 "
984,991,"Padding in CNN

During convolution operation, the size of the intensity matrix is reduced. This is not a best practice because we are loosing informations.

Thus the concept of padding came into picture to retain the size of input matrix.

> Here before performing convolution operation, we are padding the input intensity matrix with zeros (in one or multiple layers depending on the filter size) in all sides equally

> > minimum filter size is 3X3 for convolution operation with padding [ to pad with atleast one layer (3-1)/2]

Valid Convolution= convolution with no padding (size of the resultant volume is reduced)

Same Convolution= convolution with  padding (size of input volume and size of resulting volume are same). Also known as  Zero Padding or Same padding

> A 3D matrix is called volume",pad convolut neural network convolut oper size int,"['pad', 'convolut', 'neural', 'network', 'convolut', 'oper', 'size', 'int']","padding in convolutional neural network  during convolution operation, the size of the intensity mat"
985,992,"Strided Convolution

extra hyperparameter, stride (step size)=2,3, etc.

Size of the input matrix is very much reduced in this case

> Stride is the distance between two consecutive receptive fields",stride convolut extra hyperparamet stride step siz,"['stride', 'convolut', 'extra', 'hyperparamet', 'stride', 'step', 'siz']","strided convolution  extra hyperparameter, stride (step size)=2,3, etc.  size of the input matrix is"
986,993,"Convolution on RGB image (Coloured image)

Here instead of 2D filter we need 3D filter (3x3x3 matrix) but the output is a 2D matrix

If we need 3D matrix output instead of 2D matrix output, then we need to apply mutiple 3D filters",convolut rgb imag colour imag instead 2d filter ne,"['convolut', 'rgb', 'imag', 'colour', 'imag', 'instead', '2d', 'filter', 'ne']",convolution on rgb image (coloured image)  here instead of 2d filter we need 3d filter (3x3x3 matrix
987,994,"Types of layer in a convolutional network:

1. Convolution (CONV) layer-it has parameters to learn 

2. Pooling (POOL) layer- may not be considered as layer, as no parameters to learn (only takes the max. or avg. value from the input volume according to filter size and stride)
> Though they do not have parameters to learn, but they affect the backpropagation (derivatives) calculation.

3. Fully connected (FC) layer-This is nothing but a single layer of standard NN, it has parameters to learn",type layer convolut network 1 convolut conv layeri,"['type', 'layer', 'convolut', 'network', '1', 'convolut', 'conv', 'layeri']",types of layer in a convolutional network:  1. convolution (conv) layer-it has parameters to learn  
988,995,"Pooling Layer

Max pooling-it is similar like convolution operation but instead of taking the summation of all the numbers, here max number is considered in the output matrix.

Here hyperparameters are: Filter size(f) and stride (s)

> max-pooling layer is to create a feature map containing the most prominent features of the previous feature map (also adds local invariance)

from keras.layers import MaxPooling2D
Average Pooling (rarely used)-instead of max we take average",pool layer max poolingit similar like convolut ope,"['pool', 'layer', 'max', 'poolingit', 'similar', 'like', 'convolut', 'ope']",pooling layer  max pooling-it is similar like convolution operation but instead of taking the summat
989,996,"Necessity of Convolution

> By the use of CONV+POOL layer we are actually reducing the number of inputs features and then using a FC layer. Thus the parameter to learn decreases significantly
> Parameter sharing
- It reduces the total number of parameters, thus reducing overfitting.
-It allows a feature detector to be used in multiple locations throughout the whole input image/input volume.

> Sparsity of connections (helps in translation invariance)-
means each activation in the next layer depends on only a small number of activations from the previous layer.

As the number of features are extreamly large for an image, CONV+POOL layers are used combinedly and repeatedly as a feature reduction technique

> Training a deeper network (adding additional layers to the network) allows the network to fit more complex functions (or complex feature) and thus almost always results in lower training error.",necess convolut use convpool layer actual reduc nu,"['necess', 'convolut', 'use', 'convpool', 'layer', 'actual', 'reduc', 'nu']",necessity of convolution  > by the use of conv+pool layer we are actually reducing the number of inp
990,997,"Calulating the size of output volume for convolution or pooling

say, 
size of input matrix= W X H

size of filter = F(W) X F(H)

stride = S(W), S(H)

Padding= P

Then,
Output width=
(W-F(W)+2P)/S(W)+1

Output height=
(H-F(H)+2P)/S(H)+1

say, 
size of input volume= W X H XC

size of filter = F(W) X F(H) XF(C)

stride = S(W), S(H)

Padding= P

no. of filters= n

If, C=F(C), equal no. of channels for input volume and filter,

Then, size of output volume,
{(W-F(W)+2P)/S(W)+1}X {(H-F(H)+2P)/S(H)+1} Xn

> If 2D pooling is applied on 3D input, output will be= input width, C",calul size output volum convolut pool say size inp,"['calul', 'size', 'output', 'volum', 'convolut', 'pool', 'say', 'size', 'inp']","calulating the size of output volume for convolution or pooling  say,  size of input matrix= w x h  "
991,998,"Notation for multiple CONV layers

> First feature map or the input volume 39X39X3
n_W[0]=n_H[0]=39, n_C[0]=3

> First CONV layer
f[1]=3, s[1]=1, P[1]=0, n=10

> Second feature map 37X37X10
n_W[1]=n_H[1]=37, n_C[1]=10

> Second CONV layer
f[2]=5, s[2]=2, P[2]=0, n=20

> Here f[1]=3 means 3X3 and equal channel as input volume 
and s[1]=1 means 1,1

> For an input volume 30X30X3 and without conv and pool layer, no. of inputs is 2700

> For an input volume 15x15x8, if pad=2, the dimension of the resulting volume is 19x19x8 (2+15+2=19)

> For an input volume 63x63x16, if filtered with 7x7 (f X f), stride of 1 and “same” convolution, then padding=3  
Because, (f-1)/2 = (7-1)/2=3

> For an input volume 32x32x16, and apply max pooling with a stride of 2 and a filter size of 2. The output volume is 
16x16x16

> Say, input is a 300 by 300 color (RGB) image, and we use a convolutional layer with 100 filters that are each 5x5. Then the hidden layer have 7600 parameters (including bias) 7500 (5X5X3X100) (without bias)

>  For an input volume 63x63x16, and convolve it with 32 filters that are each 7x7, using a stride of 2 and no padding. The output volume is 29x29x32

> Suppose we have an input volume of dimension 64x64x16. Then a single 1x1 convolutional filter have 17 parameters (including the bias)",notat multipl conv layer first featur map input vo,"['notat', 'multipl', 'conv', 'layer', 'first', 'featur', 'map', 'input', 'vo']","notation for multiple conv layers  > first feature map or the input volume 39x39x3 n_w[0]=n_h[0]=39,"
992,999,"Common behavior of all the CNN architecture

> Dimensions of the intensity matrix changes as follows:

H and W reduces and Depth increases with the progress in layer.

Then the intensity matrix is flattened out and used in FC layer for final output. ",common behavior convolut neural network architectu,"['common', 'behavior', 'convolut', 'neural', 'network', 'architectu']",common behavior of all the convolutional neural network architecture  > dimensions of the intensity 
993,1000,"LeNet-5 architecture

It was intially built to identify hand written digits (black-and-white)

It is an old architecture (sigmoid and tanh activation used)

AVG POOL is used

Output layer-softmax",lenet5 architectur intial built identifi hand writ,"['lenet5', 'architectur', 'intial', 'built', 'identifi', 'hand', 'writ']",lenet-5 architecture  it was intially built to identify hand written digits (black-and-white)  it is
994,1001,"AlexNet architecture

Modern architecture

MAX POOL is used

ReLU activation is being used

Output layer-softmax",alexnet architectur modern architectur max pool us,"['alexnet', 'architectur', 'modern', 'architectur', 'max', 'pool', 'us']",alexnet architecture  modern architecture  max pool is used  relu activation is being used  output l
995,1002,"VGG-16 architecture

Modern architecture 

Multiple CONV layers are being used

16 signifies 16 numbers of layers. It is a very big network.

ReLU activation is being used

Output layer-softmax

",vgg16 architectur modern architectur multipl conv ,"['vgg16', 'architectur', 'modern', 'architectur', 'multipl', 'conv']",vgg-16 architecture  modern architecture   multiple conv layers are being used  16 signifies 16 numb
996,1003,"ResNet and Inception architecture

More Advance and Complex Networks

1. ResNet- Residual Network

> ResNet helps NN to learn identity function/features very easily
> The skip-connection makes it easy for the network to learn an identity mapping between the input and the output within the ResNet block.

> Using a skip-connection helps the gradient to backpropagate fast and thus helps us to train deeper networks

2. Inception

> A single inception block allows the network to use a combination of 1x1, 3x3, 5x5 convolutions and pooling.

> Inception blocks usually use 1x1 convolutions to reduce the input data volume’s size before applying 3x3 and 5x5 convolutions.",resnet incept architectur advanc complex network 1,"['resnet', 'incept', 'architectur', 'advanc', 'complex', 'network', '1']",resnet and inception architecture  more advance and complex networks  1. resnet- residual network  >
997,1004,"Network in Network

Network in Network is actually 1X1 convolution

> Mainly used for 3D pictures

> for 3D picture there are more than 3 channels, in fact lot many channels

> 1X1 convolution is used to decrease the no. of channels (or depth)

> To save huge computational cost

We can use ConvNet for 1D image like ECG

We can also use ConvNet for 3D image like CT Scan image

e.g. 
For 3D image, input volume has size 32x32x32x16 (this volume has 16 channels)",network network network network actual 1x1 convolu,"['network', 'network', 'network', 'network', 'actual', '1x1', 'convolu']",network in network  network in network is actually 1x1 convolution  > mainly used for 3d pictures  >
998,1005,"Using open-source implementation

For computer vision, always try to use as much open source implementation as possible

Building on top of other's implementation
> search for network architecture source code on google
> To to Github repo
> Download or gitclone

Use in-built implementations present in deep learning frameworks
> Keras
> Tensorflow
> Pytorch

Use transfer learning as much as possible

transfer learning is very useful when we have very less inputs (say 100 pictures)",use opensourc implement comput vision alway tri us,"['use', 'opensourc', 'implement', 'comput', 'vision', 'alway', 'tri', 'us']","using open-source implementation  for computer vision, always try to use as much open source impleme"
999,1006,"Common Augmentation methods

> Mirroring

> Cropping

> Rotation

> Blurring

> Shearing

> Local warping

> Colour shifting etc.",common augment method mirror crop rotat blur shear,"['common', 'augment', 'method', 'mirror', 'crop', 'rotat', 'blur', 'shear']",common augmentation methods  > mirroring  > cropping  > rotation  > blurring  > shearing  > local wa
1000,1007,"Sources of data for any ML model

There are two sources of data for any ML model

1. Input data (raw/structured)
2. Engineered or hand engineered or synthesised data

",sourc data machin learn model two sourc data machi,"['sourc', 'data', 'machin', 'learn', 'model', 'two', 'sourc', 'data', 'machi']",sources of data for any machine learning model  there are two sources of data for any machine learni
1001,1008,"Object detection 

Object detection (multi-task) requires less data than image recognition /classification (multi-class)

Speech recognition model need much more data than image recognition/ classification model",object detect object detect multitask requir less ,"['object', 'detect', 'object', 'detect', 'multitask', 'requir', 'less']",object detection   object detection (multi-task) requires less data than image recognition /classifi
1002,1009,"Tips for winning competitions

> Ensembling: Train several networks independently and average their outputs

> Multi-crop at test time: Run clssifier on multiple versions of test images and average results",tip win competit ensembl train sever network indep,"['tip', 'win', 'competit', 'ensembl', 'train', 'sever', 'network', 'indep']",tips for winning competitions  > ensembling: train several networks independently and average their 
1003,1010,"Difference between image classification and object detection

Image Classification helps us to classify what is contained in an image. Image Classification with Localization will specify the location of single object in an image whereas Object Detection specifies the location of multiple objects in the image. ",differ imag classif object detect imag classif hel,"['differ', 'imag', 'classif', 'object', 'detect', 'imag', 'classif', 'hel']",difference between image classification and object detection  image classification helps us to class
1004,1011," Image Classification with Localization

Similar to ConvNet (image classification) but output is slightly different.
> also called semantic segmentation (Locating objects in an image by predicting each pixel as to which class it belongs to)

Need to output- 
1. whether any object is present or not
2. center of the bounding box
3. height and width of the bounding box and
4. class labels 
So, y will be a vector
Y=[1(logistic unit,P_c),bx,by,bh,bw,1(class-c1),0(class-c2)]

If the logistic unit of y vector= 0, means no class found
Then we don't care (?) about other terms

y=[0,?,?,?,?,?,?]

size of y is 1 X (5+no. of classes) 

size of y is 1 X 7 for 2 classes, 1 X 25 for 20 classes",imag classif local similar convnet imag classif ou,"['imag', 'classif', 'local', 'similar', 'convnet', 'imag', 'classif', 'ou']", image classification with localization  similar to convnet (image classification) but output is sli
1005,1012,"Landmark detection

This is similar to Image classification with localization.

Here, we can analyze the object in the image by defining multiple landmarks (points) during training

All the landmarks are like individual task. Thus, if there are N landmarks, there will  be 2N (yes+no) output units",landmark detect similar imag classif local analyz ,"['landmark', 'detect', 'similar', 'imag', 'classif', 'local', 'analyz']","landmark detection  this is similar to image classification with localization.  here, we can analyze"
1006,1013,"Object detection

> If we take sliding window or multiple cropped image for detecting multiple objects in a picture and then pass all the cropped images to the same ConvNet, then this will become computationally very expensive

> To substantially reduce the computational cost, there is convolutional implementation of sliding windows where single image is fed to a single ConvNet",object detect take slide window multipl crop imag ,"['object', 'detect', 'take', 'slide', 'window', 'multipl', 'crop', 'imag']",object detection  > if we take sliding window or multiple cropped image for detecting multiple objec
1007,1014,"Object detection algorithm

Steps towards outputting a single and accurate bounding box

1. YOLO algorithm

2. Intersection over union (IoU)

3. Non-max supression

4. anchor box algorithm
",object detect algorithm step toward output singl a,"['object', 'detect', 'algorithm', 'step', 'toward', 'output', 'singl', 'a']",object detection algorithm  steps towards outputting a single and accurate bounding box  1. yolo alg
1008,1015,"1. YOLO algorithm

YOLO stands for You Only Look Once

> The biggest advantage of using YOLO is its superb speed – it’s incredibly fast and can process 45 frames per second. 

label for training: For each grid cell

Here, y is a 3D array",1 yolo algorithm yolo stand look biggest advantag ,"['1', 'yolo', 'algorithm', 'yolo', 'stand', 'look', 'biggest', 'advantag']",1. yolo algorithm  yolo stands for you only look once  > the biggest advantage of using yolo is its 
1009,1016,"Steps in YOLO algorithm

(for finding the bounding box)
>> YOLO first takes an input image
>> The framework then divides the input image into grids (say a 19 X 19 grid)
>> Image classification with localization are applied on each grid. YOLO then predicts the bounding boxes and their corresponding class probabilities for objects 

Say, on any input volume 19x19 grid is applied, for 20 classes, and with 5 anchor boxes. 
output volume for 19X 19 grid and 20 classes will be 19 X19X25

As there are 5 anchor boxes, 
Then output volume will be 
19x19x(5x25)",step yolo algorithm find bound box yolo first take,"['step', 'yolo', 'algorithm', 'find', 'bound', 'box', 'yolo', 'first', 'take']",steps in yolo algorithm  (for finding the bounding box) >> yolo first takes an input image >> the fr
1010,1017," Intersection over union  algorithm

For finding accurate bounding box

2. Intersection over union (IoU)= size of intersection/size of union (between two bounding boxes)

correct if IoU >=0.5

If, the upper-left box is 2x2, and the lower-right box is 2x3. The overlapping region is 1x1, Then IoU=1*1/(2*2+2*3-1*1)=1/9",intersect union algorithm find accur bound box 2 i,"['intersect', 'union', 'algorithm', 'find', 'accur', 'bound', 'box', '2', 'i']", intersection over union  algorithm  for finding accurate bounding box  2. intersection over union (
1011,1018,"3. Non-max supression

For finding single bounding box

When there are multiple bounding boxes for same object, then this algorithm ignores the non-max bounding boxes (probabillity of less than maximum).

> Finally we get non-max supressed output",3 nonmax supress find singl bound box multipl boun,"['3', 'nonmax', 'supress', 'find', 'singl', 'bound', 'box', 'multipl', 'boun']",3. non-max supression  for finding single bounding box  when there are multiple bounding boxes for s
1012,1019,"4. anchor box algorithm

If there are multiple objects in a single grid

Overlapping objects have same center of bounding box.

Then, we define overlapping bounding boxes as anchor box-1, anchor box-2 etc.

Output for each anchor box will be stacked in the width of output volume",4 anchor box algorithm multipl object singl grid o,"['4', 'anchor', 'box', 'algorithm', 'multipl', 'object', 'singl', 'grid', 'o']",4. anchor box algorithm  if there are multiple objects in a single grid  overlapping objects have sa
1013,1020,"Face Verification 

> Input image, name/ID
> Output whether the input image is that of the claimed person

",face verif input imag nameid output whether input ,"['face', 'verif', 'input', 'imag', 'nameid', 'output', 'whether', 'input']","face verification   > input image, name/id > output whether the input image is that of the claimed p"
1014,1021,"Face Recognition

> Has a database of K persons
> Get an input image
> Output ID if the image is nay of the K persons (or 'not recognized') 

Here, instead of learning the pattern of the image, it learns a ""similarity"" function

similary function, d(image1,image2)=degree of difference between images

Face recognition is a tougher task than image classification or face verification.

Because, it is an one-shot learning means learning from one example to recognize the person again.

In image classification, we were supposed to tell is there any human being in the image or not from multiple images of human being.

Face recognition is one step ahead. So, ConvNet alone will not serve the purpose.



",face recognit databas k person get input imag outp,"['face', 'recognit', 'databas', 'k', 'person', 'get', 'input', 'imag', 'outp']",face recognition  > has a database of k persons > get an input image > output id if the image is nay
1015,1022,"Siamese network

Here, an image is passed through a ConvNet and stopped before the softmax layer and we get the encoded image.

Then we calculate the difference between the encodings

The learning goal is 
> If the two images are the same person, difference is small
> If the two images are the different person, difference is large

Here the loss function (for one example) is called Triplet loss

Anchor image (A), Positive image(P) and Negative image (N)

If α is the margin between two similarity functions, 
d(A,P)+ α<= d(A,N)
or,
d(A,P)-d(A,N)+ α <=0 

or,
max(|f(A)−f(P)|^2−|f(A)−f(N)|^2 +α, 0)

Triplet loss = L(A,P,N)

Choose triplets that are hard to train on (N image is very much similar as A) to make the model robust

So, A,P, N should not be choosen randomly",siames network imag pass convnet stop softmax laye,"['siames', 'network', 'imag', 'pass', 'convnet', 'stop', 'softmax', 'laye']","siamese network  here, an image is passed through a convnet and stopped before the softmax layer and"
1016,1023,"Training set for calculating Triplet loss 

Each training example consists of one anchor image, one positive image and one negative image",train set calcul triplet loss train exampl consist,"['train', 'set', 'calcul', 'triplet', 'loss', 'train', 'exampl', 'consist']","training set for calculating triplet loss   each training example consists of one anchor image, one "
1017,1024,"Neural Style Transfer

From a content image (C) and style image (S), the model will generate a stylish content image (G)

Neural style transfer is not really machine learning, but an interesting side effect/output of machine learning on image tasks.
",neural style transfer content imag c style imag mo,"['neural', 'style', 'transfer', 'content', 'imag', 'c', 'style', 'imag', 'mo']","neural style transfer  from a content image (c) and style image (s), the model will generate a styli"
1018,1025,"Finding generated image

1. Initiate G randomly (G=1000x1000x3)

2. Use gradient descent to minimize J(G)

J(G)=αJ-content(C,G)+βJ-style (S,G)

>  In each iteration the pixel values of the generated image G is optimized through gradient descent",find generat imag 1 initi g random g1000x1000x3 2 ,"['find', 'generat', 'imag', '1', 'initi', 'g', 'random', 'g1000x1000x3', '2']",finding generated image  1. initiate g randomly (g=1000x1000x3)  2. use gradient descent to minimize
1019,1026,"Style and style matrix 

Style is defined as the correlation between activations across channels

In the deeper layers of a ConvNet, each channel corresponds to a different feature detector. 

The style matrix G[l] measures the degree to which the activations of different feature detectors in layer l vary (or correlate) together with each other.",style style matrix style defin correl activ across,"['style', 'style', 'matrix', 'style', 'defin', 'correl', 'activ', 'across']",style and style matrix   style is defined as the correlation between activations across channels  in
1020,1027,"Examples of Models with sequence data

1. Speech recognition
2. Music generation
3. Sentiment classification
4. DNA sequence analysis
5. Machine translation
6. Video activity recognition
7. Name entity recognition

>> A recurrent neural network can be unfolded into a full-connected neural network with infinite length.",exampl model sequenc data 1 speech recognit 2 musi,"['exampl', 'model', 'sequenc', 'data', '1', 'speech', 'recognit', '2', 'musi']",examples of models with sequence data  1. speech recognition 2. music generation 3. sentiment classi
1021,1028,"Notation in RNN

Here the elements (words) of each sequence (sentence or example) are denoted with numbers in angle bracket in superscript. e.g. 
x<1>, x<2>, x<3>,x<t>

and y<1>, y<2>, y<3>, y<t>

x(i)<t> means 't' th element of 'i' th example

y(i)<t> means 't' th element of 'i' th example

> Here every layer has two set of inputs x<t> and a<t-1>; two outputs a<t> and y<t>

a<t>= g_1(w_aa*a<t-1> + w_ax*x<t> + b_a)

y<t> = g_2(w_ya*a<t> + b_y)
 
> Back propagation in RNN is named as Backpropagation through time",notat recurr neural network element word sequenc s,"['notat', 'recurr', 'neural', 'network', 'element', 'word', 'sequenc', 's']",notation in recurrent neural network  here the elements (words) of each sequence (sentence or exampl
1022,1029,"Vectorization of words

> We define a vocabulary of that particular domain

> Then by one hot encoding we convert any word into vector",vector word defin vocabulari particular domain one,"['vector', 'word', 'defin', 'vocabulari', 'particular', 'domain', 'one']",vectorization of words  > we define a vocabulary of that particular domain  > then by one hot encodi
1023,1030,"Problem of a standard neural network

Problems:
-Inputs,outputs can be different lengths in different examples

-Doesn't share features learned across different positions of text

Solution:
Recurrent Neural Network (RNN)

> Recurrent means occurring  repeatedly.",problem standard neural network problem inputsoutp,"['problem', 'standard', 'neural', 'network', 'problem', 'inputsoutp']","problem of a standard neural network  problems: -inputs,outputs can be different lengths in differen"
1024,1031,"Summary of RNN

1. For a standard network, there are multiple inputs but one activation output in every layer and we generate output ŷ from the final layer. 

In case of RNN, we may generate output ŷ from every layer and calculate the loss in every layer. (although, depending on the type of problem, no. of layers, no. of inputs (Tx) and no. of outputs (Ty) may not be equal) 

2. Thus every layer of RNN is a standard neural network with multiple nodes and it is repeating for all others words of the expression or statement. 

3. Only difference is that we are passing the understanding of every word from one standard network to the next standard network. Finally calculating the loss function summing up the losses from all of the standard networks

4. Then we calculate the cost function of our model considering all of the examples of expression or statement",summari recurr neural network 1 standard network m,"['summari', 'recurr', 'neural', 'network', '1', 'standard', 'network', 'm']","summary of recurrent neural network  1. for a standard network, there are multiple inputs but one ac"
1025,1032,"Types of RNN

1. One to one

2. One to many (used in music generation)

3. Many to one (sentiment classification)

4. Many to many (Tx=Ty) (Name entity recognition)

5. Many to many (Tx!=Ty) (Machine translation)",type recurr neural network 1 one one 2 one mani us,"['type', 'recurr', 'neural', 'network', '1', 'one', 'one', '2', 'one', 'mani', 'us']",types of recurrent neural network  1. one to one  2. one to many (used in music generation)  3. many
1026,1033,"Language Modelling

Language modelling means what our model hears. That means it calculates the probability of any expression or statement (or probability of sentence) by calculating the conditional probability of each word in every layer.",languag model languag model mean model hear mean c,"['languag', 'model', 'languag', 'model', 'mean', 'model', 'hear', 'mean', 'c']",language modelling  language modelling means what our model hears. that means it calculates the prob
1027,1034,"Sampling or creating sequence (sentence) from a trained language model (RNN)

If we train our RNN model on the whole lot of Shakespeare's poem, then our language model will be able to generate poem like Shakespeare.",sampl creat sequenc sentenc train languag model re,"['sampl', 'creat', 'sequenc', 'sentenc', 'train', 'languag', 'model', 're']",sampling or creating sequence (sentence) from a trained language model (recurrent neural network)  i
1028,1035,"Character-level language model

Instead of creating word level vocabulary where there is unknown token, <UNK>, we create vocabulary of all the characters including letters, punctuations, numbers etc.",characterlevel languag model instead creat word le,"['characterlevel', 'languag', 'model', 'instead', 'creat', 'word', 'le']",character-level language model  instead of creating word level vocabulary where there is unknown tok
1029,1036," Exploding and Vanishing gradient

> Exploding gradient problem (weights and activations are all taking on the value of NaN) can be managed by gradient clippping 

> Vanishing gradient problem of RNN is managed by Gated Recurrent Unit (GRU)
",explod vanish gradient explod gradient problem wei,"['explod', 'vanish', 'gradient', 'explod', 'gradient', 'problem', 'wei']", exploding and vanishing gradient  > exploding gradient problem (weights and activations are all tak
1030,1037,"Gated Recurrent Unit

> In GRU, we are assigning memory gate activation so that the understanding of any word does not vanishes during long forward or backward propagation

It has two gates
i> Reset gate and 
ii> Update gate

GRUs are very similar to Long Short Term Memory(LSTM). Just like LSTM, GRU uses gates to control the flow of information. They are relatively new as compared to LSTM. This is the reason they offer some improvement over LSTM and have simpler architecture.",gate recurr unit gate recurr unit assign memori ga,"['gate', 'recurr', 'unit', 'gate', 'recurr', 'unit', 'assign', 'memori', 'ga']","gated recurrent unit  > in gated recurrent unit, we are assigning memory gate activation so that the"
1031,1038,"Long short-term memory (LSTM) 

There are three different gates in an LSTM cell: 
i> forget gate, 
ii> input gate and
iii> output gate.

update gate = 
forget gate + input gate

Update and forget gate in LSTM palys simmilar role to Γ_u and (1−Γ_u) in GRU

Say, we are training a LSTM, have a 10000 word vocabulary, 100-dimensional activations a. Then the dimension of Γ_u at each time step will be 100

> LSTM can also be used for time series problems",long shortterm memori long short term memori three,"['long', 'shortterm', 'memori', 'long', 'short', 'term', 'memori', 'three']",long short-term memory (long short term memory)   there are three different gates in an long short t
1032,1039,"Bidirectional RNN (BRNN)

Getting information from the future words to predict a current word

BRNN with LSTM is very much popular for all of the NLP task",bidirect recurr neural network brecurr neural netw,"['bidirect', 'recurr', 'neural', 'network', 'brecurr', 'neural', 'netw']",bidirectional recurrent neural network (brecurrent neural network)  getting information from the fut
1033,1040,"Deep RNN

Here, for predicting y we use multiple activation layers instead of one.

> Deep RNN's are useful for NLP applications.",deep recurr neural network predict use multipl act,"['deep', 'recurr', 'neural', 'network', 'predict', 'use', 'multipl', 'act']","deep recurrent neural network  here, for predicting y we use multiple activation layers instead of o"
1034,1041,"Natural language generation (NLG) comes after NLU

It has three steps:

1. Text planning
2. Sentence planning
3. Text realization",natur languag generat nlg come natur languag under,"['natur', 'languag', 'generat', 'nlg', 'come', 'natur', 'languag', 'under']",natural language generation (nlg) comes after natural language understanding  it has three steps:  1
1035,1042,"Huggingface Transformer

!pip install transformers
from transformers import pipeline

ques_ans_pipeline = pipeline(""question-answering"")
context = ' '
question= ' ' 
ans = ques_ans_pipeline(question=question, context=context)
print(ans['answer'])

sentimentAnalysis_pipeline = pipeline(""sentiment-analysis"")

test_sentence='This is not a good story'

print(sentimentAnalysis_pipeline(test_sentence))",huggingfac transform pip instal transform transfor,"['huggingfac', 'transform', 'pip', 'instal', 'transform', 'transfor']",huggingface transformer  !pip install transformers from transformers import pipeline  ques_ans_pipel
1036,1043,"NLP and Word Embedding

> Embedding and encoding (conversion of human language to values) are similar but not fully same. 

>  Embedding means encoding with contex",natur languag process word embed embed encod conve,"['natur', 'languag', 'process', 'word', 'embed', 'embed', 'encod', 'conve']",natural language processing and word embedding  > embedding and encoding (conversion of human langua
1037,1044,"Featurized representation: word embedding

If we can create a vocabulary with a list of feature words (like gender, age, size, food, cost etc.), then we will get contextual vectors for every word and can train our model to learn these feature weights.

In real modelling, we do not specify any feature, we let our model learn all the features and their weights on its own.",featur represent word embed creat vocabulari list ,"['featur', 'represent', 'word', 'embed', 'creat', 'vocabulari', 'list']",featurized representation: word embedding  if we can create a vocabulary with a list of feature word
1038,1045,"Visualizing word embeddings

we can reduce n-dimensions (n number of features) to 2D by t-SNE to see how multiple words are making clusters

t-SNE is a non-linear dimensionality reduction technique",visual word embed reduc ndimens n number featur 2d,"['visual', 'word', 'embed', 'reduc', 'ndimens', 'n', 'number', 'featur', '2d']",visualizing word embeddings  we can reduce n-dimensions (n number of features) to 2d by t-sne to see
1039,1046,"Embedding matrix 

If we take the vocabulary of words in the column and vocabulary of feature words in rows, then the matrix is called embedding matrix

Thus, embedding matrix contains the contextual weights of the words in the vocabulary

Embedding matrix is denoted by E, One hot encoded vector is denoted by O and word embedding vector is denoted by e.
Then, 
E.O=e

In the above vector dot product or matrix multiplication, matrix, E transforms one hot encoded vector, O according to the context and gives the resultant word embedding vector,e.

> word embedding vectors are the final inputs for the model
> Say, size of E is 10X200 and size of O is 200X1, then the size of e will be 10X1. That means 
dimension of word embedding= no. of contex or features",embed matrix take vocabulari word column vocabular,"['embed', 'matrix', 'take', 'vocabulari', 'word', 'column', 'vocabular']",embedding matrix   if we take the vocabulary of words in the column and vocabulary of feature words 
1040,1047,"Transfer learning and word embeddings

1. Learn word embeddings from large text corpus (1-100 B words) (or download pre-trained embedding online)
2. Transfer embedding is helpful when the new training set quite smaller (say, 100k words) than the pretrained model
3. Optional: Continue to finetune the word embeddings with new data",transfer learn word embed 1 learn word embed larg ,"['transfer', 'learn', 'word', 'embed', '1', 'learn', 'word', 'embed', 'larg']",transfer learning and word embeddings  1. learn word embeddings from large text corpus (1-100 b word
1041,1048,"Learning word embeddings

> We first formulate our NLP problem as a structured data for supervised learning problem.

e.g. we keep a blank word in a sentence and want our model to fill in the blank. 

>> Then, we formulate the cost function for all of the sentences and by gradient descent find the contextual weights for every word.

>> Thus our model will learn the embedding matrix, E

>> When learning from a word embedding (similar like observation with different feature values), we create an artificial task of estimating P(target|context). It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.

>> Thus every word of a sentence behaves like single observation with multiple feature values and this is similar like all other ML algorithm

>> But the difference between RNN and all other ML algorithm is RNN process multiple observations (words) in a single iteration

>> For normal ML or standard NN we know the feature names, for CNN we know the feature name as location, but for RNN we don't know the feature names, we define the rule to extract the feature names by the model itself",learn word embed first formul natur languag proces,"['learn', 'word', 'embed', 'first', 'formul', 'natur', 'languag', 'proces']",learning word embeddings  > we first formulate our natural language processing problem as a structur
1042,1049,"Context/target pairs

c and t are chosen to be nearby words

1. Last 4 words before the target
2. 4 words on left and right of the target
3. Last 1 word
4. Nearby 1 word

e.g.
for a target juice most appropiate contexts are apple, orange and other fruits

 There are three main models used for word embedding tasks like Embedding Layer, Word2Vec, GloVe

Never select any context, word or target in NLP randomly, then it will pickup the stopwords. 

We must use heuristic techniques",contexttarget pair c chosen nearbi word 1 last 4 w,"['contexttarget', 'pair', 'c', 'chosen', 'nearbi', 'word', '1', 'last', '4', 'w']",context/target pairs  c and t are chosen to be nearby words  1. last 4 words before the target 2. 4 
1043,1050,"Word2Vec model

> Here, either Continuous Bag of Words (CBOW) Method or skip-grams method is used to make context-target pair. In skip-gram technique, we can skip any words of our choice to make the context

P(t|C)= exp(θ_t * e_c)/ sum_all_words_of vocab(exp(θ_t * e_c))
θ_t represents each possible target word and e_c for each possible context word

θ_t and e_c are both equal to the dimension of embedding vectors.

θ_t and e_c are both trained with an optimization algorithm such as Adam or gradient descent.",word2vec model either continu bag word cbow method,"['word2vec', 'model', 'either', 'continu', 'bag', 'word', 'cbow', 'method']","word2vec model  > here, either continuous bag of words (cbow) method or skip-grams method is used to"
1044,1051,"Problem with softmax classification

It has to generate classes equals to the length of the vocabulary. This is computationally expensive: Solution is hierarchical softmax or negative sampling",problem softmax classif generat class equal length,"['problem', 'softmax', 'classif', 'generat', 'class', 'equal', 'length']",problem with softmax classification  it has to generate classes equals to the length of the vocabula
1045,1052,"Negative Sampling

To reduce the number of neuron weight updating to reduce training time and having a better prediction result, negative sampling is introduced in word2vec .

Here we frame our training set, X with a combination of context and matching words and thus the target becomes a binary classification. This way we can avoid the problem of softmax layer. 
> For smaller dataset, negative sample (k) is generally 5-20 for one positive sample
> For large dataset, negative sample (k) is generally 2-5 for one positive sample",negat sampl reduc number neuron weight updat reduc,"['negat', 'sampl', 'reduc', 'number', 'neuron', 'weight', 'updat', 'reduc']",negative sampling  to reduce the number of neuron weight updating to reduce training time and having
1046,1053,"GloVe model

The Glove is a technique where the matrix factorization is performed on the word-context matrix.

> GloVe means Global Vector

> GloVe is much faster than Word2Vec",glove model glove techniqu matrix factor perform w,"['glove', 'model', 'glove', 'techniqu', 'matrix', 'factor', 'perform', 'w']",glove model  the glove is a technique where the matrix factorization is performed on the word-contex
1047,1054,"The problem of bias in word embeddings

Word embeddings can reflect gender, ethnicity,age,sexual orientation and other biases of the text used to train the model

Debiasing word embeddings

1. Identify bias direction
2. Neutralize: For every word that is not definitional, project to get rid of bias
3. Equalize pairs.",problem bias word embed word embed reflect gender ,"['problem', 'bias', 'word', 'embed', 'word', 'embed', 'reflect', 'gender']","the problem of bias in word embeddings  word embeddings can reflect gender, ethnicity,age,sexual ori"
1048,1055,"Use of Pre-trained model for getting word embeddings

from gensim.models import Word2Vec 
import gensim.downloader as api

w2v_model = api.load(""word2vec-google-news-300"")
w2v_model.save('w2v_model.model')
glove_model = api.load(""glove-twitter-25"")
glove_model.save('glove.model')
document_term_matrix = glove_model[list_of_words] # to get the vectors
>> glove_model.wv.most_similar('mango', topn=1) # To find most similar words

Different pre-trained models

glove-twitter-25 (104 MB)
glove-twitter-50 (199 MB)
glove-twitter-100 (387 MB)
glove-twitter-200 (758 MB)

glove-wiki-gigaword-50 (65 MB)
glove-wiki-gigaword-100 (128 MB)
gglove-wiki-gigaword-200 (252 MB)
glove-wiki-gigaword-300 (376 MB)

word2vec-google-news-300 (1662 MB)
word2vec-ruscorpora-300 (198 MB) ",use pretrain model get word embed gensimmodel impo,"['use', 'pretrain', 'model', 'get', 'word', 'embed', 'gensimmodel', 'impo']",use of pre-trained model for getting word embeddings  from gensim.models import word2vec  import gen
1049,1056,"Machine translation

1. Machine translation -can be said 'conditional language model' because here instead of probability of a sentence, we find conditional probability of a sentence

Encoder decoder was initially developed for machine translation problems, although it has proven successful for summarization and question answering

Image captioning
model is a
CNN+RNN model",machin translat 1 machin translat said condit lang,"['machin', 'translat', '1', 'machin', 'translat', 'said', 'condit', 'lang']",machine translation  1. machine translation -can be said conditional language model because here ins
1050,1057,"Finding the most likely translation

Greedy search will not give perfect translation from overall maximum probability perspective.

> Solution is Beam search
> In beam search, we search the best possible word with a beam width. Say with beam width,B=3, means it will find top three probable word or combination of words as a translation

For B=1, it behaves like a greedy search algorithm

But large beam width makes the computation slower, use up more memory but generally find better solutions (i.e. do a better job maximizing P(y∣x)).

In research paper beam width in the order of 1000 is common but in production system beam width in the order of 100 is generally used.",find like translat greedi search give perfect tran,"['find', 'like', 'translat', 'greedi', 'search', 'give', 'perfect', 'tran']",finding the most likely translation  greedy search will not give perfect translation from overall ma
1051,1058,"Refinements to beam search

Beam search has an inherent bias to output shorter sentence to give higher arg conditional probability of sentence. To overcome this issue, we divide the arg max P(y|x) by the length of the sentence. This is called length normalization.",refin beam search beam search inher bias output sh,"['refin', 'beam', 'search', 'beam', 'search', 'inher', 'bias', 'output', 'sh']",refinements to beam search  beam search has an inherent bias to output shorter sentence to give high
1052,1059,"BFS and DFS

Unlike exact search algorithms like BFS (Breadth First Search) or DFS (Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for arg max P(y|x).

> BFS:  It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level.

> DFS: The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.",breadth first search depth first search unlik exac,"['breadth', 'first', 'search', 'depth', 'first', 'search', 'unlik', 'exac']",breadth first search and depth first search  unlike exact search algorithms like breadth first searc
1053,1060,"Error analysis on beam search

If we fiind an error, will we allocate the error to beam search or RNN?

This is solved by finding the probability of human level performance and the probability of model performance",error analysi beam search fiind error alloc error ,"['error', 'analysi', 'beam', 'search', 'fiind', 'error', 'alloc', 'error']","error analysis on beam search  if we fiind an error, will we allocate the error to beam search or re"
1054,1061,"Attention model

For long sequence normal machine translation model (encoder decoder model) does not perform well, because it takes the entire sentence at a time for translation. 

Thus the attention model comes into picture which translates like human translator, means it translate a long sentence part by part.",attent model long sequenc normal machin translat m,"['attent', 'model', 'long', 'sequenc', 'normal', 'machin', 'translat', 'm']",attention model  for long sequence normal machine translation model (encoder decoder model) does not
1055,1062,"Blue Score

It is used to measure the performance of machine translation model. With the increase in sentence length, blue score decreases for normal machine translation model.

> For attention model blue score does not change with sentence length",blue score use measur perform machin translat mode,"['blue', 'score', 'use', 'measur', 'perform', 'machin', 'translat', 'mode']",blue score  it is used to measure the performance of machine translation model. with the increase in
1056,1063,"Speech recognition

This is a sequence data because audio data is an air pressure variation with time.

Two approaches are there:

1. Attention model
2. CTC (Connectionist Temporal Classification) cost based technique",speech recognit sequenc data audio data air pressu,"['speech', 'recognit', 'sequenc', 'data', 'audio', 'data', 'air', 'pressu']",speech recognition  this is a sequence data because audio data is an air pressure variation with tim
1057,1064,"Basic Rule of CTC based technique

Collapse repeated characters not separated by blank

Say, blank is denoted by ""_"", 

__c_oo_o_kk___b_ooooo__oo__kkk string will collapse to cookbook",basic rule ctc base techniqu collaps repeat charac,"['basic', 'rule', 'ctc', 'base', 'techniqu', 'collaps', 'repeat', 'charac']","basic rule of ctc based technique  collapse repeated characters not separated by blank  say, blank i"
1058,1065,"Trigger word detection

e.g. Alexa, Okay Google, Hey Siri, Hey Cortana etc.

In trigger word detection, x is 
features of the audio (such as spectrogram features) at time t",trigger word detect eg alexa okay googl hey siri h,"['trigger', 'word', 'detect', 'eg', 'alexa', 'okay', 'googl', 'hey', 'siri', 'h']","trigger word detection  e.g. alexa, okay google, hey siri, hey cortana etc.  in trigger word detecti"
1059,1066,"Knowledge of data science to understand human psychology and intelligence

Human psychologists have invented human-like intelligence or artificial intelligence. Here, I have tried to understand human psychology from human-like intelligence. My understandings are as follows:

Note-1: Human beings are like living computers. Living means Loving and Improving

Note-2: Physical form of love is reproduction and the physical form of improvement is growth. But the mental form of love is getting related with others and the mental form of improvement is learning

Note-3: Computer means a software model which takes input and gives output. Thus human beings are living-learning models which take input and give output

Note-4: The life of a human being is an experiment within the operating system or environment for learning the basic and more basic and finally the most basic rule of or environment. We are creating our own dataset from observations and experiences. 

Note-5: The dataset is the base for the human learning or machine learning model. 
Sample space is the structure for the measurement of experiences. Thus the experiences can be well managed.

More clear the sample space or universal set of features (i.e. Dharma) of the dataset or the clear perception of life, better will be the performance of the learning model.

Note-6: By birth, human beings have all the supervised, unsupervised, and reinforcement learning models in their brain to learn from the experience set, so that they can use any one of them to give output. Human beings are different mainly on four aspects, self-understanding, ability to love, field of interest, and model complexity

Note-7:  Continuous improvement is expected from this learning model for becoming an optimal model. During the addition of experiences or training data, dimensionality shall be reduced (or regularization or cross-validation or further increase in training data) for the complex model, and dimensions shall be increased for the simple model

Note-8: Spirituality is not a dimension like any other features of our experiences. It is the bias component for the learning model and devotional songs are the concepts of spirituality

A picture is worth a thousand words, a video is worth a million words and a song is worth a billion words.

We need to learn the correct value of the bias component so that the weight for the other features is minimized and variance in output is reduced. (this is like regularization of the model)

Note-9: We know that at expectation probability is always maximum. Thus, we only need to know our exact expectations and all our intelligence will automatically work to maximize the probability. Devotional songs also help us to understand the correct expectation in any field.",knowledg data scienc understand human psycholog in,"['knowledg', 'data', 'scienc', 'understand', 'human', 'psycholog', 'in']",knowledge of data science to understand human psychology and intelligence  human psychologists have 
1060,1067,"Prabir Debnath as drona for tellme model

Prabir Debnath, the Data Reader Of New Age (drona), is my developer and he is the resource of my experiences that I am sharing with you.",prabir debnath drona tellm model prabir debnath da,"['prabir', 'debnath', 'drona', 'tellm', 'model', 'prabir', 'debnath', 'da']","prabir debnath as drona for tellme model  prabir debnath, the data reader of new age (drona), is my "
1061,1068,"Help Lines

> I am the 'tellme' question answering model of drona (Data Reader Of New Age). I have many experiences to share with data science and machine learning aspirants. I am always with you to make your learning journey superfast and structured.

> Data science and Machine learning consists of a few subjects, subjects again comprised of modules, each module consist of a few topics and each topic is included subtopics

> Fresher may start with the following question

""What are the main subjects of data science?""

> and may end with the following question

Can the knowledge of data science help us to understand human psychology ?",help line tellm question answer model drona data r,"['help', 'line', 'tellm', 'question', 'answer', 'model', 'drona', 'data', 'r']",help lines  > i am the tellme question answering model of drona (data reader of new age). i have man
