,documents,documents_processed,document_tokens,first_100_letters
0,"python tokens, Keywords, Identifiers, Literals, Punctuations, Operators 

Like any other computer programming language, python tokens (smallest meaningful part of any statement, expression or command) are:

1. Keywords (basic words of any language)

2. Identifiers (programmer defined words of any program)

3. Literals (data or data structure)

4. Punctuations (, : etc)

5. Operators (+-*/)",python token keyword identifi liter punctuat oper,"['python', 'token', 'keyword', 'identifi', 'liter', 'punctuat', 'oper']","python tokens, Keywords, Identifiers, Literals, Punctuations, Operators "
1,"Python keywords

There are 32 keywords in C++ and 35 keywords in python. 

Python keywords are:

(False, await, else, import, pass,
None, break, except, in, raise,
True, class, finally, is, return,
and, continue, for, lambda, try,
as, def, from, nonlocal, while,
assert, del, global, not, with,
async, elif, if, or, yield)
",python keyword,"['python', 'keyword']",Python keywords
2,"python data types (Numeric, Boolean, String, Datetime)

Numeric data types: Integers or whole numbers (1,2,-5,1000), Floats or real numbers (1.2,-0.5,2e2 or 2E2) and Complex numbers

Boolean Variable (true or false): comparison Operators (==, !=, >, <, >=, <=) returns a boolean value

String: ""mango"", ""mango is very tasty"" etc.

> 2e2 (called Scientific notation) means 2*10^2",python data type numer boolean string datetim,"['python', 'data', 'type', 'numer', 'boolean', 'string', 'datetim']","python data types (Numeric, Boolean, String, Datetime)"
3,"Arithmatic Operation

(addition[+], subtraction[-], multiplication[*], division[/], floor division[//], exponentiation[**], remainder modulus[%])

12%4 is 0, 13%4 is 1, 1%2 is 1, 4%5 is 4, 13%0O7 is 6

a += 2 means first add 2 with ""a"" and then save it again in ""a""

same instruction is a = a+2

a /=2 means first divide ""a"" by 2 and then save it again in ""a""

same instruction is a = a/2",arithmat oper,"['arithmat', 'oper']",Arithmatic Operation
4,"floor function

In mathematics and computer science, the floor function is the function that takes as input a real number x, and gives as output the greatest integer less than or equal to x, denoted floor or ⌊x⌋.",floor function,"['floor', 'function']",floor function
5,"ceiling function

The ceiling function maps x to the least integer greater than or equal to x, denoted ceil or ⌈x⌉",ceil function,"['ceil', 'function']",ceiling function
6,"Understanding Variable

Variable is the storing place in computer memory for a data (Integers, Floats, Booleans, string, datetime) or data structure (list, tuple, set, dictionary,np.array, pd.DataFrame etc.) and the data/data structure can be changed or varied

Variable name (must start with letter or underscore)

Variable name for data=x,y,z etc.

Variable name for data structure=age_of_students, _students_age etc.

case_sensitive (best naming practice), CASE_SENSITIVE, and Case_Sensitive are each a different variable",understand variabl,"['understand', 'variabl']",Understanding Variable
7,"commenting multiple lines in colab

Ctrl + /",comment multipl line colab,"['comment', 'multipl', 'line', 'colab']",commenting multiple lines in colab
8,"Perfect number

Perfect number is a positive integer that is equal to the sum of its proper divisors. The smallest perfect number is 6, which is the sum of 1, 2, and 3. ",perfect number,"['perfect', 'number']",Perfect number
9,"To show a float upto two decimal places

print(f'Bananas in the bag is {percentage_bananas:.2f} %')

round(any_float,2)

For Numpy

np.around(my_array,2)",show float upto two decim place,"['show', 'float', 'upto', 'two', 'decim', 'place']",To show a float upto two decimal places
10,"To get the absolute value

abs(5.8-7.8) returns 2

",get absolut valu,"['get', 'absolut', 'valu']",To get the absolute value
11,"The del keyword in python is primarily used to delete objects in Python

del variable_1, variable_2",del keyword python primarili use delet object python,"['del', 'keyword', 'python', 'primarili', 'use', 'delet', 'object', 'python']",The del keyword in python is primarily used to delete objects in Python
12,"order of precedence in python

i) Parentheses

ii) Exponential

iii) Multiplication

iv) Division

v) Addition

vi) Subtraction",order preced python,"['order', 'preced', 'python']",order of precedence in python
13,"An application or app or software or model is nothing but a soft machine (a machinery products).

Every machine has minimum one function which takes some inputs and gives some outputs.

Products are mainly of two types: machinery products and non-machinery products",applic app softwar model noth soft machin machineri product,"['applic', 'app', 'softwar', 'model', 'noth', 'soft', 'machin', 'machineri', 'product']",An application or app or software or model is nothing but a soft machine (a machinery products).
14,"Four basic answers of any question

yes, no, may be and don't know",four basic answer question,"['four', 'basic', 'answer', 'question']",Four basic answers of any question
15,"
logical resoning

Statement: Is buying things on installments profitable to the customer?

Arguments:

I. Yes. He has to pay less.

II. No, paying instalments upsets the family budget.

1. Only I is strong

2. Only II is strong

3. Bother I & II are strong

4. Either I or II is strong (when both are strong but opposite)

5. Neither I nor II is strong",logic reson,"['logic', 'reson']","
logical resoning"
16,"pprint module

The pprint module provides a capability to “pretty-print” arbitrary Python data structures in a well-formatted and more readable way! ",pprint modul,"['pprint', 'modul']",pprint module
17,"Understanding String

String is a word, a phrase, a sentence, a paragraph or an entire encyclopedia

> It has sequence and indexing

> string indexing begins from 0

> 'str' object does not support item assignment means string is immutable

'string' or ""string"" 

double quotes are used when there is any 's in the string

> A string can be called a safe bridge if it has no gaps in it i.e, no spaces.",understand string,"['understand', 'string']",Understanding String
18,"to print a new line

print('Use \n to print a new line')",print new line,"['print', 'new', 'line']",to print a new line
19,"for color print

from termcolor import colored

print(colored('Hello', 'green', attrs=['bold']))",color print,"['color', 'print']",for color print
20,"Grabbing the element by index 

print(string_name[3]), 

print(string_name[-2]) 

means (length-2)th index",grab element index,"['grab', 'element', 'index']",Grabbing the element by index 
21,"Basics of String Slicing

[starting_index : ending_index],  element located at the ending_index is not included

string[0:13] or string[:13] are same

string[2:18] or string[2:] are same when 18 is the last index

string[:]  If we do not specify the starting and the ending index, it will extract all elements of the string

Reversing a string in python

string_name[::-1]

slicing with step size, string[3:14:2] means forward slicing with step size 2, string[14:3:-2] means backward slicing with step size 2",basic string slice,"['basic', 'string', 'slice']",Basics of String Slicing
22,"concatenate strings

print(string1 + string2 )

print(""D"", end = ' ')

print(""C"", end = ' ')

will return D C
",concaten string,"['concaten', 'string']",concatenate strings
23,"String functions

print(), type(), len()

ord('z') -ord('a') returns an integer

ord() method converts a character into its Unicode code.",string function,"['string', 'function']",String functions
24,"String methods 

string.lower(), string.upper(), string.count('n'),
string.index('n'), string.find('n'), string.replace('n','L'), string.split(' '), string.split()-default spliting by space,
string.join([""g"", ""h"", ""o""])",string method,"['string', 'method']",String methods 
25,"way to embed expressions inside string literals

first_name = 'Rahul'

last_name = 'Modi'

full_name = f'Left plus right makes {first_name}  {last_name}'  

f-strings provide a way to embed expressions inside string literals",way emb express insid string liter,"['way', 'emb', 'express', 'insid', 'string', 'liter']",way to embed expressions inside string literals
26,"String duplication

String duplication occurs when we multiply string by a number, 

my_name='prabir'

print(my_name*3)",string duplic,"['string', 'duplic']",String duplication
27,"Taking user's input

inputted_number = int(input())
 
or

marks = int(input(""Enter our marks: ""))",take user input,"['take', 'user', 'input']",Taking user's input
28,"String matching

import re
 pattern = re.compile(""%s"" % common_term)

      topics=[x for x in list_of_topics if pattern.match(x)]",string match,"['string', 'match']",String matching
29,"Understanding List

List is a data structure to store multiple items in a single variable. It has sequence, indexing and is mutable.

['A string',23,100.232,'o',True]

> Grabbing the element by index is same as string

> List Slicing is same as string slicing

> List concatenation is same as string concatenation 

> Duplication method similar to strings

my_string=[0]*26

> List mutability

my_list= ['spam', 'egg', 'bacon', 'tomato', 'ham', 'lobster']

my_list[2] = 10

print(my_list)

> Lists are like arrays but lists are more flexible means they have no size constraint or type constraint",understand list,"['understand', 'list']",Understanding List
30,"List functions

len(), min(), max(), sum(), sorted(), sorted(my_list,reverse=True) for sorting in reverse order, zip() function  returns a list of n-paired tuples",list function,"['list', 'function']",List functions
31,"List methods

 .append(one_element), .extend(list_of_elements), .pop()  by default removes last index, .remove() means remove by element, .count(), .index(),  .sort(), .reverse()",list method,"['list', 'method']",List methods
32,"Nested List

[lst_1,lst_2,lst_3]

Entire table of data can be stored in nested list variable",nest list,"['nest', 'list']",Nested List
33,"Range function to get a sequence list of numbers. 

for k in range(10):

list(range(10)) or list(range(0,10)) or list(range(0,10,1))

range(0) does not give any output

floats are not allowed in range function",rang function get sequenc list number,"['rang', 'function', 'get', 'sequenc', 'list', 'number']",Range function to get a sequence list of numbers. 
34,"Main Subjects of Data Science Stream

> Coding

> Data Structures

> Mathematics

> Artificial Intelligence

> Business Intelligence

> Computer Engineering",main subject data scienc stream,"['main', 'subject', 'data', 'scienc', 'stream']",Main Subjects of Data Science Stream
35,"Main Modules of Coding

> Python",main modul code,"['main', 'modul', 'code']",Main Modules of Coding
36,"Main Modules of Data Structures

> Analytics Framework",main modul data structur,"['main', 'modul', 'data', 'structur']",Main Modules of Data Structures
37,"Main Modules of Mathematics

> Mathematics",main modul mathemat,"['main', 'modul', 'mathemat']",Main Modules of Mathematics
38,"Main Modules of Artificial Intelligence

> Machine Learning

> Deep Learning",main modul artifici intellig,"['main', 'modul', 'artifici', 'intellig']",Main Modules of Artificial Intelligence
39,"Main Modules of Business Intelligence

> Industry Insights",main modul busi intellig,"['main', 'modul', 'busi', 'intellig']",Main Modules of Business Intelligence
40,"Main Modules of Computer Engineering

> Data Engineering",main modul comput engin,"['main', 'modul', 'comput', 'engin']",Main Modules of Computer Engineering
41,"Main Topics of Python

> Integers, Floats and Booleans

> Strings

> Lists

> Tuples, Sets and Dictionaries

> Statements, Indentation and Conditionals

> Loops and Iterations

> List comprehension

> Functions and Methods

> Production Grade Programming

> Competitive Coding

> General Knowledge for a Data Scientist

> Numpy

> Pandas

> Data Wrangling

> Data Visualization",main topic python,"['main', 'topic', 'python']",Main Topics of Python
42,"Main Topics of Analytics Framework

> Excel

> Tableau

> Business KPI

> SQL ",main topic analyt framework,"['main', 'topic', 'analyt', 'framework']",Main Topics of Analytics Framework
43,"Main Topics of Mathematics

> Overview of mathematics

> Calculus

> Vector Algebra

> Matrix Algebra

> Probability Theory

> Summarizing Data

> Random Variables

> Discrete Distributions

> Continuous Distributions

> Joint Distributions

> Sampling & Statistical Inference

> Confidence Intervals

> Hypothesis Testing",main topic mathemat,"['main', 'topic', 'mathemat']",Main Topics of Mathematics
44,"Main Topics of Machine Learning

> Computer science and machine learning

> General Modeling Techniques

> Linear Regression

> Bias variance tradeoff

> Regularized Linear Regression

> Cross validation and hyperparameter tuning

> Logistic regression

> Decision Trees

> Ensembles of decision trees

> Model Explainability

> k-nearest neighbors

> Naive Bayes Classifier

> Support Vector Machines

> K-means clustering

> Hierarchical clustering

> Principal Component Analysis

> Anomaly Detection

> Natural Language Processing 

> Topic modeling

> Recommender Systems

> Time Series Analysis",main topic machin learn,"['main', 'topic', 'machin', 'learn']",Main Topics of Machine Learning
45,"Main Topics of Deep Learning

> Neural Networks

> Deep Neural Networks

> Improving Deep Neural Networks

> Structuring ML Projects

> Convolutional Neural Networks

> Recurrent Neural Network",main topic deep learn,"['main', 'topic', 'deep', 'learn']",Main Topics of Deep Learning
46,"Main Topics of Industry Insights

> Guesstimate

> Case Study-ML in Heathcare

> Case Study-ML in Fraud risk analytics

> Case Study-ML in Credit Risk

> Case Study-ML in E-commerce",main topic industri insight,"['main', 'topic', 'industri', 'insight']",Main Topics of Industry Insights
47,"Main Topics of Data Engineering

> Linux basics and terminal commands

> Python modules and project setup

> Version control-Git

> API basics with flask

>  FastAPI

> Docker

> Microservices and streamlit

> ML Lifecycle

> Cloud Computing

> MLOps-MLFlow

> PySpark

> Airflow",main topic data engin,"['main', 'topic', 'data', 'engin']",Main Topics of Data Engineering
48,"Subtopics of Integers, Floats and Booleans

> python tokens, Keywords, Identifiers, Literals, Punctuations, Operators

> Python keywords

> python data types

> Arithmatic Operation 

> floor function

> ceiling function

> Understanding Variable 

> order of precedence in python",subtop integ float boolean,"['subtop', 'integ', 'float', 'boolean']","Subtopics of Integers, Floats and Booleans"
49,"Understanding Tuples Basics

Tuples, Sets and Dictionaries are data structures to store multiple items in a single variable

Tuples are similar to list but immutable

('A string',23,100.232,'o',True) or 'A string',23,100.232,'o',True

> Tuple indexing same as list indexing

> Tuple slicing same as list slicing

> Tuple functions are same as list

> Tuples methods are less than list, .index(), .count()

> Tuples are used rarely only when immutability is a must",understand tupl basic,"['understand', 'tupl', 'basic']",Understanding Tuples Basics
50,"Set basics

Sets are an unordered collection of unique elements

set() function creates an emty set. {1,6,4,'abc'} is a non empty set. Sets are mutable like list

A set cannot have a mutable item like list within.

We can not convert a set into list

my_set_a = {1,6,4,'abc'}

my_set_b = {'abc'}

(my_set_a - my_set_a) is not equal to (my_set_b - my_set_a)",set basic,"['set', 'basic']",Set basics
51,"Set methods

.add()

 .update([2,3,4]) helps to add multiple elements to a set

.remove(element)
 
.union(another list, tuple or set)

 .intersection(another list, tuple or set)

set.intersection(set1,set2,set3)
set.intersection(*set_list)

 .difference(another list, tuple or set)

.symmetric_difference(another set) returns excluding intersection",set method,"['set', 'method']",Set methods
52,"Dictionary Basics

Dictionaries are hash tables or hash maps that can map keys to values

{} makes an emty dictionary. 

Non emty dictionary {key1:value1,key2:value2,key3:value3}

We can call values by their key, 

my_dict[key_name] or can use .get(key_name) method. 

We can also call an element of any data structure values and can apply method on that element.",dictionari basic,"['dictionari', 'basic']",Dictionary Basics
53,"Dictionary methods

.keys(), .values(), .items(), .pop(key) 

.items() returns a list of tuples with key and values",dictionari method,"['dictionari', 'method']",Dictionary methods
54,"Dictionary operations

We can add a new key-value pair

 my_dict['Design'] ='Sr Data Scientist' 

We can also do this by using .update({'Design':'Sr Data Scientist'}) method. 

.update() method can also be used to update exsisting key-values

We can delete a key-value pair

del my_dict[key]

dict() function converts list of paired elements into dictionary",dictionari oper,"['dictionari', 'oper']",Dictionary operations
55,"Subtopics of Strings

> Understanding String

> Grabbing the element by index 

> Basics of String Slicing

> concatenate strings

> String functions

> String methods 

>  way to embed expressions inside string literals

> String duplication",subtop string,"['subtop', 'string']",Subtopics of Strings
56,"Subtopics of  Lists

> Understanding List

> List functions

> List methods

> Nested List",subtop list,"['subtop', 'list']",Subtopics of  Lists
57,"Subtopics of Tuples, Sets and Dictionaries

> Understanding Tuples basics

> Set basics

> Set methods

> Dictionary Basics

> Dictionary methods

> Dictionary operations",subtop tupl set dictionari,"['subtop', 'tupl', 'set', 'dictionari']","Subtopics of Tuples, Sets and Dictionaries"
58,"Subtopics of Statements, Indentation and Conditionals

> Assignment statement, conditional statement

> Understanding Expression

> Multi-line statements

> Understanding Comments

> Auto indentation",subtop statement indent condit,"['subtop', 'statement', 'indent', 'condit']","Subtopics of Statements, Indentation and Conditionals"
59,"Subtopics of Loops and Iterations

> Understanding Loops

> Understanding the main loops of python

> Enumerate function

> break, continue, and pass statements 

> Calculating program execution time",subtop loop iter,"['subtop', 'loop', 'iter']",Subtopics of Loops and Iterations
60,"Subtopics of List comprehension

> Understanding List comprehension

> Set Comprehension 

> Dictionary comprehension",subtop list comprehens,"['subtop', 'list', 'comprehens']",Subtopics of List comprehension
61,"Subtopics of Functions and Methods

> Understanding Functions, Model and  Environment 

> Defining a function 

> Functions details 

> Function and method 

> Global variable",subtop function method,"['subtop', 'function', 'method']",Subtopics of Functions and Methods
62,"Subtopics of Production Grade Programming

> Understanding Production Grade Programming

> Object Oriented Programming

>  Attribute and Object

> Defining a class

> Understanding Polymorphism

> Exception handling",subtop product grade program,"['subtop', 'product', 'grade', 'program']",Subtopics of Production Grade Programming
63,"Subtopics of Competitive Coding

> Understanding Competitive coding",subtop competit code,"['subtop', 'competit', 'code']",Subtopics of Competitive Coding
64,"Assignment statement, conditional statement

Instructions that a Python interpreter can execute are called statements. A statement may or may not return a value. 

Assignment statement (assign some data or data structure to a variable) & conditional statement (if, elif (used for nested if), else, while, for and import)",assign statement condit statement,"['assign', 'statement', 'condit', 'statement']","Assignment statement, conditional statement"
65,"Understanding Expression

Expression needs to be executed and evaluated and returns a value. For example 2+3 returns 5 in python.  All functions and methods in python returns a value and hence they are expressions

Thus we can conclude that all expressions are statements but all statements are not expressions",understand express,"['understand', 'express']",Understanding Expression
66,"Multi-line statements 

line continuation by () or [] or {}. Large string can be broken in multi line by \

a=(5+
   6)

We can also put multiple statements in a single line using semicolons. a = 1 ; b = 2 ; c = 3 or a,b,c = 1,2,3",multilin statement,"['multilin', 'statement']",Multi-line statements 
67,"Understanding Comments

Comments (single line or multi line) are for programmers to better understand a program. Do comments by #. Another way of doing this is to use triple quotes, either ''' or """"""",understand comment,"['understand', 'comment']",Understanding Comments
68,"Auto indentation

Auto indentation is done with (: + enter) or manually with tab to maintain is proper structure of the code",auto indent,"['auto', 'indent']",Auto indentation
69,"""truthy"" and ""falsy""

We use ""truthy"" and ""falsy"" to differentiate from the boolean values True and False.",truthi falsi,"['truthi', 'falsi']","""truthy"" and ""falsy"""
70,"Subtopics of Numpy

> Understanding Library

> Understanding Package

> Understanding Module

> Modular programming

> N-dimensional array

> Creating a two-dimensional array

> attributes of numpy array

> Advantages of numpy array

> Creating ndarray

> Defining Array size

> Conversion of list and tuple to ndarray

> Indexing and Slicing of ndarray

> Grabbing element by index of 2D array

> Array Manipulation

> Important functions in numpy

> Difference between view and copy in numpy

> Basic Operations & Functions in numpy

> Difference between list and numpy array

> Exponentiation of vectors

> Normalizing rows

> Writing softmax function with numpy",subtop numpi,"['subtop', 'numpi']",Subtopics of Numpy
71,"Subtopics of Pandas

> Basics of Pandas Dataframe

> Rows and columns of pandas dataframe

> Pandas series

> Convertion of numpy array to pandas dataframe

> Methods and attributes of pandas DataFrame 

> Connecting google drive to colab notebook

> For uploading any file from local computer to the colab session

> Converting a csv file to pandas DataFrame

> Converting a csv file to pandas DataFrame

> Writing data in a csv file

> Appending new row to an exsisting csv file

> Editing a csv file

> Excel file with multiple worksheets

> Writing in a new excel file  

> Editing in pandas df and then save them to csv/excel

> Slicing operation on Pandas DataFrame

> Slicing operation on a particular column  in Pandas DataFrame

> Conditional Slicing or Conditional Filtering in  Pandas DataFrame

> Adding new column in existing df

> Removing one or multiple columns 

> Removing one or multiple rows

> Setting a column as row index

> Methods for particular column

> Sort values in  Pandas DataFrame

> Creating a new column with lambda function

> Difference among and & |

> Pandas data display options

> Another way to convert dict to df

> Converting pandas df to numpy array

> String to numeric in  Pandas DataFrame

> Numpy array or pandas df to matrix convertion",subtop panda,"['subtop', 'panda']",Subtopics of Pandas
72,"Subtopics of Data Wrangling

> Basics of Data wrangling

> Concatenating pandas DataFrame

> DataFrame merging operation through joins

> Groupby operation on pandas dataframe for data analysis

> Detailed EDA

> Quick EDA

> EDA practice

> Use of ast library

> Use of explode function",subtop data wrangl,"['subtop', 'data', 'wrangl']",Subtopics of Data Wrangling
73,"Subtopics of Data Visualization

> libraries for data visualization

> Matplotlib library

> Matplotlib pyplot function

> Matplotlib Line Plot

> Matplotlib Horizontal Bar Plot

> Matplotlib Box plot and Scatter Plot

> drawing a trend line in the scatter plot

> drawing horizontal or vertial lines

> Seaborn library

> seaborn Line plot, Scatter plot, Distribution/Density Plot

> seaborn Joint Distribution Plot, Heatmap, Bar Plot

> seaborn Histogram, Factor Plot, Box plot

> Seaborn Pairplot

> Ploting directly from pandas dataframe

> Editing the plot area with matplotlib and seaborn

> Combining two plots in a single graph

> saving the plot as png in google drive",subtop data visual,"['subtop', 'data', 'visual']",Subtopics of Data Visualization
74,"Subtopics of Excel

> Excel basics

> Data Handling in excel

> sections in excel file (top to bottom)

> Menu Bar Tabs in excel

> Understanding circular reference

> Data types in excel

> To view the groupby statistics like pandas df in excel

> Filter in excel

> Conditional Formatting in excel

> Sorting in excel

> Removing Duplicates in excel

> Pivot and Slicers in excel

> Refreshing all pivot tables

> Charts in excel

> Dashboard in excel

> Waterfall or birdge graph analysis

> Excel functions

> Concatenation of text in excel",subtop excel,"['subtop', 'excel']",Subtopics of Excel
75,"Subtopics of Tableau

> Tableau basics

> Best practices for visualization

> Panes of Tableau

> Trend line models in Tableau

> Creating Calculated field in Tableau

> Chart area of Tableau

> Chart types in Tableau

> Tableau Pills

>  Tableau Desktop applications

> Components of a Dashboard

> COUNTD function

> Reference line and Reference band

> File extensions in Tableau

> Filters in Tableau

> Data blending

> Data Types in Tableau",subtop tableau,"['subtop', 'tableau']",Subtopics of Tableau
76,"Subtopics of Business KPI

> KPI Basics

> KPI vs Metric

> Need of KPI for the company

> Types of Indicators

> Effectiveness and Efficiency

> Ways to develop KPI

> Three steps to a stronger KPI strategy

> Key Performance Indicators in practice

> Executive Dashboard

> Possible dangers of industrial performance indicators ",subtop busi kpi,"['subtop', 'busi', 'kpi']",Subtopics of Business KPI
77,"Subtopics of SQL

> SQL Basics

> List of Relational database

> Difference between SQL and Python

> Basics of Relational Database (RDBMS)

> Basics of Non-Relational Database

> Parts of SQL

> SQL Statements

> SQLite

> Practicing sql queries

> Reading SQLite Database file as pandas df

> SQL Query/ Statements/ commands

> SELECT and SELECT DISTINT statements

> Conditional operators

> File extensions

> LIKE and ILIKE

> ADVANCED SQL COMMANDS

> TIMESTAMP & EXTRACT

> SUB-QUERY 

> SELF JOIN

> DATA/ DATA STRUCTURE TYPES IN SQL

> Primary Key and Foreign Key

> CONSTRAINTS IN SQL

> CREATING DATABASES & TABLES

> CONDITIONAL EXPRESSIONS & PROCEDURES

> Simple View and Complex View

> Syntax for the CREATE VIEW

> SQL queries in Python

> SQL aliases

> Different Types of SQL JOINs

> CREATE DATABASE, DROP DATABASE and BACKUP DATABASE

> CREATE TABLE in SQL

> DELETE in SQL

> SQL TRUNCATE TABLE

> SQL ALTER TABLE

> SQL string datatypes

> INSERT INTO in SQL

> UPDATE in SQL

> SQL CHECK

> UNION in SQL

> SQL TIMESTAMP and DATETIME

> SQL NOT IN

> SQL trigger

> Operation with Null values in SQL

> SQL CONCAT

> SQL and String

> Temporary table and Heap table

> Sequence in SQL

> Primary Key, Super Key and Candidate Key",subtop structur queri languag,"['subtop', 'structur', 'queri', 'languag']",Subtopics of SQL
78,"Subtopics of Calculus

> Radian measure

> Machine Learning Use Cases of Calculus

> Basics of derivative

> The chain rule

> Point of maxima and point of minima

> Partial Derivatives

> Ways to find the slope of f(x,y)

> Jacobian Matrix

> Derivatives Formulas

>  Definite Integrals

> Formulas for integration

> Basics of Limit

> Solution method of limit of indeterminate form",subtop calculus,"['subtop', 'calculus']",Subtopics of Calculus
79,"Subtopics of Vector Algebra

> Scaler, Vector, Matrix and Tensor

> Dimension and Direction

> position vector/ location vector/ radius vector

> Applications of Linear Algebra in Data Science

> Vector operations

> Vector Dot Product

> Vector Projection

> Vector Cross product

> Vector norms

> Representing multivariable linear equation in vector space

> Multivariable linear equations

> Visualization of vector",subtop vector algebra,"['subtop', 'vector', 'algebra']",Subtopics of Vector Algebra
80,"Subtopics of Matrix Algebra

> Types of Matrices

> Matrix operations

> Matrix multiplication

> Equation in matrix form

> Identity matrix

> Determinant of a matrix

> Transpose of a matrix

> Adjoint of a matrix

> cofactor matrix

> Inverse of a matrix

> Eigenvalues and Eigenvectors of a square matrix

> Use of eigen values and eigen vectors

> Nilpotent matrix ",subtop matrix algebra,"['subtop', 'matrix', 'algebra']",Subtopics of Matrix Algebra
81,"Subtopics of Probability Theory

> Importance of Probability theory

> Set Theory

> Experiment, sample space, observation and experience

> Intersection and Union of Sets

> Mutually exclusive sets

> Permutation & Combination

> Basic Probability

> Probability Axiom#1

> Probability Axiom#2

> Probability Axiom#3 or Special Addition Rule

> Addition Rule

> Conditional Probability

> Multiplication Rule

> Solving any set or probability problem

> Difference between mutually exclusive and independent events",subtop probabl theori,"['subtop', 'probabl', 'theori']",Subtopics of Probability Theory
82,"Understanding Loops

Loops help us to execute a block of code repeatedly

When a statement  repeatedly execute a single statement or group of statements as long as the condition is true, then it is called a 'loop'",understand loop,"['understand', 'loop']",Understanding Loops
83,"Understanding the main loops of python

for and while (similar to an if statement but continues to execute the code repeatedly as long as the condition is True)

A while loop in Python is used for 
indefinite type of iteration

for loops are used to loop through an iterable object (string, list, tuple, set and dict) and perform the same action on each element

for i in iterable_object:
last_element=i

Here, i is the element or key of the data_structure

for x, y in list_of_tuples:
   print(x,y) 

A word of caution however! It is possible to create an infinitely running loop with while statements",understand main loop python,"['understand', 'main', 'loop', 'python']",Understanding the main loops of python
84,"Iterate means utter repeatedly.

for house in got_houses_list[::]:
  print(f""House {house}""). 

Here we are assigning the variable house as the elements of sliced list",iter mean utter repeat,"['iter', 'mean', 'utter', 'repeat']",Iterate means utter repeatedly.
85,"Enumerate function 

enumerate() function adds a counter to an iterable(string, list, tuple, set and dict) and returns it in a form of enumerate object. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() method

l1 = [""eat"",""sleep"",""repeat""]

print (list(enumerate(l1)))

returns

[(0, 'eat'), (1, 'sleep'), (2, 'repeat')]",enumer function,"['enumer', 'function']",Enumerate function 
86,"break, continue, and pass statements in our loops add additional functionality

break: Breaks out of the current closest enclosing loop.
continue: Goes to the top of the closest enclosing loop.
pass: Does nothing at all.",break continu pass statement loop add addit function,"['break', 'continu', 'pass', 'statement', 'loop', 'add', 'addit', 'function']","break, continue, and pass statements in our loops add additional functionality"
87,"Calculating program execution time

import time

>> For “real-world time”

start = time.time() # at the starting of the code

end = time.time() # at the ending of the code

print(end - start)

>> For Relative Time (It has no defined relationship to real-world time). This is mainly used to evaluate relative performance of two versions of code

start = time.process_time() # at the starting of the code

end = time.process_time() # at the ending of the code

print(f""{1000 *(end - start)} ms"")
",calcul program execut time,"['calcul', 'program', 'execut', 'time']",Calculating program execution time
88,"Subtopics of Summarizing Data

> Basics of Summarizing Data

> Numerical, Categorical, Dichotomous, and Ordinal Data

> Measure of central tendency

> Understanding Mean

> Understanding Median

> Understanding Mode

> Measure of spread

> Understanding Range

> Understanding Variance

> Understanding Standard Deviation

> Understanding Interquartile Range

> Steps to Calculate IQR

> Outliers with respect to IQR

> Measures of Symmetry

> Understanding skewness

> Libraries for Summarizing Data

> Skewness and Kurtosis measurements",subtop summar data,"['subtop', 'summar', 'data']",Subtopics of Summarizing Data
89,"Subtopics of Random Variables

> Understanding random variable

> Python Code for Random Variables

> Random seed

> Types of Random Variables

> Continuous and Discrete Random variables using numpy array

> Discrete Random Variables in Probability Distribution

> Continuous Random Variables in Probability Distribution

> Mean of Random Variables

> Variance of Random Variables

> Point probability, cumulative probability

> Formulas for expectation and variance",subtop random variabl,"['subtop', 'random', 'variabl']",Subtopics of Random Variables
90,"Subtopics of Discrete Distributions

> Basics of Probability distribution

> Types of Discrete Probability Distribution

> Uniform Distribution-discrete

> Bernoulli Distribution

> Binomial Distribution

> Geometric Distribution

> Poisson Distribution

> Moments of probability distribution

> Moments in Physics",subtop discret distribut,"['subtop', 'discret', 'distribut']",Subtopics of Discrete Distributions
91,"Subtopics of Continuous Distributions

> Basics of continuous distribution

> Types of continuous probability distribution

> Uniform Distribution-continuous

> Normal or Gaussian Distribution

> Standard Normal Distribution

> Exponential Distribution

> Gamma Function

> Gamma Distribution

> Chi-square Distribution

>  t-Distribution

> F-Distribution

> log-normal distribution

> Exponential expressions

> Euler's number

> Python Code for Continuous Distributions

> Continuous distributions in ML

> Most frequent types of distribution for data scientist",subtop continu distribut,"['subtop', 'continu', 'distribut']",Subtopics of Continuous Distributions
92,"Subtopics of Joint Distributions

> Basics of Joint Distribution

> Two dimensional random vector

> Marginal Distribution and Conditional Distribution

> Independent Random Variables

> Probability mass function

> Degree of association

> Understanding of Covariance

> Understanding of Correlation",subtop joint distribut,"['subtop', 'joint', 'distribut']",Subtopics of Joint Distributions
93,"Subtopics of Sampling & Statistical Inference

> Basics of Sampling & Statistical Inference

> Random sample

> Understanding of Statistics

> Statistical Inference

> Sample Mean and Sample Variance

> Independent and indentically distributed random variables

> Sampling with Replacement

> Central Limit Theorem (CLT) or z-results or z-score

> t-Result or t-Score

> F-result or F-Score

> Point Estimation

> Mean and Expectation",subtop sampl statist infer,"['subtop', 'sampl', 'statist', 'infer']",Subtopics of Sampling & Statistical Inference
94,"Subtopics of Confidence Intervals

> Basics of Confidence interval

> Calculating population parameters for one sample

> Population proportion for binomial experiment

> Calculating population parameters for two samples from difference population

> Pooled variance

> Confidence limits

> Sampling schemes from best to worst",subtop confid interv,"['subtop', 'confid', 'interv']",Subtopics of Confidence Intervals
95,"Subtopics of Hypothesis Testing

> Basics of Hypothesis Testing

> Rare Event Rule for Inferential Statistics

> Components of a formal hypothesis test

> Null Hypothesis

> Alternative Hypothesis

> Identifying the null and alternative hypothesis

> Test Statistic

> Significance Level

> Critical Region

> Critical Value

> Two-tailed, Right-tailed, Left-tailed Tests

> P-value or probability value

> Conclusions in Hypothesis Testing based on P-value

> Type-I error

> Type-II error

> Power of a hypothesis test",subtop hypothesi test,"['subtop', 'hypothesi', 'test']",Subtopics of Hypothesis Testing
96,"Subtopics of Linear Regression

> Supervised, parametric, regression algorithm

> Basics of linear regression

> Error or residuals

> Loss function and cost function

> Types of loss function

> OLS method for finding out the model parameters

> Gradient Descent Fundamentals

> Assumptions of regression

> Multicollinearity issue

> Heteroscedasticity issue

> Properties of regression line

> Advantages of linear regression

> Limitations of linear regression

> Data preparation for linear regression

> Omission of relevant variable from a regression equation

> Visualizing Linear Regression

> Understanding of Feature scaling

> Difference bwteen Matrix and metric

> Metrics that help in evaluating our model’s accuracy

> Model Health using KS Scores

> Decision Making using Risk Bins

> Libraries for linear regression

> Importance of csv file

> Implementation Steps of Linear Regression

> transform and fit_transform

> Polynomial model",subtop linear regress,"['subtop', 'linear', 'regress']",Subtopics of Linear Regression
97,"Subtopics of Bias variance tradeoff

> Optimal Model

> Underfit Model

> Overfit Model

> Understanding Estimator

> Noise, Underfittiing, Overfitting and Overgeneralizing",subtop bias varianc tradeoff,"['subtop', 'bias', 'varianc', 'tradeoff']",Subtopics of Bias variance tradeoff
98,"Subtopics of Regularized Linear Regression

> Basics of Regularized Linear Regression

> Types of Regularization

> Ridge Regression (L2 Regularization)

> Lasso Regression (L1 Regularization)

> Libraries for Regularized Linear Regression

> Alpha value",subtop regular linear regress,"['subtop', 'regular', 'linear', 'regress']",Subtopics of Regularized Linear Regression
99,"Understanding list comprehension 

A list comprehension is a syntax for creating a list based on an existing list

For loop which returns a list/tuple can be written as list comprehension. Tuple function may be used to convert list comprehension into tuple.

[output_expression for  variable in input_sequence (string, list, tuple, set or dict) conditionals] 

[number**2 for number in list_of_numbers if number%2!=0]

[1 for act,pred in zipped_list if act==pred]",understand list comprehens,"['understand', 'list', 'comprehens']",Understanding list comprehension 
100,"Set Comprehension

Structure of Set Comprehension is same as list comprehension.",set comprehens,"['set', 'comprehens']",Set Comprehension
101,"Dictionary comprehension

Structure of Dictionary comprehension is same as list comprehension.",dictionari comprehens,"['dictionari', 'comprehens']",Dictionary comprehension
102,"Function range and domain

Range is defined as all the possible values which a function  f(x)  can take.

Domain is defined as all the possible values which  x  can take.",function rang domain,"['function', 'rang', 'domain']",Function range and domain
103,"Subtopics of Cross validation and hyperparameter tuning

>  Cross validation Basics

> Simple Validation vs Cross Validation

> k-fold CV

> Python coding for CV

> yellowbrick CVScores

> Fundamentals of hyperparameters

> Hyperparameters tuning

> Coding for Hyperparameters tuning",subtop cross valid hyperparamet tune,"['subtop', 'cross', 'valid', 'hyperparamet', 'tune']",Subtopics of Cross validation and hyperparameter tuning
104,"Subtopics of Logistic regression

> Basics of Logistic regression

> Meaning of odds and logit function in probability

> Logistic function or sigmoid function

> Working of parametric model

> Benefits of Parametric ML models

> Limitations of Parametric ML Models

> Classification model and probability

> Generative models  and Discriminative models

> Coding for Logistic regression

> Confusion Matrix

> Accuracy, Precision, Recall and F1-Score

> Importance of F1 Score

> Checking Cross-validation scores

> Receiver operating characteristic and AUC

> Sparse matrix and Dense matrix",subtop logist regress,"['subtop', 'logist', 'regress']",Subtopics of Logistic regression
105,"Subtopics of Decision Trees

> Important Terminology in Decision Tree ML Model

> Steps of Decision tree algorithm

> Methods to measure the similarity of child nodes

> Fitting Decision tree classifier

> Fitting Decision tree regressor

> Coding for Visualizing Decision Tree

> Advantages of Decision tree

> Disadvantages of Decision tree",subtop decis tree,"['subtop', 'decis', 'tree']",Subtopics of Decision Trees
106,"Subtopics of Ensembles of decision trees

> Basics of Ensembles of decision trees

> Ensemble techniques

> Bagging technique

> Boosting technique

> Extreme Gradient Boosting

> Stacking technique

> Python coding for Ensembles of decision trees

> Finding feature importance

> Classification report

> Ways to improve random forest accuracy",subtop ensembl decis tree,"['subtop', 'ensembl', 'decis', 'tree']",Subtopics of Ensembles of decision trees
107,"Subtopics of Model Explainability

> Basics of Model Explainability

> Black Box Model vs. White Box Model

> Explainable AI

>  Importance of explainability

>  Scope of explainability

>  Approach of explainability

> Techniques or Libraries for Explainability in ML

> Local Interpretable Model-Agnostic Explanations

> Shapley Additive Explanations

> Implementing SHAP

> Explain Like I'm 5

> Other techniques for Explainability in ML",subtop model explain,"['subtop', 'model', 'explain']",Subtopics of Model Explainability
108,"Subtopics of k-nearest neighbors

> Basics of KNN

> Euclidean Distance

> Working of kNN

> Ways to select the value of k in the kNN Algorithm

> Disadvantages of kNN Algorithm

> Python coding for KNN

> Knn for recommender system",subtop knearest neighbor,"['subtop', 'knearest', 'neighbor']",Subtopics of k-nearest neighbors
109,"Subtopics of Naive Bayes Classifier

> Basics of Naive Bayes

> Bayes theorem

> Understanding of Naive Bayes

> Text Pre-processing

> One hot encoding

> Python coding for Naive Bayes",subtop naiv bay classifi,"['subtop', 'naiv', 'bay', 'classifi']",Subtopics of Naive Bayes Classifier
110,"Subtopics of Support Vector Machines

> Basics of SVM

> Kernel function

> Hinge loss function, hypersurface and hyperplane

> Python coding for SVM

> Tuning the SVM

> Advantages of SVM

> Disadvantages of SVM",subtop support vector machin,"['subtop', 'support', 'vector', 'machin']",Subtopics of Support Vector Machines
111,"Subtopics of General Modeling Techniques

> Understanding Feature engineering

> Importance of Feature Engineering

> Basic EDA

> Understanding Outliers

> Conversion of Categorical column to numerical

> Multi Label columns to Binary

> Number-String to numerical value

> Variance Inflation Factor (VIF)

> Steps of ML modelling

> Understanding Warnings

> Data preparation or data preprocessing

> Linear Transformation or Scaling

> Non-linear Transformations

> Time Complexity and Space Complexity

> Cosine Similarity",subtop general model techniqu,"['subtop', 'general', 'model', 'techniqu']",Subtopics of General Modeling Techniques
112,"Subtopics of K-means clustering

> Understanding Clustering Algorithm

> Python coding for Kmeans

> Few issues of Kmeans

> Expectation-Maximization approach

> Silhouette score for finding best no. of clusters

> Elbow method for finding best no. of clusters

> Plotting Clusters

> Features of KMeans model

> Density-based clustering",subtop kmean cluster,"['subtop', 'kmean', 'cluster']",Subtopics of K-means clustering
113,"Subtopics of Hierarchical clustering

> Basics of Hierarchical clustering

> Types of hierarchical clustering

> Proximity matrix

>  Understanding Linkage

> Finding out no. of clusters from visualization

> Python coding for Hierarchical clustering",subtop hierarch cluster,"['subtop', 'hierarch', 'cluster']",Subtopics of Hierarchical clustering
114,"Subtopics of Principal Component Analysis

> Basics of PCA

> Drawback of PCA

> Python Coding of PCA

> Understanding the important features in PCA

> Deeper Understanding of PCA

> Math behind PCA

> TruncatedSVD

> PCA Visualization

> Trace of matrix",subtop princip compon analysi,"['subtop', 'princip', 'compon', 'analysi']",Subtopics of Principal Component Analysis
115,"Subtopics of Anomaly Detection

> Basics of Anomaly detection

> Assumptions in Anomaly detection

> Finding global outliers

> Univariate Anomaly Detection

> Multivariate Anomaly Detection

> Python coding for Isolation forest

> Finding local outliers-LOF

> Visual representation of univariate anomalies

> Deeper Understanding of Anomalies",subtop anomali detect,"['subtop', 'anomali', 'detect']",Subtopics of Anomaly Detection
116,"Subtopics of Natural Language Processing 

> Natural Language Understanding (NLU) in five dimensions

> Important Applications of NLP

> Feature engineering in NLP

> n-gram in NLP

> Understanding TF-IDF

> RNN or LSTM in NLP

> Transformer Network

> Positional Encoding in Transformer

> Libraries for NLP

> Right order for a text classification

> Measuring the complexity of a sentence",subtop natur languag process,"['subtop', 'natur', 'languag', 'process']",Subtopics of Natural Language Processing 
117,"Subtopics of Topic modeling

> Basics of Topic modeling

> Basic assumptions of all topic models

> Libraries for Topic Modeling

> Understanding LDA

> Plate Notation

> Python coding of LDA model

> Visualization of LDA",subtop topic model,"['subtop', 'topic', 'model']",Subtopics of Topic modeling
118,"Subtopics of Recommender Systems

> Basics of Recommender System

> Popular Recommender Systems

> Collaborative Filtering

> Content-Based Filtering

> Hybrid Approach

> Implementation strategies of Collaborative Filtering

> Latent factor models

> Matrix Factorization

> Implementation of SVD

> Metrices used for evaluation of Recommender systems

> Shortcoming of content-based recommender systems",subtop recommend system,"['subtop', 'recommend', 'system']",Subtopics of Recommender Systems
119,"Understanding Functions, Model and  Environment

In natural language, function means assigning a memory location for taking some input and giving some output. Model is a combination of multiple functions. Environment is a collection of multiple models.

Functions is one of the most basic levels of reusing code. It groups together a set of statements so they can be run more than once.",understand function model environ,"['understand', 'function', 'model', 'environ']","Understanding Functions, Model and  Environment"
120,"Defining a function

There are many inbuilt functions associated with data or data structure, however we can define our own functions as and when required.

def example_function(argument):
  '''
  This functions returns the …...
  '''
  desired_result= f""Your input is: {argument}""

  return desired_result

> argument means the input

> function without return statement,

def change(one):
   print(one*3)",defin function,"['defin', 'function']",Defining a function
121,"Functions details

We can pass 'n' number of arguments in a function

def change(first, *second):
  return print(f""First input is {first}\nSecond input is {second}"")
 
change(1,2,3,4) returns

First input is 1
Second input is (2, 3, 4)

> Function can have multiple arguments like 

example_function(arg1,arg2,..etc) or multiple returns like return x,y,…etc

> Function can have multiple return statement like 

if ….return x else return y",function detail,"['function', 'detail']",Functions details
122,"Function and method

Function and method both look similar as they perform in an almost similar way. A method is called by its name but it is associated with an object (dependent).

Functions are to view the properties of a data or data structure and methods are to perform some actions on data or data structure. 

e.g. functions and methods can be applied on string, list, tuple, set, dict

Methods are of the form:

object.method(arg1,arg2,etc...)",function method,"['function', 'method']",Function and method
123,"Global variable

Iside a function, we can call global variable (defined outside function)

b=10 

def change():
  global b
  return b

Then, change() will return 10",global variabl,"['global', 'variabl']",Global variable
124,"Subtopics of Time Series Analysis

> Basics of Time Series

> Components of Time Series

> Multiplicative Model

> Ways to approach a Time Series Prediction Problem

> Naive Forecast

> Moving Average

> Weighted average

> Exponential smoothing

> Double exponential smoothing

> Econometric approach

> Assumption in Time Series

> Understanding ARIMA

> Understanding SARIMA

> Libraries for Time Series Analysis

> Machine learning approach in Time series

> Downside of SARIMA

> Understanding Leakage

> Prophet or Facebook Prophet

> Cross Validation in Time Series

> Demand pattern classification in time series

> Product forecastability",subtop time seri analysi,"['subtop', 'time', 'seri', 'analysi']",Subtopics of Time Series Analysis
125,"Subtopics of Neural Networks

> Basics of Artificial Neural Network

> Perceptron, neuron, node, unit

> Model coefficients

> The components of neural network

> Backward propagation

> Understanding epochs

> Understanding of batch size

> Optimal accuracy in Neural Network

> Applications of Neural Networks

> RNN for machine translation

> Steps involved in a Neural Network

> Libraries for ANN model

> Define, Create and Compile ANN model

> fit the ANN model on the dataset

> evaluate the ANN model

> Plotting roc_curve",subtop neural network,"['subtop', 'neural', 'network']",Subtopics of Neural Networks
126,"Subtopics of Deep Neural Networks

> Basic of Deep Neural Networks

> SNN, CNN and RNN

> Channels of RGB image

> Deeper understanding of deep neural network

> Loss function and Cost Function in DNN

> Writing sigmoid function

> Writing initialize weight function

> Writing propagate function

> Writing optimization function

> Writing predict function

> Coputation graph

> Calculation behind gradient descent

> Need of vectorization

> Vectorized implementation of forward propagation for layer l

> Notations in DNN

> Element-wise matrix multiplication

> Notation for multiple layers

> Activation functions

> Linear and nonlinear activation function

> Keras and TensorFlow

> Problem of zero initialization

> Initializing parameters for the model

>  Deep Learning capabilities

> Layer dimension notation in Neural Network

> Weight notation in Neural Network

> Bias notation in Neural Network

> Network notation for i th experience

> Forward propagation and backpropagation

> shallow neural network",subtop deep neural network,"['subtop', 'deep', 'neural', 'network']",Subtopics of Deep Neural Networks
127,"Subtopics of Improving Deep Neural Networks

> Basics of Improving Deep Neural Networks

> Basic 'recipe' for all machine learning models

> DNN model complexity

> Dropout regularization

> Implementing dropout

> Data Augmentation

> Early stopping technique to stop overfitting

> Importance of normalized inputs

> Vanishing/exploding gradients

> Gradient checking

> Batch size and iteration

> Optimization Algorithms

> Mini-Batch gradient descent

> Notation of Mini-batch

> Choosing our mini-batch size

> Gradient Descent with momentum

> Exponentially weighted averages

> RMSprop

> Adam Optimization Algorithm

> Learning rate decay

> Problem of local optima

> Hyperparameters tuning in DNN

> Batch normalization

> Multi-class classification

> Deep learning programming framework

> Python coding for DNN

> Different vector operations in tensor flow

> Cross entropy loss function

> Computational graph",subtop improv deep neural network,"['subtop', 'improv', 'deep', 'neural', 'network']",Subtopics of Improving Deep Neural Networks
128,"Subtopics of Structuring ML Projects

> Basics Structuring ML project

> Orthogonalization Basics

> Fundamental assumptions of supervised learning

> Setting up our goal

> Train/dev/test distribution

> Changing dev/test sets and metrics

> Comparing to human level performance

> Human level error and avoidable bias

> Error Analysis

> Mislabeled data

> Mismatched training and dev/test data

> Learning from multiple tasks

> Transfer learning

> Multi-task learning

> End to end deep learning

> Pros and cons of end to end deep learning

> Difference between Multi-class and multi-task learning

> Testing the model for the entire dataset",subtop structur machin learn project,"['subtop', 'structur', 'machin', 'learn', 'project']",Subtopics of Structuring ML Projects
129,"Subtopics of Convolutional Neural Networks

> Perceptual task

> Basics of Pixel

> Convolution on Black-and-white Image

> Convolution operation

> Padding in CNN

> Strided Convolution

> Convolution on RGB image

> Types of layer in a convolutional network

> Pooling Layer

> Necessity of Convolution

> Calulating the size of output volume for convolution or pooling

> Notation for multiple CONV layers

> Common behavior of all the CNN architecture

> LeNet-5 architecture

> AlexNet architecture

> VGG-16 architecture

> ResNet and Inception architecture

> Network in Network

> Using open-source implementation

> Common Augmentation methods

> Sources of data for any ML model

> Object detection

> Tips for winning competitions

> Importing Kaggle dataset in colab

> Building blocks of deep learning model

> Common steps for pre-processing Image Data

> Difference between image classification and object detection

> Image Classification with Localization

> Landmark detection

> Convolutional implementation of sliding windows for detecting multiple objects 

> Object detection algorithm

> YOLO algorithm

> Steps in YOLO algorithm

> Intersection over union  algorithm

> Non-max supression

> anchor box algorithm

> Face Verification

> Face Recognition

> Siamese network

> Training set for calculating Triplet loss

> Neural Style Transfer

> Finding generated image

> Style and style matrix ",subtop convolut neural network,"['subtop', 'convolut', 'neural', 'network']",Subtopics of Convolutional Neural Networks
130,"Subtopics of Recurrent Neural Network

> Examples of Models with sequence data

> Notation in RNN

> Vectorization of words

> Problem of a standard neural network in sequence data

> Summary of RNN

> Types of RNN

> Language Modelling

> Sampling or creating sequence

> Character-level language model

> Exploding and Vanishing gradient

> Gated Recurrent Unit

> Long short-term memory

> Bidirectional RNN

> Deep RNN

> Natural language generation

> Huggingface Transformer

> NLP and Word Embedding

> Featurized representation

> Visualizing word embeddings

> Embedding matrix

> Transfer learning and word embeddings

> Learning word embeddings

> Context/target pairs

> Word2Vec model

> Problem with softmax classification

> Negative Sampling

> GloVe model

> The problem of bias in word embeddings

> Use of Pre-trained model for getting word embeddings

> Machine translation

> Finding the most likely translation

> Refinements to beam search

> BFS and DFS

> Error analysis on beam search

> Speech recognition

> Attention model

> Blue Score

> Basic Rule of CTC based technique

> Trigger word detection",subtop recurr neural network,"['subtop', 'recurr', 'neural', 'network']",Subtopics of Recurrent Neural Network
131,"Subtopics of Guesstimate

> Basics of case interview

> Categorization of case studies

> Guesstimate (Problem Solving Approach)",subtop guesstim,"['subtop', 'guesstim']",Subtopics of Guesstimate
132,"Subtopics of Case Study-ML in Heathcare

> Meaning of Diagnosis

> ML in Healthcare

> Geometric Mean Length of Stay

> Features for the prediction of LOS",subtop case studyml heathcar,"['subtop', 'case', 'studyml', 'heathcar']",Subtopics of Case Study-ML in Heathcare
133,"Subtopics of Case Study-ML in Fraud risk analytics

> Understanding Fraud

> WAYS TO CAPTURE FRAUDULENT BEHAVIOURS

> Social Engineering Scams

> Synthetic Minority Over-sampling Technique",subtop case studyml fraud risk analyt,"['subtop', 'case', 'studyml', 'fraud', 'risk', 'analyt']",Subtopics of Case Study-ML in Fraud risk analytics
134,"Subtopics of Case Study-ML in Credit Risk

> Curious Case of Customer Credit

> Overall Objective of Credit Risk

> Predictive Analytics of Credit Risk

> Prescriptive Analytics",subtop case studyml credit risk,"['subtop', 'case', 'studyml', 'credit', 'risk']",Subtopics of Case Study-ML in Credit Risk
135,"Subtopics of Case Study-ML in E-commerce

> Curious Case of Customer Contacts

> Important Features in E-commerce

>  Predictive Analytics in  E-commerce",subtop case studyml ecommerc,"['subtop', 'case', 'studyml', 'ecommerc']",Subtopics of Case Study-ML in E-commerce
136,"Subtopics of Linux basics and terminal commands

> Understanding UNIX Operating System

> Kernel and shell of UNIX

> Understanding LINUX Operating System

> Ways to connect to an EC2 instance

> Steps to connect to an EC2 instance

> Security of Remote Computer

> Git Bash Understanding

> Introduction to Repository

> Understanding IP address

> Basic Bash or Linux commands",subtop linux basic termin command,"['subtop', 'linux', 'basic', 'termin', 'command']",Subtopics of Linux basics and terminal commands
137,"Subtopics of Python modules and project setup

> Few UNIX commands during project setup

> UNIX command in colab notebook

> File Permission Handling for security

> Creating a Simple Module

> Basics of modular programming

> One-Off Script Layout

> Installable Package Layout

> App with Internal Packages Layout

> Data Science Project Layout

> Using cookiecutter for packaging

> stdin, stdout and stderr

> Reading a static data file from inside a Python package

> Code for Creating python library

> Testing during packaging of our own python library",subtop python modul project setup,"['subtop', 'python', 'modul', 'project', 'setup']",Subtopics of Python modules and project setup
138,"Understanding Production Grade Programming

Production environment is the setting where  products are actually put into operation for their intended uses by end users.

1. Object Oriented Programming (OOP)

2. Handling Errors and Exceptions

Production grade code is where all chances of error are taken care with try block and classes are created suitably",understand product grade program,"['understand', 'product', 'grade', 'program']",Understanding Production Grade Programming
139,"Object Oriented Programming 

OOP is a programming language model organized around object and class (a group of objects) rather than actions and data rather than logic

> Object is an identifiable entity with some characteristic and behavior. Thus object means identifiers (a name used to identify a variable, function, class, module or other object)

> Instantiation in OOP means creating an instance of class",object orient program,"['object', 'orient', 'program']",Object Oriented Programming 
140,"Attribute and Object

An attribute is a characteristic of an object. A method is an operation we can perform with the object.",attribut object,"['attribut', 'object']",Attribute and Object
141,"Defining a class

Apart from the standard classes, we can create new class as and when required. We can define different attributes and methods inside the class with proper indendation

By convention we give classes a name that starts with a capital letter.

> Similar functions are grouped together in a class

class Cylinder:
  
  def __init__(self, radius=1, height=1):
    self.radius=radius
    self.height=height
    
  def volume(self):
    return 3.14 * ((self.radius)**2)* self.height

  def surface_area(self):
    return 2 * 3.14 * self.radius* self.height


Cylinder(4,5).radius gives 4

Cylinder(4,5).height gives 5

Cylinder(4,5).volume() gives 251.2

Cylinder(4,5).surface_area() gives 125.6

_init_ is used to define the attribute

radius=1, height=1 are the default values of the variables to avoid null error.",defin class,"['defin', 'class']",Defining a class
142,"Understanding Polymorphism

polymorphism refers to the way in which different object classes can share the same method name",understand polymorph,"['understand', 'polymorph']",Understanding Polymorphism
143,"Exception handling   

Errors detected during execution are called exceptions. There is full list of built in python exceptions-

https://docs.python.org/3/library/exceptions.html

input=""prabir""
try:
  # we do our operations here

  print(f'Your input number is greater than 5 with {input-5} points')
    
except:
  # If there is an exception, then execute this block

  print(""Sorry ! there is an exception"")
  
else:
  # If there is no exception then execute this block

  print(""The code is successfully run"")

> There can be one or more than one except statements in a try-except block

try:

   Code block here
   ...
   Due to any exception, this code may be skipped!

finally:

   This code block would always be executed.

> Finally can also be used with try and except block",except handl,"['except', 'handl']",Exception handling   
144,"Single Underscore in the variable name

Single Underscore in the variable name(var_ ): Sometimes used as a name for temporary or insignificant variables",singl underscor variabl name,"['singl', 'underscor', 'variabl', 'name']",Single Underscore in the variable name
145,"create, read, update and delete

CRUD Meaning: CRUD is an acronym that comes from the world of computer programming and refers to the four functions that are considered necessary to implement a persistent storage application: create, read, update and delete.",creat read updat delet,"['creat', 'read', 'updat', 'delet']","create, read, update and delete"
146,"Double Leading and Trailing Underscore in the variable name

Double Leading and Trailing Underscore in the variable name( __var__ ): Indicates special methods defined by the Python language. 

",doubl lead trail underscor variabl name,"['doubl', 'lead', 'trail', 'underscor', 'variabl', 'name']",Double Leading and Trailing Underscore in the variable name
147,"Subtopics of Version control-Git

> CVCS and DVCS

> GitHub workflow

> Steps of creating local git repository

> Concept of Branch

> Synchronising local git repository with Github

> Merge Conflict

> Continuous Integration/ Continuous Deployment or Delivery",subtop version controlgit,"['subtop', 'version', 'controlgit']",Subtopics of Version control-Git
148,"Subtopics of API basics with flask

> Understanding API

> Types of web pages

> Machine Learning Services

> Types of API

> Basics of Flask

> Benefits of using the Flask framework

> Popular HTTP Requests

> Use of Virtual Environment

> Creating a virtual environment

> Understanding pip install

> Creating a Web App in Flask

> Jinja techniques

> Ways to edit script code (.py file) in Colab notebook

> Flaskr as a basic blog application

> Hyper Text Markup Language",subtop applic program interfac basic flask,"['subtop', 'applic', 'program', 'interfac', 'basic', 'flask']",Subtopics of API basics with flask
149,"Subtopics of FastAPI

> Understanding FastAPI

> WSGI vs ASGI

> Features of FastAPI

> Pydantic Data Model

> FastAPI Working with SQL",subtop fastapi,"['subtop', 'fastapi']",Subtopics of FastAPI
150,"Subtopics of Docker

> Docker Basics

> Image or Docker Image

> Container or Docker Container

> Introduction to dockerhub

> Ways to install docker in EC2 instance

> Basic Docker Commands

> Docker or Container volume

> Docker Bridge Networking and Port Mapping

> Docker Compose

> Docker swarm",subtop docker,"['subtop', 'docker']",Subtopics of Docker
151,"Subtopics of Microservices and streamlit

> Understanding Microservices

> API Gateway

> Kubernetes

> Computer Networking

> Development of Microservices

> Python code for creating a Streamlit file

> Creating a file for streamlit inside the session

> Creating a Streamlit Project

> Deploying streamlit app in Google Cloud

> Deploying streamlit app in AWS EC2",subtop microservic streamlit,"['subtop', 'microservic', 'streamlit']",Subtopics of Microservices and streamlit
152,"Subtopics of ML Lifecycle

> Machine learning engineering

> 4 Phases of ML Lifecycle

> Challenges with ML during development

> Challenges with ML in production

> Data drift and Model drift",subtop machin learn lifecycl,"['subtop', 'machin', 'learn', 'lifecycl']",Subtopics of ML Lifecycle
153,"Understanding Competitive coding

Competitive coding is important for interview to test coding aptitude

30-40% companies keep this competitive coding round in their interview process

> not allowed to run the code on python interpreter or editor. 

> Pen and paper test to check how we run the code on our head.",understand competit code,"['understand', 'competit', 'code']",Understanding Competitive coding
154,"google colab notebook

To run a python code on cloud we can use google colab notebook (is the Jupyter notebook that run in the cloud and are highly integrated with Google Drive).",googl colab notebook,"['googl', 'colab', 'notebook']",google colab notebook
155,"Code editor

Code editor tend to go for a broader approach and able to edit all types of files, instead of specializing in a particular type or language. Example of Code Editors are VS Code, PyCharm",code editor,"['code', 'editor']",Code editor
156,"Compiler

Compiler scans the entire program and translates the whole of it into machine code at once. ",compil,['compil'],Compiler
157,"Interpreter 

Interpreter translates just one statement of the program at a time into machine code and takes very less time to analyse the code. 

cmd.exe is the default command-line interpreter for the OS/2, eComStation, ArcaOS, Microsoft Windows, and ReactOS operating systems. The name refers to its executable filename. It is also commonly referred to as cmd or the Command Prompt, referring to the default window title on Windows

",interpret,['interpret'],Interpreter 
158,"IDLE

IDLE is Integrated Development and Learning Environment for Python. Example of IDE's are Spyder, Jupyter notebook

",integr develop learn environ,"['integr', 'develop', 'learn', 'environ']",IDLE
159,"Anaconda and python

Python language was created in 1991

Van Rossum thought he needed a name that was short, unique, and slightly mysterious, so he decided to call the language Python. Python is a high level language.

Anaconda is the heaviest and the biggest snake in the world (can weight upto 550 pound or 250kg or more and length 25 feets or more). 

On the other hand, the python is no doubt the longest snake in the world(can be of length 33 feet or more).

To run a python code in local machine, we may install anaconda which installs python with many more libraries like numpy, pandas, matplotlib etc. and we can use jupyter notebook. ",anaconda python,"['anaconda', 'python']",Anaconda and python
160,"
Arthur Samuel and Geoffrey Everest Hinton

Arthur Samuel coined the term “Machine Learning” in 1952.

 Father of Machine Learning: Geoffrey Everest Hinton (for ANN)
",arthur samuel geoffrey everest hinton,"['arthur', 'samuel', 'geoffrey', 'everest', 'hinton']","
Arthur Samuel and Geoffrey Everest Hinton"
161,"Cloud computing or Cluster computing, Cloud Storage

Cloud (or Cluster) computing means the use of remote computer having remote storage (called cloud storage), OS and application softwares.

Cloud Storage examples: Google Drive, Amazon Drive (AWS S3) and Azure Storage. Google Drive offers 15 GB for free including emails & Google Photos and Amazon Drive (S3) gives we 5 GB for free.

> This remote computer is also called cloud server. Server (local or remote) is a computer connected to multiple users.",cloud comput cluster comput cloud storag,"['cloud', 'comput', 'cluster', 'comput', 'cloud', 'storag']","Cloud computing or Cluster computing, Cloud Storage"
162,"Types of computer languages

High-Level Language (programmer-friendly, requires a compiler/interpreter to be translated into machine code)

Low-level language (machine-friendly,requires an assembler that would translate instructions)",type comput languag,"['type', 'comput', 'languag']",Types of computer languages
163,"Jupyter notebook

Jupyter is a very popular application used for data analysis. 

It's an IPython notebook (""interactive python""). We can run each block of code separately. 

Python has two basic modes: script and interactive. The script mode is the mode where the scripted and finished .py files are run in the Python interpreter. Interactive mode is a command line shell which gives immediate feedback for each statement

Jupyter, comes from the core supported programming languages that it supports: Julia, Python, and R. There are many more languages that it supports. ",jupyt notebook,"['jupyt', 'notebook']",Jupyter notebook
164,"
Notebook documents

Notebook documents are both human-readable documents containing the analysis description and the results (figures, tables, etc..) as well as executable documents which can be run to perform data analysis.",notebook document,"['notebook', 'document']","
Notebook documents"
165,"collaboration site

A collaboration site is used to support project teams, research groups, and other collaborative work. 

Teams, committees, and student groups may make announcements, engage in online discussions, and share resources within their collaboration sites.",collabor site,"['collabor', 'site']",collaboration site
166,"Understanding of language

>> Basic data, data-structure (int, float, boolean, str, list, tuple, set, dict) and variables can be thought of basic words of python language

>> Syntax is grammer for the language

>> expression, conditional and statements are the sentences of the language

>> Loops, functions, methods and classes are some set of sentences to perform an operation on the data (experience stored in a computer)

>> Subjects like Social Science or Humanities, Science, Commerce and Technical are nothing but different features/ dimensions/ parameters/streams for understanding life experience logically. Every subject is again an experience for human being and thus have some special words or features or concepts

>> Libraries are subject specific. Thus every libraries have some special words, functions and methods and class.

>> Data scientists are mainly concerned with statistics related libraries to understand any experience (data) from logical dimension

>> The improvement of human intelligence is also based on understanding an experience logically. The first experience of human being is social science or humanites",understand languag,"['understand', 'languag']",Understanding of language
167,"Subtopics of General Knowledge for a Data Scientist

> google colab notebook

> Code editor

> Compiler

> Interpreter 

> IDLE

> Anaconda and python

> Arthur Samuel and Geoffrey Everest Hinton

> Cloud computing or Cluster computing, Cloud Storage

> Types of computer languages

> Jupyter notebook

> Notebook documents

> collaboration site

> Understanding of language",subtop general knowledg data scientist,"['subtop', 'general', 'knowledg', 'data', 'scientist']",Subtopics of General Knowledge for a Data Scientist
168,"Subtopics of Cloud Computing

> Cloud computing advantages

> Cloud service models

> Runtime system

> CPU vs GPU

> Azure machine learning platform

> Azure ML Workspace and Azure ML piplines

> Connecting to azure ml

> Different cloud deployment models",subtop cloud comput,"['subtop', 'cloud', 'comput']",Subtopics of Cloud Computing
169,"Subtopics of MLOps-MLFlow

> MLOps and DevOps

> Agile software development

> Key components of MLOps

> Advantages of MLOps

> Key outcomes of MLOps

> Model registry and Metadata

> MLFLow Basics",subtop mlopsmlflow,"['subtop', 'mlopsmlflow']",Subtopics of MLOps-MLFlow
170,"Subtopics of PySpark

> Apache Spark (PySpark)

> Advantages of Spark

> Spark Modules

> RDD operations

> Anatomy of Spark Application

> Spark application coding

> Directed Acyclic Graph

> Azure Databricks

> Spark application deploy modes",subtop pyspark,"['subtop', 'pyspark']",Subtopics of PySpark
171,"Subtopics of Airflow

> Apache Airflow

> Introduction to Jenkins

> Use of Apache Airflow

> Airflow Components

> Web Server and Scheduler

> Executor, Worker and Operator

> Metadata Database

> Applications of Airflow

> Understanding Celery",subtop airflow,"['subtop', 'airflow']",Subtopics of Airflow
172,"Understanding Library

A library is a collection of python packages with multiple programming blocks

Library means main folder

Top Python Libraries for Data Scientists : 

Numpy (stands for Numerical Python, released in 2005), 
Pandas (in 2008), 
SciPy (in 2001), 
Matplotlib (in 2003),
Seaborn (in 2018), 
Scikit-Learn (in 2007), 
TensorFlow (in 2015), 
Keras (in 2015), 
Theano, 
PyTorch,  
sys, 
time",understand librari,"['understand', 'librari']",Understanding Library
173,"Understanding Package

A package is a collection of Python modules

Package means sub folder

We may import the entire library (the main folder) or a particular package (a sub folder) or a particular module (a file) with dot(.) operator",understand packag,"['understand', 'packag']",Understanding Package
174,"Understanding Module

A module is a single Python file that consists of classes, functions, attributes and methods

Module means .py files

The term Library, Package or module are often used interchangeably, especially since many libraries only consist of a single module

Modules provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers. 

Modules also provide standardized solutions for many problems that occur in everyday programming.

some popular categories of Python modules are: 
1. operating system
2. math
3. time
4. data visualization ",understand modul,"['understand', 'modul']",Understanding Module
175,"Writing softmax function with numpy (used in deep learning)

def softmax(x):

    x_exp = np.exp(x)

    x_sum = np.sum(x_exp, axis = 1, keepdims = True)

    s = np.divide(x_exp, x_sum)

    return s",write softmax function numpi use deep learn,"['write', 'softmax', 'function', 'numpi', 'use', 'deep', 'learn']",Writing softmax function with numpy (used in deep learning)
176,"Modular programming 

Modular programming refers to the process of breaking a large, unwieldy programming task into separate, smaller, more manageable subtasks or modules. 

Advantages are Simplicity, Maintainability, Reusability",modular program,"['modular', 'program']",Modular programming 
177,"N-dimensional array

The most important object defined in NumPy is an N-dimensional array type called ndarray.

> One dimensional array with 22 elements: size=(22)

> Two dimensional array with 4 lists each having 22 elements: size=(4,22)- similar like table with 4 rows and 22 columns

> Three dimensional array with 5 lists (dimension 1) each having 4 lists (dimension 2) each having 22 elements (dimension 3): size(5,4,22)",ndimension array,"['ndimension', 'array']",N-dimensional array
178,"Creating a two-dimensional array 

import numpy as np

another_array = np.array([[1,2,5],[3,4,7]]) 

another_array.shape is (2,3). 

A full experience table can be stored in a 2D Numpy array. It may be of shape 1000k(rows),1k(column). The same may be done with nested list means a list having 1000k lists and each list haviing 1k elements",creat twodimension array,"['creat', 'twodimension', 'array']",Creating a two-dimensional array 
179,"attributes of numpy array

my_array.shape

my_array.ndim

my_array.size",attribut numpi array,"['attribut', 'numpi', 'array']",attributes of numpy array
180,"Advantages of numpy array 

1. Numpy array takes very less space in memory as compared to list

2. Execution time for any operation with Numpy array is also very less as compared to list 

3. Functionality wise Numpy array is better than list ",advantag numpi array,"['advantag', 'numpi', 'array']",Advantages of numpy array 
181,"Creating ndarray

>> create ndarray 

1. np.empty((5,3),dtype=int)

2. np.zeros((3,3))

3. np.ones((3,3))

4. np.random.random(size=(4,3))

5. np.random.randint(0,5,size=(2,3,4))

np.random.randn(2, 3) # Return a sample (or samples) from the “standard normal” distribution with shape (2,3)

6. np.full((3,3),30)- creates a (3,3) array full of 30's

>> create 1d array

np.arange(10,20,2)- 
np.linspace(1,50,10)",creat ndarray,"['creat', 'ndarray']",Creating ndarray
182,"Defining Array size

Size=(2) means an array or list with 2 elements

size=(2,3) means a list of 2 lists each having 3 elements

size=(2,3,4) means a list of 2 lists each having 3 lists each having 4 elements

array([[[1, 4, 3, 2],
        [1, 0, 2, 0],
        [0, 1, 3, 4]],

       [[4, 1, 2, 3],
        [0, 2, 0, 2],
        [2, 2, 4, 0]]])

(2,3,4) is actually the array shape and the size is 2*3*4 means 24, the total no. of elements

>> Array shape is defined as size in the syntax",defin array size,"['defin', 'array', 'size']",Defining Array size
183,"Normalizing rows (used in deep learning algorithm)

x = np.array([[1,2,5],[2,3,4]])

x_norm = np.linalg.norm(x, ord=None, axis=1, keepdims=True)

x_normalized = np.divide(x, x_norm)

> With keepdims=True the result will broadcast correctly against the original x.

>  axis=1 means you are going to get the norm in a row-wise manner. If you need the norm in a column-wise way, you would need to set axis=0.

> numpy.linalg.norm has another parameter ord where we specify the type of normalization to be done

> ord=None means 2-norm",normal row use deep learn algorithm,"['normal', 'row', 'use', 'deep', 'learn', 'algorithm']",Normalizing rows (used in deep learning algorithm)
184,"Conversion of list and tuple to ndarray

np.array(my_list or my_tuple)

 np.array(my_list, ndmin = 2)",convers list tupl ndarray,"['convers', 'list', 'tupl', 'ndarray']",Conversion of list and tuple to ndarray
185,"Indexing and Slicing of ndarray

> Indexing and Slicing ndarray is same as list

slicing of 2D array

my_array[2:5,0:3]

my_array[2:5,3]

my_array[2:5]",index slice ndarray,"['index', 'slice', 'ndarray']",Indexing and Slicing of ndarray
186,"
Grabbing element by index of 2D array

my_array[0,9] or my_array[0][9]

Grabbing element with .flat
my_array.flat[5]",grab element index 2d array,"['grab', 'element', 'index', '2d', 'array']","
Grabbing element by index of 2D array"
187,"Array Manipulation

np.reshape(my_array,new_shape) or my_array.reshape(new_shape)

> my_array.reshape(-1, 1) -It mean, if we have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1)

np.resize(my_array,shape_with_new_size) or my_array.resize(shape_with_new_size)

np.transpose(my_array) or my_array.transpose() or my_array.T

my_array.flatten()

np.stack((array_1, array_2), axis=0)

np.fill_diagonal(my_matrix,5)

my_array.dtype=""float64""",array manipul,"['array', 'manipul']",Array Manipulation
188,"Important functions in numpy

1. np.insert(my_array, location_index, insert_value, axis=0),

2. np.append(my_array, array_to_append, axis=1)- array_to_append should match no. of elements in all  dimension except axis=1

3. np.delete(my_array, location_index, axis=2),

4. np.unique(my_array, return_index=True, return_counts=True)--returns a tuple of arrays

>> axis=0 is the 1st dim or minor axis and signifies rows, axis=1 is the 2nd dim or major axis and signifies columns",import function numpi,"['import', 'function', 'numpi']",Important functions in numpy
189,"Difference between view and copy in numpy

view

a = np.array([2,34,12])
b = a
b[0] = -999 
print(b)
print(a)

[-999   34   12]
[-999   34   12]

copy

a = np.array([2,34,12])
b = a.copy()
b[0] = -999
print(b)
print(a)

[-999   34   12]
[ 2 34 12]",differ view copi numpi,"['differ', 'view', 'copi', 'numpi']",Difference between view and copy in numpy
190,"Basic Operations & Functions in numpy

a=array_1

b=array_2

np.add(a,b),

np.subtract(a,b),

np.absolute(a-b)

np.multiply(a,b),

np.divide(a,b),

np.sum(a),

np.square(a),

np.min(a,axis=2),

np.max(a,axis=1),

np.mean(a,axis=0),

np.median(a,axis=0)

np.var(a,axis=0),

np.sort(a,axis=0),

my_array.astype('float64')

np.argsort(a,axis=0)-returns the indices of sorted array,

np.argmin(a,axis=0)-returns the index of the minimum element 

np.where(conditional_statement) # This can be used for finding index of any element in numpy

np.where(a == a.max())[0][0]

my_array.tolist()- or list(my_array)

np.percentile(my_df['column'],50)--returns the value for 50th percentile or median
",basic oper function numpi,"['basic', 'oper', 'function', 'numpi']",Basic Operations & Functions in numpy
191,"Exponentiation of vectors (used in deep learning algorithm)

>> For vectors we use numpy library for exponentiation

import numpy as np

t_x = np.array([1, 2, 3])

print(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))

sigmoid_t_x = 1/(1+ np.exp(-t_x))

>> For real numbers we use math library for exponentiation

import math
from public_tests import *

x = 0 
sigmoid_x = 1/(1+math.exp(-x)) will return 0.5",exponenti vector use deep learn algorithm,"['exponenti', 'vector', 'use', 'deep', 'learn', 'algorithm']",Exponentiation of vectors (used in deep learning algorithm)
192,"Basics of Pandas Dataframe

The name pandas is derived from the term “panel data”. It is for doing practical, real world data analysis in Python

Pandas is designed to make it easier to work with structured data i.e. organised experience set.

The DataFrame object in pandas is ""a two-dimensional tabular, column-oriented data structure with both row and column labels."" (excel is a similar datastructure)

A DataFrame is like a fixed-size dict in that we can get and set values by index label

Due to its inherent tabular structure, pandas dataframes also allow cells to have null values (i.e. no data value such as blank space, NaN(Not a Number), -999, etc).",basic panda datafram,"['basic', 'panda', 'datafram']",Basics of Pandas Dataframe
193,"Difference between list and numpy array

Python lists are flexible and can store data items of various types (e.g. integers, floats, text strings). List can be converted to numpy array.

However, numpy arrays generally deals with numerical data. Thus, numpy arrays can provide more functionality for running calculations such as element-by-element arithmetic operations",differ list numpi array,"['differ', 'list', 'numpi', 'array']",Difference between list and numpy array
194,"Rows and columns of pandas dataframe

The columns in pandas DataFrames can be different types (e.g. the first column containing integers and the second column containing text strings). Each value in pandas dataframe is referred to as a cell that has a specific row index and column index within the tabular structure.",row column panda datafram,"['row', 'column', 'panda', 'datafram']",Rows and columns of pandas dataframe
195,"Pandas series

We can creare pandas series.

number_series = pd.Series([2, 3, 5, 6, 8])

Pandas series is nothing but a 1D numpy array",panda seri,"['panda', 'seri']",Pandas series
196,"Convertion of numpy array to pandas dataframe

We can convert a 2D numpy array, list of lists, dictionary, list of dicts, list of tuples into pandas DataFrame.

1. pd.DataFrame(some_2D_array , columns = ['Name', 'Age'])

2. pd.DataFrame(list_of_lists, columns = ['Name', 'Age'])

3. pd.DataFrame([employee_dict])

4. pd.DataFrame(list_of_dicts)

5. pd.DataFrame(list_of_tuples, columns = ['Name', 'Age'])",convert numpi array panda datafram,"['convert', 'numpi', 'array', 'panda', 'datafram']",Convertion of numpy array to pandas dataframe
197,"For uploading any file from local computer to the colab session

from google.colab import files
upload=files.upload()",upload file local comput colab session,"['upload', 'file', 'local', 'comput', 'colab', 'session']",For uploading any file from local computer to the colab session
198,"Methods and attributes of pandas DataFrame 

1. my_df.index

2. my_df.columns

3. my_df.head()

4. my_df.tail()

5. my_df.info()

6. my_df.rename(columns = {'Name':'Actor Name','Age' :'Actor Age'}, inplace=True)

or,
my_df.columns = list_of_new column_names

7. my_df.shape

8. my_df.describe() 

or my_df.groupby('column_name').describe()

9. my_df.values

10. my_df.keys() or  my_df.columns 

11.  
my_df.replace(r'?', np.NaN)  

my_df['col_name'].replace([np.inf, -np.inf], 0, inplace=True)

my_df['col_name'].replace(np.nan, my_df['col_name'].median(), inplace=True)

>> r string stands for 'raw string'

>> u string stands for 'unicode string'

>> np.inf means numpy infinite number

12. my_df.dropna(inplace=True) is called for removing the rows which contains the null values",method attribut panda datafram,"['method', 'attribut', 'panda', 'datafram']",Methods and attributes of pandas DataFrame 
199,"Connecting google drive to colab notebook

from google.colab import drive
drive.mount('/content/drive')

-After mounting the drive in colab, we can  read a file/create a file/write in a file/append in a file/edit a file(csv/excel) 

",connect googl drive colab notebook,"['connect', 'googl', 'drive', 'colab', 'notebook']",Connecting google drive to colab notebook
200,"Converting a csv file to pandas DataFrame

my_df=pd.read_csv('file_path')

or 
data = pd.read_csv(open_source_csv_url)

> url is also a file_path

> When data is in latin language,

my_df=pd.read_csv('file_path', encoding='latin-1')

For creating a duplicate copy of the dataframe

new_df = my_df.copy()",convert csv file panda datafram,"['convert', 'csv', 'file', 'panda', 'datafram']",Converting a csv file to pandas DataFrame
201,"Creating an empty csv file on a specific folder

my_df = pd.DataFrame(list())

my_df.to_csv('folder_path/name_of_empty_csv.csv')
",creat empti csv file specif folder,"['creat', 'empti', 'csv', 'file', 'specif', 'folder']",Creating an empty csv file on a specific folder
202,"Writing data in a csv file

1. Create the csv writer

import csv

writer = csv.writer(open('file_path', 'w'))

2. Define variables

header=['Name','Weight','Height']

data=[['Ram',600,10],['Sham',400,20],['Jadu',200,10]]

3. Write multiple rows to the csv file

writer.writerow(header)

writer.writerows(data)",write data csv file,"['write', 'data', 'csv', 'file']",Writing data in a csv file
203,"Appending new row to an exsisting csv file

import csv

writer = csv.writer(open('file_path', 'a'))

data=['Madhu',800,90]

writer.writerow(data)",append new row exsist csv file,"['append', 'new', 'row', 'exsist', 'csv', 'file']",Appending new row to an exsisting csv file
204,"Editing a csv file

> Read the csv as pandas df and then do all the editing and then save it to csv

>> Reading a json file as pandas df

my_df=pd.read_json('file_path')",edit csv file,"['edit', 'csv', 'file']",Editing a csv file
205,"Excel file with multiple worksheets

xls = pd.ExcelFile('file_path')

xls.sheet_names

df1 = pd.read_excel(xls, 'sheet_name')

Reading an excel file first sheet directly

df = pd.read_excel ('file_path')",excel file multipl worksheet,"['excel', 'file', 'multipl', 'worksheet']",Excel file with multiple worksheets
206,"Writing in a new excel file

import xlwt
  
my_workbook = xlwt.Workbook() 

sheet1 = my_workbook.add_sheet(""sheet_name"")
  
font_style = xlwt.easyxf('font: bold 1,color red;')
  
sheet1.write(0, 0, 'JADU', font_style)

my_workbook.save(""file_path/file_name.xls"")",write new excel file,"['write', 'new', 'excel', 'file']",Writing in a new excel file
207,"Editing in pandas df and then save them to csv/excel

my_df = pd.DataFrame([['Ram',600,10],['Sham',400,20],['Jadu',200,10]], columns=['Name','Weight','Height'])

my_df.to_csv('/content/drive/MyDrive/my_data.csv')

or,

my_df.to_excel('/content/drive/MyDrive/my_data.xlsx')",edit panda df save csvexcel,"['edit', 'panda', 'df', 'save', 'csvexcel']",Editing in pandas df and then save them to csv/excel
208,"Slicing operation on Pandas DataFrame

my_df.iloc[:,-4:] or my_df.iloc[:, 7]  slicing operation both on rows and columns by index

my_df.loc[list_of_index_name] -slicing by index name

my_df.loc[:, list_of_columns] -slicing operation on rows by index and on columns by column names

my_df[0:10] -slicing operation only on rows

my_df[list_of_columns] -slicing operation only on columns",slice oper panda datafram,"['slice', 'oper', 'panda', 'datafram']",Slicing operation on Pandas DataFrame
209,"Slicing operation on a particular column  in Pandas DataFrame

my_df['column_name'][0:5]

type(my_df['column_name']) is pandas series

Writing any value at a particular cell

my_df['column_name'][row_index]=value -This is similar like cell reference in excel",slice oper particular column panda datafram,"['slice', 'oper', 'particular', 'column', 'panda', 'datafram']",Slicing operation on a particular column  in Pandas DataFrame
210,"Conditional Slicing or Conditional Filtering in  Pandas DataFrame

>> Normal Filtering with single selection on single or multiple columns

my_df.loc[(my_df['new_cast']=='Robert De Niro')]

my_df.loc[(my_df['new_cast']=='Robert De Niro') & (my_df['original_language']=='en')]

>> Normal Filtering with multiple selection on single column

my_df.loc[(my_df['original_language']=='en') | (my_df['original_language']=='fr')]

>> Conditional Filtering on single or multiple columns in  Pandas DataFrame

long_movies = my_df[my_df['runtime'] > 120]

long_english_movies = my_df[(my_df['original_language']=='en') & (df['runtime'] > 120)]",condit slice condit filter panda datafram,"['condit', 'slice', 'condit', 'filter', 'panda', 'datafram']",Conditional Slicing or Conditional Filtering in  Pandas DataFrame
211,"Adding new column in existing df

my_df['half_runtime'] = 250 
or 
my_df.half_runtime = [list_with_same_len]

my_df['half_runtime'] = my_df['runtime'] * 0.5

my_df['movie_profit'] = my_df['revenue'] - df['budget']

>> We can also add rows/columns by concatination

>> Adding new row in existing df

my_df.loc[(my_df.index.max()+1)] =list_of_values_for columns ",ad new column exist df,"['ad', 'new', 'column', 'exist', 'df']",Adding new column in existing df
212,"Removing one or multiple columns

my_df.drop(list_of_columns, axis = 1, inplace=True)
",remov one multipl column,"['remov', 'one', 'multipl', 'column']",Removing one or multiple columns
213,"Removing one or multiple rows

my_df=my_df.drop(my_df.index[list_of_index]).reset_index()

>> We can also remove rows/columns by slicing",remov one multipl row,"['remov', 'one', 'multipl', 'row']",Removing one or multiple rows
214,"Setting a column as row index

my_df.set_index('column_name', inplace=True)

inplace=True for parmanent change
",set column row index,"['set', 'column', 'row', 'index']",Setting a column as row index
215,"Methods for particular column

1. my_df['column_name'].apply(any_function)

2. list(my_df['column_name'].unique())

3. my_df.column_name.nunique()

4. my_df['column_name'].value_counts()

5. my_df['column_name'].isnull()

6. my_df['column_name'].fillna(0)- works when there is np.NaN values

>> .isna() and .notna() functions are also useful

>>  Checking zero values
if 0 in my_df.values returns T/F

To get the total null values in the entire dataset

print(my_df.isnull().sum())",method particular column,"['method', 'particular', 'column']",Methods for particular column
216,"Sort values in  Pandas DataFrame

my_df.sort_values('column_name', ascending=False)

my_df.sort_values(['column_name1','column_name2'], ascending=[False,True])

Sorting by index
my_df.sort_index(inplace=True)",sort valu panda datafram,"['sort', 'valu', 'panda', 'datafram']",Sort values in  Pandas DataFrame
217,"Datetime operations

from datetime import datetime

from datetime import date

from datetime import timedelta

strptime means string parser, this will convert a string format to datetime.

strftime means string formatter, this will convert a datetime to string format.",datetim oper,"['datetim', 'oper']",Datetime operations
218,"Creating a new column with lambda function (applied on pandas series)

my_df['new_column_name']=my_df['exsisting_column_name'].apply(lambda x : datetime.strptime(x,'%m/%d/%Y'))

my_df['new_column_name'] = my_df['column_name'].apply(lambda x : 1 if x == ""Yes"" else 0)

A lambda function is just like any normal python function, except that it has no name when defining it.

lambda input_variables : output expression

add_lambda = lambda a, b: a + b

> To extract the year

pd.DatetimeIndex(my_df['datetime_col']).year

>>
df['column_name'] = df['column_name'].apply(lambda x:[i['name'] for i in x] if isinstance(x,list) else [])

>> isinstance() function returns True if the specified object is of the specified type, otherwise False ",creat new column lambda function appli panda seri,"['creat', 'new', 'column', 'lambda', 'function', 'appli', 'panda', 'seri']",Creating a new column with lambda function (applied on pandas series)
219,"Pandas data display options

pd.options.display.float_format = '{:.4f}'.format",panda data display option,"['panda', 'data', 'display', 'option']",Pandas data display options
220,"Difference among and & |

if a==5 and b==10:
  print('mango') - This and operator is used along with if statement and returns boolean value

(column_a==5 & coumn_b==10) - returns the result when both the conditions are true

(column_a==5 | coumn_b==10) - returns the result when first condition is true and then returns the result when second condition is true",differ among,"['differ', 'among']",Difference among and & |
221,"Use of Unary operator, ~

Unary means involving a single component

~ reverses an integer or boolean value",use unari oper,"['use', 'unari', 'oper']","Use of Unary operator, ~"
222,"Another way to convert dict to df

pd.DataFrame.from_dict(dictionary_object, orient='index')",anoth way convert dict df,"['anoth', 'way', 'convert', 'dict', 'df']",Another way to convert dict to df
223,"Converting pandas df to numpy array

my_df.to_numpy()",convert panda df numpi array,"['convert', 'panda', 'df', 'numpi', 'array']",Converting pandas df to numpy array
224,"String to numeric in  Pandas DataFrame

my_df[list_of columns]=my_df[list_of columns].astype('float64')

or

pd.to_numeric(my_df[list_of columns])",string numer panda datafram,"['string', 'numer', 'panda', 'datafram']",String to numeric in  Pandas DataFrame
225,"Numpy array or pandas df to matrix convertion

my_matrix = np.asmatrix(my_array or my_df)",numpi array panda df matrix convert,"['numpi', 'array', 'panda', 'df', 'matrix', 'convert']",Numpy array or pandas df to matrix convertion
226,"Subtopics of computer science and machine learning

> Understanding computer science

> Understanding convensional programming

> Types of Computer languages

> Programming language  vs scripting language

> AI technique

> Basics of machine learning

> Basics of deep learning

> Basics of data science

> Role of a data scientist

> Difference between Business Analyst and Data Scientist

> Predictive ML model

> Meaning of heuristic technique

> Comparison between Heuristic technique and ML technique

> Types of learning models

> List of ML models

> Application of regression model

> Application of classification model

> Application of Clustering model

> Components of reinforcement learning

> Application of Reinforcement learning model",subtop comput scienc machin learn,"['subtop', 'comput', 'scienc', 'machin', 'learn']",Subtopics of computer science and machine learning
227,"Basics of Data wrangling

Data wrangling is the process of cleaning and unifying messy and complex data sets for easy access and analysis.",basic data wrangl,"['basic', 'data', 'wrangl']",Basics of Data wrangling
228,"Concatenating pandas DataFrame

pd.concat(list_of_df)-concats both axis

pd.concat(list_of_df, axis=1)-Concatenate along axis 1- means columnwise",concaten panda datafram,"['concaten', 'panda', 'datafram']",Concatenating pandas DataFrame
229,"DataFrame merging operation through joins

1. Inner join

2. Left join

3. Right join

4. Outer join

pd.merge(english_movies, long_movies, how='left',left_on='imdb_id',right_on='imdb_id')

> default join is inner",datafram merg oper join,"['datafram', 'merg', 'oper', 'join']",DataFrame merging operation through joins
230,"Groupby operation on pandas dataframe for data analysis

my_df.groupby('Year_of_release')['runtime'].max().reset_index()

my_df.groupby('Year_of_release')['imdb_id'].count().median()

grouped_actors = new_df.groupby('new_cast').agg({'new_genre':'sum','profit':'mean'}).reset_index()

agg() function is used to pass a function or list of functions to be applied

df.groupby([list_of_columns])['column_name'].median()",groupbi oper panda datafram data analysi,"['groupbi', 'oper', 'panda', 'datafram', 'data', 'analysi']",Groupby operation on pandas dataframe for data analysis
231,"Detailed EDA

1. Connection with the Data:

2. First Feelings of the Data:

We can not see the whole of an extreamly large dataset. 

Therefore to understand the dataset we see the head(), tail() and shape of the dataset.

3. Deeper Understanding of the Data:

Then we try to understand the dataset from info() (missing values or np.NaN values, others strings like '?', '-', 'Not available' etc. and also check the type of values, wheather list, tuple, set or dict is available as string) and describe() methods

4. Cleaning the Data:

Then we perform data cleaning and simplify the data (converting others strings like '?', '-', 'Not available' etc. to np.NaN) and may do concatinating or merging operation in case of multiple datasets

> Deleting the column with missing data

updated_df = df.dropna(axis=1, how='all')

> Deleting the row with missing data

updated_df = newdf.dropna(axis=0)

> Filling the Missing Values – Imputation

updated_df['Age']=updated_df['Age'].fillna(updated_df['Age'].mean())

>> imputation with mode for categorical column

> To check duplication of data
len(my_df[my_df.duplicated()]) shall be 0

> Searching for the columns on which the dataset can be filtered. If not found then creating/extracting the columns/features on which the dataset can be filtered 

> We do filtering on different conditions (extracting subset of data) and groupby operation (classfying the whole dataset on all possible groups) for further analysis

5. Detecting Anomalies in the Data:

6. Visualizing the Data:

Then we plot the  data on different aspects to analyse it more deeply

> We can plot the observed values for one feature with respect to observation numbers or we can plot relation between two features

7. The Conclusion from the Data:",detail exploratori data analysi,"['detail', 'exploratori', 'data', 'analysi']",Detailed EDA
232,"Quick EDA

For serious exploratory data analysis without writing code for different statistics and visualization, we can import  
pandas_profiling  for quick data analysis.

Pandas profiling in colab notebook

1. Run the following code to install the new version of pandas profiling in colab

pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip 

2. Restart the kernel (connecting again to the hosted runtime)

3. Re-import the libraries

import pandas as pd
import numpy as np
from pandas_profiling import ProfileReport

4. Reread the data set as pandas dataframe

5. Generate profile report
 data_profile=ProfileReport(my_dataframe,title='file_name',html={'style':{'full_width':True}})

6. Viewing profile report in colabnotebook
 data_profile.to_notebook_iframe()

7. Saving the report as html

data_profile.to_file(output_file='folder_path/file_name.html')",quick exploratori data analysi,"['quick', 'exploratori', 'data', 'analysi']",Quick EDA
233,"EDA practice

sklearn is a machinelearning library that provides following datasets for practice

1. Toy datasets (Boston house prices,Iris plants,Diabetis,hand writen digits,Breast Cancer etc.), 

2. Real world datasets, 

3. Generated datasets and 

4. Image Data

from sklearn.datasets import load_diabetes

my_data=load_diabetes()",exploratori data analysi practic,"['exploratori', 'data', 'analysi', 'practic']",EDA practice
234,"Use of ast library

import ast

ast stands for Abstract Syntax Trees

ast.literal_eval(): A function that  evaluates a string containing a Python literal (basic words) as list, tuple, set or dictionaries

or,
df['column_name'] = df.apply(lambda row: eval(row['column_name']), axis=1)",use ast librari,"['use', 'ast', 'librari']",Use of ast library
235,"Use of explode function

new_df = df.explode('column_name')

explode() function is used to transform each element of a list-like to a row, replicating the index values",use explod function,"['use', 'explod', 'function']",Use of explode function
236,"Subtopics of Overview of mathematics

> Main branches of pure mathematics

> Sub-branches of Algebra

> Linear algebra

> Application of mathematics in machine learning",subtop overview mathemat,"['subtop', 'overview', 'mathemat']",Subtopics of Overview of mathematics
237,"libraries for data visualization

A Picture is worth a thousand words!

Main libraries for data visualization are 

1. matplotlib and 

2. seaborn",librari data visual,"['librari', 'data', 'visual']",libraries for data visualization
238,"Matplotlib library

MATLAB is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks.

Matplotlib work like MATLAB.

One of the core aspects of Matplotlib is matplotlib.pyplot.

Different plots are: 

1. line plot

2. bar plot, 

3. box plot,

4. scatter plot,

5. step plot,

6. histogram, 

7. Time Series, 

8. Fill Between 

> A time series graph is a line graph of repeated measurements taken over regular time intervals. Time is always shown on the horizontal axis.",matplotlib librari,"['matplotlib', 'librari']",Matplotlib library
239,"Matplotlib  pyplot function

Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.

import matplotlib.pyplot as plt

%matplotlib inline

from matplotlib import rcParams

%matplotlib inline sets the backend of matplotlib to the 'inline' backend. The resulting plots will then also be stored in the notebook document. This is line oriented magic function (Cell oriented magic functions are prefixed with a double %%)

If we are not using matplotlib in interactive mode at all, figures will only appear if we invoke plt.show().

plt.axis('equal') is used to show multiple plots under same axis

plt.legend(list_of_features) shows the legend",matplotlib pyplot function,"['matplotlib', 'pyplot', 'function']",Matplotlib  pyplot function
240,"Matplotlib Line Plot

plt.plot(pandas_series_for_x_label, pandas_series_for_y_label,'bo--',linewidth=2, markersize=8)

> 'bo-' means blue line, 0 as marker, -- is line style",matplotlib line plot,"['matplotlib', 'line', 'plot']",Matplotlib Line Plot
241,"Matplotlib Horizontal Bar Plot

plt.barh(y,x,width=0.4,color='rosybrown')",matplotlib horizont bar plot,"['matplotlib', 'horizont', 'bar', 'plot']",Matplotlib Horizontal Bar Plot
242,"Matplotlib Box plot and Scatter Plot

plt.boxplot(my_new_df['column_name'])

Scatter Plot
plt.scatter(x, y,s=80)

> s=80 is the marker size

",matplotlib box plot scatter plot,"['matplotlib', 'box', 'plot', 'scatter', 'plot']",Matplotlib Box plot and Scatter Plot
243,"drawing a trend line in the scatter plot

fit = np.polyfit(x, y, deg=1) 

p = np.poly1d(fit) 

plt.plot(x,p(x),""r--"") 
",draw trend line scatter plot,"['draw', 'trend', 'line', 'scatter', 'plot']",drawing a trend line in the scatter plot
244,"drawing horizontal or vertial lines

plt.axvline(x=1.8, ymin=0, ymax=1, color ='red', linestyle =""--"",  linewidth = 1)

plt.axhline(y=2.2, xmin=0, xmax=1, color ='red', linestyle =""--"", linewidth = 1)

>> plt.gcf().autofmt_xdate() -to show x-axis labels inclined",draw horizont vertial line,"['draw', 'horizont', 'vertial', 'line']",drawing horizontal or vertial lines
245,"Seaborn library

Seaborn is a library for making statistical graphics in Python. It is built on top of matplotlib.

import seaborn as sns

Based on matplotlib but many special features are available in these plots

sns.color_palette() to see the available colors",seaborn librari,"['seaborn', 'librari']",Seaborn library
246,"seaborn Line plot, Scatter plot, Distribution/Density Plot

1. Line plot

sns.lineplot(x=""index"",y=i1,data=eps,label=i1)

2. Scatter plot with regression line

sns.lmplot(x='feature_name', y='feature_name', data=my_df,height=10, aspect=1)

> lmplot is advance than sns.regplot(x='feature_name', y='feature_name', data=my_df)

3. Distribution/Density Plot

rcParams['figure.figsize'] = 7,7
sns.distplot(my_df['column_name'], hist=True)

> From the distplot we can find wheather the variables are normally distributed

> > A histogram is a bar graph-like representation of data that buckets(or bins) a range of outcomes into columns along the x-axis. The y-axis represents the number count or percentage of occurrences in the data. Alternate is sns.distplot",seaborn line plot scatter plot distributiondens plot,"['seaborn', 'line', 'plot', 'scatter', 'plot', 'distributiondens', 'plot']","seaborn Line plot, Scatter plot, Distribution/Density Plot"
247,"seaborn Joint Distribution Plot, Heatmap, Bar Plot

4. Joint Distribution Plot

sns.jointplot(x='', y='', data=my_df,height=10)

5. Heatmap (for finding correlation between variables)

sns.heatmap(my_df.corr(), vmin=-1, cmap='Greens', annot=True)- visually pleasing

6. Bar Plot

plt.figure(figsize=(7,7))
 or
plt.rcParams['figure.figsize'] = (7,7)

sns.barplot(x='feature name', y='feature name', data=my_df)

",seaborn joint distribut plot heatmap bar plot,"['seaborn', 'joint', 'distribut', 'plot', 'heatmap', 'bar', 'plot']","seaborn Joint Distribution Plot, Heatmap, Bar Plot"
248,"seaborn Histogram, Factor Plot, Box plot

7. Histogram (plot of data group vs. frequency)

rcParams['figure.figsize'] = 10,5

sns.histplot(my_df['column_name'], kde=True)

8. Factor Plot

sns.factorplot(x='sample' ,y= 'value',data=my_df,height=10)

9. Box plot

rcParams['figure.figsize'] = 7,7

sns.boxplot(x='lsample', y='value', data=my_df)

sns.boxplot(new_df['RM'])

> We can check both distribution and outliers by boxplot",seaborn histogram factor plot box plot,"['seaborn', 'histogram', 'factor', 'plot', 'box', 'plot']","seaborn Histogram, Factor Plot, Box plot"
249,"10. Seaborn Pairplot

It is a very useful tool to carry out univariate, bivariate and multivariate analysis in a single plot

sns.pairplot(data=my_df, kind='scatter', diag_kind='kde')
plt.show()

>> Kind of diagonal graphs are Kernel Density Estimates(kde) and kind of non-diagonal graphs are scatter

>> Diagonal graphs are for univariate analysis

>> If dependent variable is the first column of dataset, then the first row graphs are for bivariate analysis

>> In pairgrid plot, we can pass various parameter for more flexibility",10 seaborn pairplot,"['10', 'seaborn', 'pairplot']",10. Seaborn Pairplot
250,"Ploting directly from pandas dataframe

# set the required column as index(parameter to be plotted in x axis)

my_df=my_df.set_index('column_name')

It is very useful to compare multiple time series. Then 'Time' shall be set as index and then

my_df.plot.line(figsize=(7,7))

my_df.groupby(['iyear'])['nkill', 'nwound'].sum().plot.line(figsize=(10,8))

>> requirement of scaler transformation may be investigated

# We can plot one parameter vs one parameter

my_df.plot.line(x='MS Flow through Turbine',y='Load\n', figsize=(15,10))

# We can also plot bar, scatter, box etc. 

df.groupby(['month_number'])[['facewash','facecream']].mean().plot.bar()

my_df.hist(bins=10, figsize=(7,7))

> The plot method on Series and DataFrame is just a simple wrapper around plt.plot()",plote direct panda datafram,"['plote', 'direct', 'panda', 'datafram']",Ploting directly from pandas dataframe
251,"Editing the plot area with matplotlib and seaborn

plt.title('my_chart_title')
plt.ylabel('chart_y_label')
plt.xlabel('chart_x_label')
plt.grid()

plt.rcParams['figure.figsize'] = (7,7) 

> size=7 inches/ 7 inches

sns.set_style(""darkgrid"")

>  pandas data structures, matplotlib and seaborn are closely integrated",edit plot area matplotlib seaborn,"['edit', 'plot', 'area', 'matplotlib', 'seaborn']",Editing the plot area with matplotlib and seaborn
252,"Combining two plots in a single graph

plt.figure(figsize=(7,7))
sns.distplot(np.log10(my_df['feature_name']),color=""y"")

-combination of bar plot and distribution plot

Create multiple plots

1. Using for loop

2. Using subplot

plt.suptitle(""Title for whole figure"", fontsize=16)

plt.subplot(total_subplot_rows,total_subplot_columns, subplot_number)

sns.countplot(my_df['feature_name'])",combin two plot singl graph,"['combin', 'two', 'plot', 'singl', 'graph']",Combining two plots in a single graph
253,"Saving any print output as word file in google drive

with open(""folder_path/file_name.docx"", ""a"") as f:
    print(""Hello World!"", file=f)
    print(""I have a question."", file=f)",save print output word file googl drive,"['save', 'print', 'output', 'word', 'file', 'googl', 'drive']",Saving any print output as word file in google drive
254,"saving the plot as png in google drive

plt.savefig('folder_path/my_fig.png')",save plot png googl drive,"['save', 'plot', 'png', 'googl', 'drive']",saving the plot as png in google drive
255,"Excel basics

Excel is a datastructure with columns and rows. It is also a programming platform

>> In excel, cell reference (a cell or a range of cells) like A5,H6 etc. are feature names or variable names or identifiers

>> Thus, excel stores all the values (input value or output of any function) mentioned in different cell in different variables (A5,H6 etc.) 

>> Thus we can call the variables inside any function (=A5*H6) as and when required and it is stored in another variable (say, K1)

For many organisation presentation in excel is needed",excel basic,"['excel', 'basic']",Excel basics
256,"Data Handling in excel

> Paste special option may be used whenever only one feature of a cell or a group of cells are required to paste.

We can also do arithmetic operations and transpose with paste special. 

> During presentation, we shall present the workbook with multiple worksheets

> During arithmatic operation with a particular cell, we can use $ (press F4 key) for freezing of cell (e,g, $A63)",data handl excel,"['data', 'handl', 'excel']",Data Handling in excel
257,"Sections in excel file (top to bottom)

1. Menu bar

2. Standard toolbar

3. Formula bar (including Name box)

4. Headings (A,B,C…)

5. Spreadsheet

6. Sheet tabs

7. Status bar",section excel file top bottom,"['section', 'excel', 'file', 'top', 'bottom']",Sections in excel file (top to bottom)
258,"Menu Bar Tabs in excel

1. Home tab 

i. Font alignment, type, size, colour

ii. Cell colour

iii. Merge Cells

iv. Hide columns and rows (cell> format)

v. Adjusting row height and column width  (cell> format)

vi. Clear format

2. Insert tab 

i. Pictures or logo

ii. Hyperlinks (mainly used for multiple worksheets)

3 Page Layout tab

4. Formula (Functions) tab

i. Logical (AND, OR, IF, NOT etc.)

ii. Date & Time

iii. Text (Left, Right, Concatenate(&), Find, Text to Column)

iv. Lookups (VLOOKUP& HLOOKUP)

-VLOOKUP returns value after a search from data range that is set up vertically.

VLOOKUP(search_by_value_as _row_index, A1:B4, required_value_column_index, range_lookup)

v. Math

vi. More Functions (Financial, Statistical, Engineering and more)

vii. Formula Auditing> Error Checking> circular ref, Data>edit links, 
IFERROR
evaluate formula, 
trace dependent etc.

5. Data tab 

i. Grouping Rows or Columns

6. Review tab

7. View tab

i. Freeze panes

ii. Gridlines
",menu bar tab excel,"['menu', 'bar', 'tab', 'excel']",Menu Bar Tabs in excel
259,"Data types in excel

It is very important to check the data types in excel to avoid mistakes

General,

Date,

Number,

Time,

Text,

Percentage
",data type excel,"['data', 'type', 'excel']",Data types in excel
260,"To view the groupby statistics like pandas df in excel

> First hide the not necessary columns or group the columns 

> Then sort the values of the coulmns on which groupby operation to be performed

> Then click on Data tab> Outline> Subtotal and use functions like sum, average, count, min, max etc. in the required column

> Mode statistic in pivot table is available in powerpivot of office 2013. But mode can be calculated through mode() function in lower version

> Then copy different  subtotal statistics and paste in separate worksheet (Select the required rows and columns and then press Alt+; for selecting the visible cells only) for creating the dashboard
 
> PivotTable is better option than Subtotal for creating the dashboard",view groupbi statist like panda df excel,"['view', 'groupbi', 'statist', 'like', 'panda', 'df', 'excel']",To view the groupby statistics like pandas df in excel
261,"Filter in excel

(Home/Data tab)

1. Filter

> The most important function for data analysis and experiment

> We can filter data by custom filter

> We can filter data by font colour or cell colour

> Shortcut for selecting the filter: Alt+down arrow

Few other keyboard shortcuts

> When doing any arithmatic operation with the filtered cells, we need to select the cells and then press ALT+;

> selecting entire column: Ctrl+space

> Protect sheet: Alt+T+P+P

> Sheet change: Ctrl+Pgdn

> Hiding entire row: Ctrl+9

> Unhide row: Ctrl + Shift + 9

> Hiding column: Ctrl+0

> Getting to cell A1: Ctrl+home

> To copy cell contents using drag and drop press the Ctrl key",filter excel,"['filter', 'excel']",Filter in excel
262,"Conditional Formatting in excel

(Home tab)

2. Conditional Formatting 

> This is for automatic colour coding, bar coding or icon coding of the data 

> It helps in data analysis by  highlighting cells with certain rule like certain text or duplicate values",condit format excel,"['condit', 'format', 'excel']",Conditional Formatting in excel
263,"Sorting in excel

(Home/ Data tab)

3. Sorting

> Like filter, sorting can also be done by both values and colour (text or cell)

> Sorting can also be done for multiple levels (go to add level option in the sort dialog box)

> Sorting may be done for rows means for column headings",sort excel,"['sort', 'excel']",Sorting in excel
264,"Removing Duplicates in excel

(Data tab)

4. Remove Dups

> Directly removes duplicate observation with one column or multiple columns",remov duplic excel,"['remov', 'duplic', 'excel']",Removing Duplicates in excel
265,"Pivot and Slicers in excel

(Insert tab)

5. Pivot and Slicers

> Pivot is a way to summarize data

> It helps us to create a summary table playing with Row labels, Column labels, Filters and Summarizing Values

> We can copy-paste the  summary table multiple times and create different combinations of results

> By right click on values we can change the summarizing data type (sum,count,avg,max,min,etc.)

> For pivot table, excel shows option tab in which we can do more arrangement of the summary table

> We can show additional calculated column (option tab> formulas> calculated field) if required

> If we double click on the summarized value, excel shows the exact data for that summarization on different worksheet

> We can also do groupby or other operations from other tabs apart from pivot table option tab if required

Pivot Slicer

> In pivot table option tab, we have slicer (a type of filter which operates from outside the table). We can connect multiple pivot tables in a single slicer.

Slicers is not available in Excel 2007.",pivot slicer excel,"['pivot', 'slicer', 'excel']",Pivot and Slicers in excel
266,"Refreshing all pivot tables

How do we refresh data in all pivot tables or entire worksheet at a time

> Pivot table  options> Refresh all and pivot chart analyze> Refresh all",refresh pivot tabl,"['refresh', 'pivot', 'tabl']",Refreshing all pivot tables
267,"Charts in excel

(Insert tab)

6. Charts

> After selecting the table, we can insert chart of our choice

> On getting the chart, we can select the chart and will get design, layout and format tabs for the chart

> In layout tab> analysis> Trendline

- we have More trendline options which allows us to adjust the trendline and see the equation of the trend line on the chart

>> For comparing values over categories Column Chart (vertical bar graph) is useful

>> To track the progress of the stock market on a daily basis or to track or compare performance of employees in a year, Line chart is useful",chart excel,"['chart', 'excel']",Charts in excel
268,"Dashboard in excel

> Dashboard is the panel facing the driver of a vehicle or the pilot of an aircraft, containing instruments and controls

> A data dashboard is an information management tool that visually tracks, analyzes and displays key performance indicators (KPI), metrics and key data points to monitor the health of a business, department or specific process

> Data dashboard is created with pivot table, slicer, additional tables and charts, title, legend

> At first we need to do all data crunching in python and create a clean csv or excel. 

> If management want the visualization (dashboard) in excel, then we do all  onward excel operations.

> If the management want the dashboard in Tableau (or Power BI), then we can connect the clean excel file in Tableau (or Power BI) for onward operations.",dashboard excel,"['dashboard', 'excel']",Dashboard in excel
269,"Waterfall or birdge graph analysis

> This graph shows, how things have changed between two periods(or two states) 

> Insert a stacked column chart and adjust the invisible colour",waterfal birdg graph analysi,"['waterfal', 'birdg', 'graph', 'analysi']",Waterfall or birdge graph analysis
270,"Excel functions

COUNT(B2:C8) function calculates the number of cells that have numeric entries

The COUNTA(H2:O16) Function is categorized under Excel Statistical functions. It will calculate the number of cells that have numeric or text entries (i.e not blank)

LEN(H2) returns the string lenth of the cell

IF(LEN(H2)>50,1,0) # returns 1 if string length is greater than 50 elase 0

INDEX(A1:B5,2,2) function returns the value at a given location in a range or array (table ) by index (here row index and column index starts from 1)-// This actually returns value in B2

MATCH(41,B2:B5,0) function returns the index after exact matching of the value with the given range (vertical or horizontal)

-MATCH function is similar like VLOOKUP or HLOOKUP but match function returns index and VLOOKUP or HLOOKUP returns value

=A1=B1 returns True or False

ROUND(235.415, -1) returns 240

ROUND(235.415, -2) ruturns 200

ROUND(235.415, 2) returns 235.42

ROUND(235.415, 0) or ROUNDDOWN(235.415, 0) returns 235

> Positive number (1,2) indicates digits after the decimal point and negative number (-1,-2) indicates digits before 0 point (unit place or one's place of the number)

0,1,2,-1,-2 etc indicates the rounding location in ROUND function",excel function,"['excel', 'function']",Excel functions
271,"Concatenation of text in excel

using Ampersand (&)

= A5&A6

or using CONCATENATE(A5,A6)

>> Function To insert current date and time

now()",concaten text excel,"['concaten', 'text', 'excel']",Concatenation of text in excel
272,"label value

A numeric value can be treated as label value if Apostrophe (‘) precedes it.

Microsoft Excel uses the table function to calculate the results in the data table.",label valu,"['label', 'valu']",label value
273,"Tableau basics

Tableau is a data visualization software and also a business intelligence (BI) software (may also be used for data crunching)

By definition, Tableau displays measures over time as a Line.

Alternative is Power BI (by microsoft)

Connect Tableau with static data (excel, text etc.) or dynamic data (SQL server)

Like worksheets in excel, Tableau has sheets and Dashboard. In one sheet there can be one visualization. In Dashboard we can have multiple visualization.  

Tableau operation is similar like pivot chart operation in excel

> Disaggregation returns all records in the underlying data source.

> The symbol related with the field that has been assembled is a 
Paper Clasp (An orange check mark indicates that the data source is the secondary data source in the workbook)

> Cell Size Option In Format Menu is TO CUSTOMIZE THE SIZE OF THE CELLS DISPLAYING THE DATA

> We can publish the workbook in the server whenever required",tableau basic,"['tableau', 'basic']",Tableau basics
274,"Best practices for visualization

> Keep things simple and digestible (more important than beauty)

> There must be a meaningful association with colour

> In Bar graphs, size is more iterpretable than circular graphs

> Attention map for visualization: center is the most emphasized one, top left is the second emphasized part, bottom right is least emphasized one

In visualization we can play with following parameters

> Colour

> Size

> labels

> Tooltip

> Details",best practic visual,"['best', 'practic', 'visual']",Best practices for visualization
275,"Panes of Tableau

In Tableau, There are two panes in the left side:

1. Data

2. Analysis

under data pane there are:

i. Dimensions (column or variable names)

ii. Measures

iii. Calculated fields

iv. Sets (Sets are custom fields based on existing dimensions and criteria that we specify)

v. Parameters

>> Data pane fields are as per sql schema when connected to sql server

under analysis pane there are:

i. Summarize (constant line, avg. line, median and quartile)

ii. Model (Trend line, forecast)

> Maximum of 32 tables can be join in tableau",pane tableau,"['pane', 'tableau']",Panes of Tableau
276,"Trend line models in Tableau

Tableau provides users with five trend line models: 

Linear, 

Logarithmic, 

Exponential, 

Polynomial and 

Power.",trend line model tableau,"['trend', 'line', 'model', 'tableau']",Trend line models in Tableau
277,"Creating Calculated field in Tableau

> Select “Analysis” present in the menu bar.

> Select “Create Calculated Field” from the list.

> Then calculated Field window will open. Name it.

> Type the estimated value of the measure.

For creating variable size bins, we use Calculated fields (In the Data pane, right-click a measure and select Create > Bins)",creat calcul field tableau,"['creat', 'calcul', 'field', 'tableau']",Creating Calculated field in Tableau
278,"Chart area of Tableau

In Tableau, the chart area is divided into grid (helps to show multiple dimensions and multiple measures in a single chart). The columns of the grid shows the dimensions and the rows of the grid shows the measures. (can be reversed)

On dragging dimension to columns and measure to rows, Tableau automatically creates a bar graph (vertical).

Click on Show Me button (top right corner) to change the visualization


>> As an alternative to Dashboard, we can create tableau stories

>> We can drag and drop a variable in the Detail Mark or any of the marks, whenever detailing on visualization is required.

>> We can disable or enable the highlighting action for the workbook or sheets from the toolbar.",chart area tableau,"['chart', 'area', 'tableau']",Chart area of Tableau
279,"Chart types in Tableau

1. Area Chart

2. Bar Chart

3. Box and Whisker Plots

4. Bullet Chart (like horizontal bar garph, also called projectile chart)-This is to compare multiple measures for a single category

5. Scatter Plot

6. Pie Chart

7. Bubble Chart

8. Line Chart

> The chart wizard term data series refers to a collection of chart data markers",chart type tableau,"['chart', 'type', 'tableau']",Chart types in Tableau
280,"Tableau Pills

The fields drag and dropped in the columns and rows look like pills and so they are generally called pills

> Pill colors

Most of the dimensions we use are blue and most of the measures are green. 

Blue pills however in fact correspond to discrete data and green pills to continuous data. If we see a red pill at any point, it means there is an error of some kind – a dimension/measure is missing or the calculation we did hasn't been recognised.

e.g. 

Discrete Dimensions: Product Name

Continuous Dimensions: Year(order date)

Discrete Measures: Sum(discrete profit)

Continuous Measures:  Sum(continuous profit)",tableau pill,"['tableau', 'pill']",Tableau Pills
281,"Tableau Desktop applications

> Tableau Desktop 

> Tableau Prep

> Tableau Server

 > Tableau Online 

> Tableau Reader

> Tableau Public

> Tableau Viewer

> Tableau Explorer",tableau desktop applic,"['tableau', 'desktop', 'applic']",Tableau Desktop applications
282,"Components of a Dashboard

1. Horizontal bar graph

2. Vertical bar graph

3. Image Extract",compon dashboard,"['compon', 'dashboard']",Components of a Dashboard
283,"COUNTD function

COUNTD function is used for displaying a distinct or unique value of the dimension. It will count the distinct value of the number of items in a group and will display it. It will ignore NULL values.",countd function,"['countd', 'function']",COUNTD function
284,"Reference line and Reference band

Say we are analyzing the monthly sales for several products, we can include a reference line at the average sales mark so we can see how each product performed against the average.

A Reference Band can be based on two fixed points.",refer line refer band,"['refer', 'line', 'refer', 'band']",Reference line and Reference band
285,"File extensions in Tableau

Tableau Workbook (.twb)

Tableau Packaged Workbook (.twbx)

Tableau Data Source(.tds)",file extens tableau,"['file', 'extens', 'tableau']",File extensions in Tableau
286,"Filters in Tableau

Filters are very helpful to create dashboards in Tableau. Filters can help to minimize the size of data sets for efficient use, eliminate irrelevant dimension elements, clean up underlying data, set date ranges and measures as required, simplify and organize data, etc.

There are several different kinds of filters in Tableau and they get executed in the following order from top to bottom

1. Extract filters

2. Data Source filters

3. Context filters

4. Dimension filters

5. Measure filters

6. Table Calc filters",filter tableau,"['filter', 'tableau']",Filters in Tableau
287,"Data blending 

Data blending is a method for combining data from multiple sources.

Blends, unlike relationships or joins, never truly combine the data. Instead, blends query each data source independently, the results are aggregated to the appropriate level, then the results are presented visually together in the view.

> We can perform all kinds of joins using data blending

> Blending uses Left join by default",data blend,"['data', 'blend']",Data blending 
288,"Data Types in Tableau

> String

> Numeric 

> Date and Time

> Boolean 

> Geographic

> Cluster or Mixed ",data type tableau,"['data', 'type', 'tableau']",Data Types in Tableau
289,"KPI Basics

A popular saying in organizations is “what gets measured gets managed“. -by management guru Peter Drucker

Don’t just Measure. Measure what matters. Therefore we use KPI

KPI stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. 

There are KPIs specific to marketing, development, and support etc.",kpi basic,"['kpi', 'basic']",KPI Basics
290,"KPI vs Metric

While key performance indicators and metrics are related, they’re not the same. 

Metrics are not the most critical measures. Some examples include “monthly store visits” or “white paper downloads”.

To build business intelligence, we should track both strategic key performance indicators and tactical metrics.  ",kpi vs metric,"['kpi', 'vs', 'metric']",KPI vs Metric
291,"Need of KPI for the company

1. we Can Measure our Targets

2. Create an Atmosphere of Learning

3. Receive Important Information

4. Encourage Accountability

5. Boost Morale

6. Keep our teams aligned

7. Provide a health check

8. Make adjustments

9. Hold our teams accountable",need kpi compani,"['need', 'kpi', 'compani']",Need of KPI for the company
292,"Types of Indicators

1. Quantitative indicators

2. Qualitative indicators

3. Leading indicators

4. Lagging indicators

5. Input indicators

6. Process indicators

7. Output indicators

8. Practical indicators

9. Directional indicators

10. Actionable indicators

11. Financial indicators",type indic,"['type', 'indic']",Types of Indicators
293,"Effectiveness and Efficiency

Effectiveness = atual output/ expected output

It is the comparison between actual output and expected output

Efficiency = output (expected cost of product)/ input (actual cost of resource consumed)

It is the comparison between  actual output and actual input

> In our real life, if we maximize the expectation equal to the input (wherever possible), then effectiveness improves and becomes equal to efficiency (e.g. we may expect to score 100 out of 100 in any exam)",effect effici,"['effect', 'effici']",Effectiveness and Efficiency
294,"Ways to develop KPI

1. Define how KPIs will be used

2. Tie them to strategic goals

3. Write SMART KPIs

4. Keep them clear-cut

5. Plan to iterate

6. Avoid KPI overload",way develop kpi,"['way', 'develop', 'kpi']",Ways to develop KPI
295,"Three steps to a stronger KPI strategy

1. Select KPIs that matter most

2. Create a KPI-driven culture

3. Iterate
",three step stronger kpi strategi,"['three', 'step', 'stronger', 'kpi', 'strategi']",Three steps to a stronger KPI strategy
296,"Key Performance Indicators in practice

1. Result indicators – what have we done?

2. Key result indicators – how well have we done?

3. Performance indicators – what shall be done?

4. Key Performance indicators – what needs to be done?

> Performance Measures  is an indicator of the measurement of success in any organization

> On-Time To Promise is a KPI measure (the percentage of time we're successful at delivering the product when we promised our customers we would deliver it.)

> In Industrial performance, resource means Material resources, Human resources and Financial resources",key perform indic practic,"['key', 'perform', 'indic', 'practic']",Key Performance Indicators in practice
297,"Executive Dashboard 

An Executive Dashboard is a reporting tool that provides a visual display of organizational KPIs, metrics, and data. The objective of executive dashboards is to give CEOs an at-a-glance visibility into business performance across all units and projects.",execut dashboard,"['execut', 'dashboard']",Executive Dashboard 
298,"Possible dangers of industrial performance indicators

1. Do not balance all the goals

2. Perform unnecessary indicators

3. Difficulty of defining objectives by service",possibl danger industri perform indic,"['possibl', 'danger', 'industri', 'perform', 'indic']",Possible dangers of industrial performance indicators
299,"SQL Basics

SQL means Structured Query Language

> In csv file there can be only one worksheet means one table

> In excel file there can be multiple worksheets means multiple tables

> Similar like excel, in .db or .tar files (SQL) there are  multiple inter-related tables (relational database) but used for very large amount of data

> Sql needs a server because it is dynamic database

> SQL statement is also called clause

> SQL is case insensitive",structur queri languag basic,"['structur', 'queri', 'languag', 'basic']",SQL Basics
300,"List of relational database

-Oracle Database 12c

-MySQL (Oracle)

-Microsoft SQL Server

-PostgreSQL

-SQLite

-DB2 (owner IBM)

-SAP HANA (owner SAP)

> many of them are open source",list relat databas,"['list', 'relat', 'databas']",List of relational database
301,"Difference between SQL and Python

SQL is a query language primarily aimed to store, manipulate and retrieve data

Python is a general-purpose programming language that enables experimentation with the data.

But SQL is faster than Python, thus used for extreamly large data",differ structur queri languag python,"['differ', 'structur', 'queri', 'languag', 'python']",Difference between SQL and Python
302,"Basics of Relational Database (RDBMS)

A relational database is a type of database that stores and provides access to data points that are related to one another (from inter-related tables).

 The relationship between tables and field types is called a schema. Schema is a logical collection of database objects

> The columns of the table hold attributes of the data, and each record usually has a value for each attribute.

> When the values of minimum one attribute of two tables are same, they are called relational tables or relational database 
e.g. 
Say two tables contains one same attribute (types of job) with same values (service,business), then the two tables are relational",basic relat databas rdbms,"['basic', 'relat', 'databas', 'rdbms']",Basics of Relational Database (RDBMS)
303,"Basics of Non-Relational Database

A non-relational database is any database that does not use the tabular schema of rows and columns like in relational databases. Rather, its storage model is optimized for the type of data it’s storing

For extreamly large dataset non-relational databases (ms word is a simple non-relational database for static data) are used.

NoSQL databases

1. Document-oriented databases

2. Key-Value Stores

3. Wide-Column Stores

4. Graph Stores

-MongoDB (non relational database or NoSQL-semi structured data)",basic nonrel databas,"['basic', 'nonrel', 'databas']",Basics of Non-Relational Database
304,"Parts of SQL

1) DDL - Data Definition Language

2) DML - Data Manipulation Language

3) DCL - Data Control Language (View Definition)

4) TCL - Transaction Control Language

> Group of operations that form a single logical unit of work is known as Transaction",part structur queri languag,"['part', 'structur', 'queri', 'languag']",Parts of SQL
305,"SQL Statements

DDL statements are used to define the database structure or schema. 

1) CREATE

2) ALTER

3) DROP

4) RENAME

5) TRUNCATE

DML statements are used for managing data within schema objects. 

1) SELECT

2) INSERT

3) UPDATE

4) DELETE

5) LOCK

6) CALL

7) EXPLAIN PLAN

DCL statements

1. GRANT (to allow specified users to perform specified tasks)

2. REVOKE (to remove the user accessibility to database object)

TCL statements

1. COMMIT

2. ROLLBACK",structur queri languag statement,"['structur', 'queri', 'languag', 'statement']",SQL Statements
306,"Practicing sql queries (with static data)

> In PgAdmin schema, we can find all the table names and column names for each table. 

> But while practicing sql queries in colab, we need to see all the table names and column names of each table to understand the schema of the database

import sqlite3

connnection_engine=sqlite3.connect('.db_file_path_in_google_drive') # After mounting drive in colab

my_cursor = connnection_engine.cursor()

>  To view all the table names

my_cursor.execute(""SELECT name FROM sqlite_master WHERE type='table';"")

my_cursor.fetchall()

>  To view all the column names along with datatype of a table

my_cursor.execute(""PRAGMA table_info(table_name); "")

info_list=my_cursor.fetchall()

[k[1] for k in info_list] # To view the column names

> To check the shape of the table

my_cursor.execute(""SELECT COUNT(*) FROM table_name;"")

my_cursor.fetchall() # Returns the number of rows that match a specific condition of a query

> To see the head of the table

my_cursor.execute(""SELECT * FROM table_name LIMIT 5;"")

my_cursor.fetchall()",practic structur queri languag queri static data,"['practic', 'structur', 'queri', 'languag', 'queri', 'static', 'data']",Practicing sql queries (with static data)
307,"SQL Query/ Statements/ commands

1. Structural commands: SELECT, DISTINCT, ALL,  AS, FROM, WHERE

SELECT * FROM table_name WHERE column_name=5;

2. Comparison commands: IN, BETWEEN, LIKE, ILIKE

3. Grouping commands (aggregate functions): GROUP BY, HAVING, COUNT(), SUM(), AVG(), MAX(), MIN(), ROUND()

SELECT COUNT(DISTINCT column_name) FROM table_name;

4. Display commands: ORDER BY, ASC/ DESC, LIMIT

""SELECT column_name FROM table_name ORDER BY column_name DESC LIMIT 10; (default for ORDER BY is ASC)

5. Logical Commands: AND, OR, NOT

6. Output commands: INTO TABLE/ CURSOR, TO SCREEN

7. Union Commands: UNION

>> HAVING and WHERE are similar operation. “WHERE” is used to filter rows but “HAVING” is used to filter groups

>> WHERE” is always used before “GROUP BY” and HAVING after “GROUP BY”",structur queri languag queri statement command,"['structur', 'queri', 'languag', 'queri', 'statement', 'command']",SQL Query/ Statements/ commands
308,"SELECT and SELECT DISTINT statements

In order to select the entire table SELECT * is used

SELECT DISTINCT column_name1, column_name2 FROM table_name;

-returns only the unique values from the columns
",select select distint statement,"['select', 'select', 'distint', 'statement']",SELECT and SELECT DISTINT statements
309,"Conditional operators

=
<
>
<=
>=
!=

SELECT city, temperature, condition FROM weather WHERE condition = 'sunny' OR condition = 'cloudy' AND temperature >= 60",condit oper,"['condit', 'oper']",Conditional operators
310,"Reading SQLite Database file (.sqlite3, .sqlite, .db) as pandas df 

my_df = pd.read_sql_query(""select * from table_name;"", connnection_engine)",read sqlite databas file sqlite3 sqlite db panda df,"['read', 'sqlite', 'databas', 'file', 'sqlite3', 'sqlite', 'db', 'panda', 'df']","Reading SQLite Database file (.sqlite3, .sqlite, .db) as pandas df "
311,"SQLite

SQLite is very lightweight and comes as already installed dbms server in colab and requires no username and password. 

Thus, SQLite is very useful for practicing SQL queries.

> We can convert sql dadabase file to .sqlite file (rebasedata.com) whenever required",sqlite,['sqlite'],SQLite
312,"File extensions

> Common database file extensions: .tar, .db, .accdb, .nsf, and .fp7

> Postgresql, mysql, mssql and other database file extension: .tar

.db- Mobile Device Database File

.accdb- Access 2007 Database File (mainly used for offline applications)

",file extens,"['file', 'extens']",File extensions
313,"LIKE and ILIKE

LIKE and ILIKE allows us to perform pattern matching against string data with the use of wildcard characters (%, _)

percentage(%)- matches any sequence of characters

underscore(_)- matches any group of characters

LIKE is case-sensitive while ILIKE is case-insensitive

> Wildcard characters are useful When an exact match is not possible in a SELECT statement

WHERE column_name LIKE A% # returns names that begins with A

WHERE column_name LIKE %a # returns names that ends with a

WHERE column_name LIKE 'any_string_' # returns names with any_string

> both the wildcard characters can also be used in combination",like ilik,"['like', 'ilik']",LIKE and ILIKE
314,"ADVANCED SQL COMMANDS

1)TIMESTAMP & EXTRACT

2)Math Functions

3)String Functions

4)Sub-query

5)Self Joins
",advanc structur queri languag command,"['advanc', 'structur', 'queri', 'languag', 'command']",ADVANCED SQL COMMANDS
315,"TIMESTAMP & EXTRACT

1)TIME : Contains only Time

2)DATE : Contains only Date

3)TIMESTAMP : Contains Time and Date

4)TIMESTAMPTZ : Contains Time, Date and Time Zone

EXTRACT : Extracts YEAR/ MONTH/ DAY/ WEEK/ QUARTER from a date_column

AGE : Calculates and returns the current age given a timestamp

TO_CHAR : Converts date types to text and is useful for formatting",timestamp extract,"['timestamp', 'extract']",TIMESTAMP & EXTRACT
316,"SUB-QUERY 

(allows us to construct complex queries, essentially performing a query on the results of another query)

>> This is one of the basic approaches for joining tables. Other approaches are Union Join and  Natural join

>> When we have a subquery inside of the main query, subquery  is executed first",subqueri,['subqueri'],SUB-QUERY 
317,"SELF JOIN

(Query in which a table is joined to itself. They are useful for comparing values in a column within the same table)

> EQUI JOIN is also called an INNER JOIN",self join,"['self', 'join']",SELF JOIN
318,"DATA/ DATA STRUCTURE TYPES IN SQL

1)Boolean : True or False

2)Character : Char, Varchar or Text

3)Numeric : Integer or Floating Point Numbers

4)Temporary :  Date, Time, Timestamp and Interval

5)UUID : Universally Unique Identifier

6)Array : Stores an array of strings or numbers

7)JSON (array like data structure)

8)Hstore : Key Value pair

9)Special : Geometrical Data or Network Address",data data structur type structur queri languag,"['data', 'data', 'structur', 'type', 'structur', 'queri', 'languag']",DATA/ DATA STRUCTURE TYPES IN SQL
319,"Primary Key and Foreign Key

1)Primary Key : Column or Group of Columns that are used to uniquely identify a row in a table. They allow us to easily discern what columns are to be used when joining tables

> We can have only one primary key in a table 

2)Foreign Key: column or group of columns that refers to the primary key/unique key of other table.

i)  Parent Table : Table to which FK references

ii) Child Table : Table that contains the FK

> It is used to establish and enforce a link between the data in two tables",primari key foreign key,"['primari', 'key', 'foreign', 'key']",Primary Key and Foreign Key
320,"CONSTRAINTS IN SQL

The rules enforced on data columns in tables and are used to prevent invalid data being entered in a table. 

Constraints can be divided into 2 types : COLUMN CONSTRAINTS and TABLE CONSTRAINTS

1)NOT NULL (enforces a column to NOT accept NULL values)

2)UNIQUE (ensures that all values in a column are different)

3)PRIMARY KEY (automatically has a UNIQUE constraint and cannot contain NULL values)

4)FOREIGN KEY (to prevent actions that would destroy links between tables)

5)CHECK

Example of CONSTRAINTS

CREATE TABLE Orders (
    OrderID int NOT NULL,
    OrderNumber int NOT NULL,
    PersonID int,
    PRIMARY KEY (OrderID),
    FOREIGN KEY (PersonID) REFERENCES Persons(PersonID)
);
",constraint structur queri languag,"['constraint', 'structur', 'queri', 'languag']",CONSTRAINTS IN SQL
321,"CREATING DATABASES & TABLES

1)CREATE

2)INSERT

3)UPDATE

4)DELETE

5)ALTER 

6)DROP

7)CHECK (CONSTRAINTS)",creat databas tabl,"['creat', 'databas', 'tabl']",CREATING DATABASES & TABLES
322,"CONDITIONAL EXPRESSIONS & PROCEDURES

1. CASE-Very similar to IF/ ELSE

GENERAL CASE : Allows us to do all kinds of conditional checks

CASE EXPRESSION : Allows us to do only certain checks

2. COALESCE

Accepts unlimited  number of arguments and returns the first argument which is not NULL

COALESCE(arg_1, arg_2......arg_n)

-substituting the NULL values with a variable

SELECT (PRICE - COALESCE(DISCOUNT,0))  AS FINAL_PRICE FROM table

3. CAST
Let’s us convert one Data Type into another.

4. NULLIF
This returns a NULL Value if the arguments inside NULLIF() are equal.

5. VIEWS
View can be accessed as a Virtual Table and it does not store the data physically. It simply stores the query. ",condit express procedur,"['condit', 'express', 'procedur']",CONDITIONAL EXPRESSIONS & PROCEDURES
323,"
Simple View and Complex View

>> There are 2 types of Views in SQL: Simple View and Complex View. Simple views can only contain a single base table. Complex views can be constructed on more than one base table",simpl view complex view,"['simpl', 'view', 'complex', 'view']","
Simple View and Complex View"
324,"Syntax for the CREATE VIEW

CREATE VIEW view_name AS SELECT columns FROM tables [WHERE conditions];

",syntax creat view,"['syntax', 'creat', 'view']",Syntax for the CREATE VIEW
325,"SQL queries in Python

Step 1: Importing SQLAlchemy, psycopg2 etc.

from sqlalchemy import create_engine

Step 2: Creating a SQL engine

engine = create_engine('mysql://user:password@server')

engine.execute(""CREATE DATABASE dbname"") #create db

engine.execute(""USE dbname"") # select new db

Step 3: Running queries using SQL statements

Step 4: Writing to DB

Step 5: Creating a Table in DB",structur queri languag queri python,"['structur', 'queri', 'languag', 'queri', 'python']",SQL queries in Python
326,"> First install postgreesql and then pgadmin (needs windows 64 bit)

set Port: 5432
set password:  prabir
user: postgres (default)

> Open pgadmin

> load the .tar database

> Then we can see different tables names under schemas

> Now we can write queries to check all the tables 
",first instal postgreesql pgadmin need window 64 bit,"['first', 'instal', 'postgreesql', 'pgadmin', 'need', 'window', '64', 'bit']",> First install postgreesql and then pgadmin (needs windows 64 bit)
327,"Ways to see the files inside a .tar or .tar.gz file (zip file) without uncompressing by winzip

import tarfile

tar = tarfile.open('.tar file path')

file_names=tar.getnames() 

# most of the files are actually different relational tables in .dat format (.dat files can only be accessed by the application that created them)",way see file insid tar targz file zip file without uncompress winzip,"['way', 'see', 'file', 'insid', 'tar', 'targz', 'file', 'zip', 'file', 'without', 'uncompress', 'winzip']",Ways to see the files inside a .tar or .tar.gz file (zip file) without uncompressing by winzip
328,"SQL aliases

SQL aliases are used to give a table, or a column in a table, a temporary name. Aliases are often used to make column names more readable. 

Syntax

SELECT column_name AS alias_name FROM table_name;

Example

SELECT CustomerID AS ID, CustomerName AS Customer FROM Customers;",structur queri languag alias,"['structur', 'queri', 'languag', 'alias']",SQL aliases
329,"Different Types of SQL JOINs

(INNER) JOIN: Returns records that have matching values in both tables

LEFT (OUTER) JOIN: Returns all records from the left table, and the matched records from the right table

RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched records from the left table

FULL (OUTER) JOIN: Returns all records when there is a match in either left or right table

e.g.

SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID;",differ type structur queri languag join,"['differ', 'type', 'structur', 'queri', 'languag', 'join']",Different Types of SQL JOINs
330,"MOD() function

> MOD() function in MySQL is used to find the remainder of one number divided by another.",mod function,"['mod', 'function']",MOD() function
331,"CREATE DATABASE, DROP DATABASE and BACKUP DATABASE

CREATE DATABASE Syntax

CREATE DATABASE databasename;

DROP DATABASE Syntax

DROP DATABASE databasename;

BACKUP DATABASE Syntax

BACKUP DATABASE databasename TO DISK = 'filepath';",creat databas drop databas backup databas,"['creat', 'databas', 'drop', 'databas', 'backup', 'databas']","CREATE DATABASE, DROP DATABASE and BACKUP DATABASE"
332,"CREATE TABLE in SQL

CREATE TABLE table_name (
    column1 datatype,
    column2 datatype,
    column3 datatype,
   ....
);

Example

CREATE TABLE Persons (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);",creat tabl structur queri languag,"['creat', 'tabl', 'structur', 'queri', 'languag']",CREATE TABLE in SQL
333,"DELETE in SQL

DELETE FROM table_name WHERE condition;

Example

DELETE FROM Customers WHERE CustomerName='Alfreds Futterkiste';",delet structur queri languag,"['delet', 'structur', 'queri', 'languag']",DELETE in SQL
334,"SQL TRUNCATE TABLE 

In SQL TRUNCATE TABLE statement always first drops, then re-creates a new table.  It removes all rows from a table without logging the individual row deletions

DROP TABLE command deletes complete table but it would remove complete table structure form the database and we would need to re-create this table once again if we wish we store some data.

> DELETE command can be rolled back but TRUNCATE and DROP command cannot be rolled back

> TRUNCATE is usually faster than DELETE command",structur queri languag truncat tabl,"['structur', 'queri', 'languag', 'truncat', 'tabl']",SQL TRUNCATE TABLE 
335,"SQL ALTER TABLE 

The SQL ALTER TABLE clause modifies a table definition by altering, adding, or deleting table columns and/or constraints

1. To add a column in a table

ALTER TABLE table_name ADD column_name datatype;

2. To delete a column in a table

ALTER TABLE table_name DROP COLUMN column_name;

3. To change the data type of a column in a table

ALTER TABLE table_name ALTER COLUMN column_name datatype;

> In place of ALTER COLUMN, MODIFY may be used",structur queri languag alter tabl,"['structur', 'queri', 'languag', 'alter', 'tabl']",SQL ALTER TABLE 
336,"SQL string datatypes

> CHAR, 

> VARCHAR, 

> BINARY, 

> VARBINARY, 

> BLOB, 

> TEXT, 

> ENUM, and 

> SET",structur queri languag string datatyp,"['structur', 'queri', 'languag', 'string', 'datatyp']",SQL string datatypes
337,"INSERT INTO in SQL

INSERT INTO table_name (column1, column2, column3, ...) VALUES (value1, value2, value3, ...);

INSERT INTO table_name VALUES (value1, value2, value3, ...); # When inserting values for all the columns",insert structur queri languag,"['insert', 'structur', 'queri', 'languag']",INSERT INTO in SQL
338,"UPDATE in SQL

UPDATE table_name SET column1 = value1, column2 = value2, … WHERE condition;

Example

UPDATE Customers SET ContactName = 'Alfred Schmidt', City= 'Frankfurt' WHERE CustomerID = 1;

> we can UPDATE a single table at a time
",updat structur queri languag,"['updat', 'structur', 'queri', 'languag']",UPDATE in SQL
339,"SQL CHECK 

SQL CHECK can be used along with CREATE, ALTER, DROP commands

CHECK on CREATE TABLE

CREATE TABLE Persons (
    ID int NOT NULL,
    LastName varchar(255) NOT NULL,
    FirstName varchar(255),
    Age int CHECK (Age>=18)
);

CHECK on ALTER TABLE

ALTER TABLE Persons ADD CHECK (Age>=18);
",structur queri languag check,"['structur', 'queri', 'languag', 'check']",SQL CHECK 
340,"UNION in SQL

SELECT column_name(s) FROM table1 UNION SELECT column_name(s) FROM table2;

UNION ALL

SELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2;

> SELECT on a MERGE table is like UNION ALL 

MERGE() is the combination of three statements (INSERT, DELETE and UPDATE)

In MERGE, we specify a ""Source"" record set and a ""Target"" table and the JOIN condition between the two.",union structur queri languag,"['union', 'structur', 'queri', 'languag']",UNION in SQL
341,"SQL TIMESTAMP and DATETIME 

SQL TIMESTAMP and DATETIME both store without time zone.

TIMESTAMP is stored in UTC (Coordinated Universal Time) values, and DATETIME is stored in without time zone.

UTC is effectively a successor to Greenwich Mean Time (GMT).

Both the data type can be used for values that contain both date and time parts.

In ' YYYY-MM-DD hh:mm:ss ' format, the supported range DATETIME is '1000-01-01 00:00:00' to '9999-12-31 23:59:59' . 

 TIMESTAMP has a range of '1970-01-01 00:00:01' UTC to '2038-01-19 03:14:07' UTC.",structur queri languag timestamp datetim,"['structur', 'queri', 'languag', 'timestamp', 'datetim']",SQL TIMESTAMP and DATETIME 
342,"SQL trigger

SHOW TRIGGERS; returns a list of triggers in the current database

A SQL trigger is a database object which fires when an event occurs in a database. It consist of procedural code (includes procedural and SQL statements)

For example, a trigger can be set on a record insert in a database table.",structur queri languag trigger,"['structur', 'queri', 'languag', 'trigger']",SQL trigger
343,"SQL NOT IN

NOT IN operator can be used with a multiple-row subquery
",structur queri languag,"['structur', 'queri', 'languag']",SQL NOT IN
344,"Operation with Null values in SQL

Null + 1 = Null
Null * 2 = Null

> IS NULL operator is used to compare the NULL values in SQL",oper null valu structur queri languag,"['oper', 'null', 'valu', 'structur', 'queri', 'languag']",Operation with Null values in SQL
345,"SQL CONCAT

In SQL, the CONCAT() takes 2 to 255 input strings and joins them into one

SELECT CONCAT('W3Schools', '.com');

To convert 212-555-1212 to ***-555-1212 where 212-555-1212 is a data from the column named ""phone""

concat(left(phone,3),'***')",structur queri languag concat,"['structur', 'queri', 'languag', 'concat']",SQL CONCAT
346,"SQL and String

String  index starts from 1 in SQL

INSTR ('ALMAX POINT', 'P') returns 7 as the index of 'P' is 7 in the given string

LEFT function allows us to extract a substring from a string, starting from the left-most character.

Syntax

LEFT(string_value , number_of_characters_to_be_returned)",structur queri languag string,"['structur', 'queri', 'languag', 'string']",SQL and String
347,"Temporary table and Heap table

In SQL, a temporary table is a base table that is not stored in the database, but instead exists only while the database session in which it was created is active.

Heap tables are tables without a Clustered Index. In a heap table, the data is not sorted in any way, it's just a pile of unordered, unstructured records",temporari tabl heap tabl,"['temporari', 'tabl', 'heap', 'tabl']",Temporary table and Heap table
348,"Sequence in SQL

A sequence in SQL can generate a maximum 38 digits number

Sequence can generate Numeric value or Alphanumeric value",sequenc structur queri languag,"['sequenc', 'structur', 'queri', 'languag']",Sequence in SQL
349,"In SQL When we need an exact copy of a table with all columns and indexes

SHOW CREATE TABLE tablename

",structur queri languag need exact copi tabl column index,"['structur', 'queri', 'languag', 'need', 'exact', 'copi', 'tabl', 'column', 'index']",In SQL When we need an exact copy of a table with all columns and indexes
350,"Primary Key, Super Key, and Candidate Key

I. Minimal super key is a candidate key

II. Only one candidate key can be a primary key

Say, we have a column ‘A’ in table1 which is a primary key and it refers to column ‘B’ in table2. Further, column ‘A’ has only three values (1,2,3). Then

> Inserting a value 4 in column ‘B’ of table2 will result in an error

> Inserting a value 4 in column ‘A’ of table1 will be successful",primari key super key candid key,"['primari', 'key', 'super', 'key', 'candid', 'key']","Primary Key, Super Key, and Candidate Key"
351,"Main branches of pure mathematics

1. Arithmetic (It deals with numbers and the basic operations are +-*/)

2. Algebra (It is a kind of arithmetic where we use unknown quantities along with numbers)

3. Geometry (It is the most practical branch of mathematics that deals with shapes and sizes of figures and their properties)

4. Trigonometry (It is the study of relationships between angles and sides of triangles)

>  Sides of a right triangle are Base, perpendicular and hypotenuse

5. Calculus (It is the branch that deals with the study of the rate of change in different quantities. Calculus forms the base of analysis)

6. Statistics and Probability (Statistics is all about understanding a data or dataset based on different measurements or scores. Whereas probability is all about chance of any future event.)",main branch pure mathemat,"['main', 'branch', 'pure', 'mathemat']",Main branches of pure mathematics
352,"Application of mathematics in machine learning

""What gets measured gets managed""

>> From the knowledge of arithmetic, we measure different feature values of different observations of any experience and store in a dataset or table

>> From the knowledge of statistics, we understand the dataset through different scores of the features

>> From the knowledge of probability, we understand the dataset through probability distributions

>> From the knowledge of algebra, we form different equations and find the values of the dependent variable

>> From the knowledge of calculus, we analyse the dataset with respect to different feature variables. It is also used (in GD) to find values of the dependent variable for large dataset

>> From the knowledge of geometry and trigonometry, we understand the dataset and calculations through visualization

>> Finally, from the knowledge of vector algebra and matrix algebra, we understand any data as a point in the vector space. Thus handling and use of data becomes simple and efficient.",applic mathemat machin learn,"['applic', 'mathemat', 'machin', 'learn']",Application of mathematics in machine learning
353,"Radian measure

> Calculus is always done in radian measure. Radians make it possible to relate a linear measure and an angle measure.

> Radian is a unit of measurement of angles equal to about 57.3° and 
π (3.14) radians = 180°

> π is the is the ratio of the circumference of any circle to the diameter of that circle and the value of π is 3.14",radian measur,"['radian', 'measur']",Radian measure
354,"Machine Learning Use Cases of Calculus

1. Numerical Optimization

2. Gradient Computations

3. Probability Density Functions

4. Variational Inference and Related Techniques ",machin learn use case calculus,"['machin', 'learn', 'use', 'case', 'calculus']",Machine Learning Use Cases of Calculus
355,"Basics of derivative 

Derivative means,

> Instantaneous rate of change (in Physics)

> Slope of a line at a specific point (in Geometry)

If a function is differentiable, it must it be continuous

Derivative of y = f(x) with respect to x is represented as dy/dx or f’(x)

A function is differentiable at a < c < b if and only if the left hand derivatives (LHD-slope calculated at the left side of the point)  and right hand derivatives (RHD- slope calculated at the right side of the point) at c both exist and are equal",basic deriv,"['basic', 'deriv']",Basics of derivative 
356,"The chain rule

The chain rule is a formula for calculating the derivatives of composite functions (functions composed of functions inside another function(s))

f(x) = h(g(x))
df/dx  = df/dg*dg/dx",chain rule,"['chain', 'rule']",The chain rule
357,"Point of maxima and point of minima

In a smoothly changing function, a maximum or minimum is always where the function flattens out (except for a saddle point). Where the slope is zero.

f'(x)=0
=>x= x0 , x1

The second derivative will tell us!

If f’’(x0) < 0 => x0 is point of maxima

If f’’(x0) > 0 => x0 is point of minima",point maxima point minima,"['point', 'maxima', 'point', 'minima']",Point of maxima and point of minima
358,"Partial Derivatives

Let’s take an example: 
z = f(x, y)

If we change x, but hold all other variables constant, how does f(x, y) change? That’s one partial derivative. 

The next variable is y. If we change y but hold x constant, how does f(x, y) change?",partial deriv,"['partial', 'deriv']",Partial Derivatives
359,"Ways to find the slope of f(x,y)

slope of f(x) is a line but slope of z=f(x,y) will be a plane

slope of a plane is represented by a vector 

<dz/dx, dz/dy>",way find slope fxi,"['way', 'find', 'slope', 'fxi']","Ways to find the slope of f(x,y)"
360,"Jacobian Matrix

Jacobian matrices are used to transform the infinitesimal vectors from one coordinate system to another. We will mostly be interested in the Jacobian matrices that allow transformation from the Cartesian to a different coordinate system.

The determinant of the jacobian matrix is called jacobian",jacobian matrix,"['jacobian', 'matrix']",Jacobian Matrix
361,"Derivatives Formulas 

f(x)= x^n, 

f’(x)= nx^(n-1)

f(x)= e^g(x)

f’(x)= e^g(x)* g'(x)

f(x)= m(x)*g(x)

f’(x)= m(x)*g'(x)+ m'(x)*g(x)

f(x)=  ln(x)

f’(x)= 1/x

f(x)=  ln(g(x))

f’(x)= 1/g(x)*g'(x)

f(x)=  x^x

f’(x)= x^x*(1+ln(x))

f(x)= sin(x)

f'(x) = cos(x)

f(x)= tan(x)

f'(x) = sec2(x)

f(x)= cot(x)=1/tan(x)

f'(x) = -cosec2(x)",deriv formula,"['deriv', 'formula']",Derivatives Formulas 
362,"Definite Integrals

A definite integral is the area under a curve between two fixed limits. 

The integral of f(x) corresponds to the computation of the area under the graph of f(x).",definit integr,"['definit', 'integr']",Definite Integrals
363,"Formulas for integration

∫x^n dx =x^(n+1)/(n+1) + C

∫e^g(x)dx = e^g(x)*1/g'(x) + C

∫m(x)*g(x) dx = m(x)*∫g(x)dx -∫g(x)*m'(x)dx + C

∫(1/x) dx = ln(x) + C

∫sin(x)dx = - cos(x) + C",formula integr,"['formula', 'integr']",Formulas for integration
364,"Basics of Limit

Limit is used to solve a function for a particular value where it becomes indeterminate (0/0, 1/0, ∞/∞)

e.g. f(x)=(x^2-4)/(x-2) is indeterminate at x=2

Then we need to find Lim f(x) at x approaches 2

> x can tend to 2 from two sides : positive higher values or from negative higher values",basic limit,"['basic', 'limit']",Basics of Limit
365,"Solution method of limit of indeterminate form

1. Factorization method

In factorization method, we convert indeterminate form to determinate form by factorizing the numerator and denominator with negative limit value first

Lim f(x) at x approaches 2=(x^2-4)/(x-2)=(x-2)(x+2)/(x-2)=x+2=4

2. By L' Hospital's Rule (also known as Bernoulli's rule)
 
Differentiate the numerator and differentiate the denominator and then take the limit.

Lim f(x)/g(x) at x approaches a = Lim f'(x)/g'(x) at x approaches a

Lim f(x) at x approaches 2 =(x^2-4)/(x-2)=2x/1=2x=4",solut method limit indetermin form,"['solut', 'method', 'limit', 'indetermin', 'form']",Solution method of limit of indeterminate form
366," fraction

A fraction has two parts. The number on the top of the line is called the numerator. The number below the line is called the denominator.",fraction,['fraction'], fraction
367,"Sub-branches of Algebra 

Algebra is divided into different sub-branches such as
 
1. Elementary algebra 

2. Advanced algebra 

3. Abstract algebra 

4. Linear algebra and 

5. Commutative algebra
",subbranch algebra,"['subbranch', 'algebra']",Sub-branches of Algebra 
368,"Linear algebra

It concerns the linear equations for the linear functions with their representation in vector spaces (=sample spaces) and through the matrices.

To fully comprehend machine learning, linear algebra fundamentals are the essential prerequisite.

In machine learning, the majority of data is most often represented as vectors, matrices or tensors. Therefore, the machine learning heavily relies on the linear algebra.

> Numpy is a linear algebra library for python. 

> Pandas is built on Numpy through Numpy API for Data Science area.

> Sklearn is built on Numpy through Numpy API for Machine Learning area.

> TensorFlow is built on Numpy through Numpy API for deep learning area.",linear algebra,"['linear', 'algebra']",Linear algebra
369,"Scaler, Vector, Matrix and Tensor

Scaler: a single number.

A vector is a 1D array: a list of numbers

A matrix is a 2D array of numbers, that has a fixed number of rows and columns: a list of lists or a list of vectors or a list of experiences like Pandas DataFrame 

The term tensor is generally used for >2D array. But actually tensor can be of 1 to n dmensions.

A tensor of dimension one is a vector. 

A vector <1,2,3...,n> can be called a matrix because a list n elements may be represented as a list of n number of lists each having one element [[1],[2],[3],...[n]]

A Tensor of Dimension three may be flattened to dimension two and then it can again be flattened to dimension one. 

Thus tensor and array are same.

> An array can be of one dimension to n dimensions",scaler vector matrix tensor,"['scaler', 'vector', 'matrix', 'tensor']","Scaler, Vector, Matrix and Tensor"
370,"Dimension and Direction

Dimension is the mathematical structure for storing or plotting data. This structure can be of one dimensional to n dimensional. 

Whereas direction is the movement of an experience in that structure or environment.

> With two dimensions we can create four(2^2) spaces (x,y), (x,-y), (-x,y), (-x,-y)

> With three dimensions we can create eight(2^3) spaces (x,y,z), (x,-y,z), (-x,y,z), (-x,-y,z), (x,y,-z), (x,-y,-z), (-x,y,-z), (-x,-y,-z) ",dimens direct,"['dimens', 'direct']",Dimension and Direction
371,"position vector/ location vector/ radius vector

The magnitude of movement of an experience in any direction is the scalar quantity or numbers.

A vector is a mathematical quantity with both magnitude and direction. 

> In geometry, a position or position vector, also known as location vector or radius vector, is a Euclidean vector that represents the position of a point P (x,y,z) in space in relation to an arbitrary reference origin

> A position vector,r may be represented as <x,y> or <rcosθ, rsinθ> 

Here r is magnitude and θ is derection of the position vector",posit vector locat vector radius vector,"['posit', 'vector', 'locat', 'vector', 'radius', 'vector']",position vector/ location vector/ radius vector
372,"Applications of Linear Algebra in Data Science

1. Coordinate Transformations

2. Linear Regression

3. Dimensionality Reduction

4. Natural Language Processing

5. Computer Vision

6. Network Graphs etc.",applic linear algebra data scienc,"['applic', 'linear', 'algebra', 'data', 'scienc']",Applications of Linear Algebra in Data Science
373,"Vector operations

Vector operation makes the calculation simple and superfast. Thus we can directly perfom different operations on the column (vector) of a dataset without going for individual operation on every element using loop.

1. Scalar multiplication with vector

2. Vector Addition

If vector,
a.shape=(2,3) and b.shape=(3,1)

Then,  
> vector addition, a+b will give error because atleast one dimension (row/column ) should match

> vector addition with transpose, a+b.T will give result and shape of resultant vector will be (2,3)

3. Vector Subtraction

4. Vector dot product

5. Vector projection

6. Vector cross product

7. Vector norms",vector oper,"['vector', 'oper']",Vector operations
374,"Vector Dot Product

a.b=|a||b|cosθ, where θ is the angle between vector a and b

a.b=a1b1+a2b2+a3b3+…..+anbn, where vector a=(a1,a2,a3,..an) and vector b=(b1,b2,b3,…bn)

>> The dot product is a fundamental way we can combine two vectors. 

>> Intuitively, it tells us something about how much two vectors point in the same direction.

>> In coordinate system, as the vector i, j and k are perpendicular to each other, i.i=1, j.j=1 and k.k=1, i.j=0, j.k=0, i.k=0

If vector,
a.shape=(2,3) and b.shape=(3,1)

> Vector dot product, np.dot(a,b) will give result and the shape of the resultant vector will be (2,1). 

> Vector dot product gives the result of matrix multiplication because dot products are done between the rows of the first matrix and the columns of the second matrix.",vector dot product,"['vector', 'dot', 'product']",Vector Dot Product
375,"Vector Projection

The projection of u onto v is another vector that is parallel to v and has a length equal to what vector u's shadow would be

projection of vector a onto the vector b: 
vector p = (a·b / b·b) * b 

>> The magnitude of any vector is also called the modulus or the length of the vector.",vector project,"['vector', 'project']",Vector Projection
376,"Vector Cross product

a x b=|a||b|sinθ
The cross product of any two vectors is a vector that is perpendicular to the two vectors. It has both magnitude and direction. The magnitude of the resultant vector is equal to the area of the parallelogram, whose side lengths are equal to the magnitude of the two given vectors.

>> In coordinate system, as the unit vector i, j and k are perpendicular to each other, iXi=0, jXj=0 and kXk=0, iXj=k, jXk=i, kXi=j, j x i= -k. k x j= -i. i x k= -j.",vector cross product,"['vector', 'cross', 'product']",Vector Cross product
377,"Vector norms

L1 norm 

It is defined as the sum of magnitudes of each component

a =  ( a1 , a2 , a3 )

L1 norm of vector a  =  |a1| + |a2| + |a3|

> This is like manhatten distance (Minkowski distance with p = 1)

L2 norm 

It is defined as the square root of sum of squares of each component 

L2 norm of vector a =  √( a1^2  + a2^2 + a3^2 )

> This is like euclidean distance (Minkowski distance with p = 2)

> L1 and L2 norms are used for minimizing the loss function",vector norm,"['vector', 'norm']",Vector norms
378,"Representing multivariable linear equation in vector space

1. Say we are considering a list type data structure with 3 numbers of integer data, i.e. [2,4,6]

2. [2,4,6] is a 1D array or a vector

3. So, we can write 

vector y= <2,4,6> or
vector y= vector2+ vector4+ vector6 or 
y=x1+x2+x3 considering x,y and z axis

4. Here the resultant vector y will be from point (0,0,0) to point (2,4,6) in x,y,z coordinate system",repres multivari linear equat vector space,"['repres', 'multivari', 'linear', 'equat', 'vector', 'space']",Representing multivariable linear equation in vector space
379,"Multivariable linear equations

y=1*x1+1*x2+1*x3, 1 is the slope of the line when x2 vector and x3 vector are zero.

y=5*x1+3*x2+4*x3, where 5 is the slope of the line when x2 vector and x3 vector are zero.

",multivari linear equat,"['multivari', 'linear', 'equat']",Multivariable linear equations
380,"Visualization of vector

Vector we can understand as straight pipeline.

The slopes 1,5,3,4 are scalar quantities and these are multiplied with vectors and we know that scalar multiplication changes the magnitude of a vector. Thus it is better to understand them as weights per unit length.

If the magnitude of a vector is length, then it is length vector and if the magnitude of a vector is weight, then it is a weight vector (after multiplication with weight per unit length)

Thus we can write an observation row (vector) having a list of elements as linear equation and visualize the experience in a vector space as a piping layout.

The changing weight per unit length in different direction can be understood as changing diameter of the pipeline (weight per unit length is the property of the dimension)",visual vector,"['visual', 'vector']",Visualization of vector
381,"Types of Matrices

1. Symmetric matrix (in linear algebra, a symmetric matrix is a square matrix that is equal to its transpose)

2. Anti-symmetric matrix

3. Column matrix

4. Row matrix",type matric,"['type', 'matric']",Types of Matrices
382,"Matrix operations

Addition, Subtraction, multiplication

> For addition or subtraction of matrix A and B, both the matrices needs to have same dimension",matrix oper,"['matrix', 'oper']",Matrix operations
383,"Matrix multiplication

For matrix multiplication, 
No. of elements in column of 1st matrix must be equal to no. of elements in row of 2nd matrix

Size of resultant matrix = Row of 1st matrix* Column of 2nd matrix

Matrix A = (aij) where i=rows, j=columns

Thus, 1st element of resultant matrix = (a11*b11+ a12*b21+ a13*b31+ a1n*bn1)

Matrix multiplication induces some transformation (rotation/ reflection/ shearing etc.) on other vector or matrix and gives the resultant vector or matrix",matrix multipl,"['matrix', 'multipl']",Matrix multiplication
384,"Equation in matrix form

K11X1+K12X2=R1,
K21X1+K22X2=R2 can be written as

(K11 K12)(X1)=(R1)
k21 K22   X2   R2

or, KX = R

or, X= (K−inverse)*R
 
Thus, we can find the values of x1 and x2",equat matrix form,"['equat', 'matrix', 'form']",Equation in matrix form
385,"Identity matrix

A square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros. The effect of multiplying a given matrix by an identity matrix is to leave the given matrix unchanged.

> If a matrix is multiplied with its conjugate matrix and give an identity matrix, we call it an unitary matrix

> A diagonal matrix  has non-negative real numbers on the diagonal",ident matrix,"['ident', 'matrix']",Identity matrix
386,"Determinant of a matrix

In mathematics, the determinant is a scalar value that is a function of the entries of a square matrix.

The determinant is useful for solving linear equations.

>> Determiants can only be found for a square matrix. Means, no. of equations (observations) must be equal to no. of unknowns (features)

Easy way is diagonal method

For 3X3 matrix,

A = (a b c)
     d e f
     g h i

|A| = a(ei − fh) − b(di − fg) + c(dh − eg)",determin matrix,"['determin', 'matrix']",Determinant of a matrix
387,"Transpose of a matrix

In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal; simply means we are interchanging the rows and columns.",transpos matrix,"['transpos', 'matrix']",Transpose of a matrix
388,"Adjoint of a matrix

The adjugate or classical adjoint of a square matrix is the transpose of its cofactor matrix. It is also occasionally known as adjunct matrix
",adjoint matrix,"['adjoint', 'matrix']",Adjoint of a matrix
389,"cofactor matrix 

A cofactor matrix is formed by finding the cofactors for all elements. Cofactor of any element is obtained by eliminating the row and column of that particular element and then finding the determinant.",cofactor matrix,"['cofactor', 'matrix']",cofactor matrix 
390,"Inverse of a matrix 

Inverse of matrix A= adj(A)/det(A)

Why Do We Need an Inverse?

Because, there is no concept of dividing a matrix by another matrix. Thus we multiply by an inverse, which achieves the same thing.",invers matrix,"['invers', 'matrix']",Inverse of a matrix 
391,"Eigenvalues and Eigenvectors of a square matrix

The eigenvector is a vector which does not change in direction after transformation (matrix) is applied

In simple words, when we find the eigenvector of any big matrix, number of features or columns are reduced substantially. Thus a complex data can be better understood.

> Eigenvalues are just the scaling or magnification factors of the eigenvector or the simple data

Av=λv

or, (A-λI)=0

or, |A-λI|=0

where v is the eigenvector of the matrix A and matrix A only does scaling of the eigenvector. λ is the eigen value or the scaling factor. I is the identity matrix of size A. 

Thus we can find the eigenvalues and eigenvectors of any matrix. 

For some matrices there may not be any eigenvectors. ",eigenvalu eigenvector squar matrix,"['eigenvalu', 'eigenvector', 'squar', 'matrix']",Eigenvalues and Eigenvectors of a square matrix
392,"Use of eigen values and eigen vectors

> To Compress the data

> To transform the original features into another feature subspace

> Optimal computational efficiency

> Cluster optimization in k-means clustering

> To better understand and visualize linear mappings

> To understand the stability of mechanical constructions

> For solving systems of differential equations, 

> To recognize images, 

> To interpret and visualize quadratic equations, 

> For image segmentation.",use eigen valu eigen vector,"['use', 'eigen', 'valu', 'eigen', 'vector']",Use of eigen values and eigen vectors
393,"Nilpotent matrix 

Nilpotent matrix is a square matrix with n-dimensional triangular matrix with zeros along the main diagonal.",nilpot matrix,"['nilpot', 'matrix']",Nilpotent matrix 
394,"Importance of Probability theory 

Probability theory is very important in machine learning because, our ML model learns the rules/trend of the training data or training experiences and does prediction about future event. Prdiction is all about calculating the probability/chances of a event to occur.  ",import probabl theori,"['import', 'probabl', 'theori']",Importance of Probability theory 
395,"Set Theory

Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects.

An event is a subset of the sample space or universal set (with all possible outcomes).

Universal Set: Set containing all elements and of which all other sets are subsets. Denoted by U.",set theori,"['set', 'theori']",Set Theory
396,"Experiment, sample space, observation and experience

> An experiment (e.g. studying in any class) can be infinitely repeated and has a well-defined set of possible outcomes, called the sample space (e.g. column names of a table). 

> One observation means one outcome (e.g. one cell of a table) and one experience means a set of observations (e.g. one row of a table).

> Thus from an experiment, we get a dataset or a set of experiences (e.g. the entire table)",experi sampl space observ experi,"['experi', 'sampl', 'space', 'observ', 'experi']","Experiment, sample space, observation and experience"
397,"Intersection and Union of Sets

> Complement of A (denoted by A’ or A^c)- Everything that is not in A 

> Intersection of A and B (denoted by A ∩ B)- Everything in A and B

n(A ∩ B)= n(A)+ n(B)-n(A U B)

The number of elements in the union of set A and B,
n(A∪B) = n(A - B) + n(A ∩ B) + n(B - A) 

n(B) = n(A ∩ B) + n(B - A)

(A ∩ B) means (A and B)

> Union of A and B (denoted by A U B)-Everything in A or everything in B or both in A and B

(A U B) means (A or B)",intersect union set,"['intersect', 'union', 'set']",Intersection and Union of Sets
398,"Mutually exclusive sets

If no element is common between two sets, we say that they are mutually exclusive.

A ∩ B = φ,",mutual exclus set,"['mutual', 'exclus', 'set']",Mutually exclusive sets
399,"Permutation & Combination (Counting)

nPr=n!/(n-r)!

nCr=n!/{r!(n-r)!}

For counting, we can import libraries in python

from itertools import permutations

from itertools import combinations

combinations(my_list,4)",permut combin count,"['permut', 'combin', 'count']",Permutation & Combination (Counting)
400,"Basic Probability

> Probability is a numerical way of describing how likely (or not) an event is to happen. If each of the elements in the sample space are equally likely, then we can define the probability of event A as P(A) = n(A) / n(S)

n(S) is the no. of elements in the set or no. of possible outcomes

> Not event A, P(~A)=1-P(A)",basic probabl,"['basic', 'probabl']",Basic Probability
401,"Probability Axiom#1

The relative frequency of event that is certain to occur must be 1.

The probability of getting any one of the numbers 1 to 6 on a dice is certain! 

P(S)=1, where S={1,2,3,4,5,6}",probabl axiom1,"['probabl', 'axiom1']",Probability Axiom#1
402,"Probability Axiom#2

The relative frequency of occurrence of any event must not be negative, that is, probabilities can never be negative. 

So the probability of an impossible event is 0.

Rule 1 and 2 together are telling us that probabilities lie between 0 (impossible) and 1 (certain). 
",probabl axiom2,"['probabl', 'axiom2']",Probability Axiom#2
403,"Probability Axiom#3 or Special Addition Rule

P(A U B) = P(A) + P(B)    if A ∩ B = φ

This property is known as additivity.",probabl axiom3 special addit rule,"['probabl', 'axiom3', 'special', 'addit', 'rule']",Probability Axiom#3 or Special Addition Rule
404,"Addition Rule

In general, where two sets are not necessarily mutually exclusive, Axiom#3 can be extended as follows: 

P(A U B) = P(A) + P(B) - P(A ∩ B)

> Event A and B can occur at the same time (not mutually exclussive)",addit rule,"['addit', 'rule']",Addition Rule
405,"Conditional Probability

A conditional probability is the probability of an event, given some other event has already occurred. It is denoted by P(A|B) inferred as probability of A given B.

P(A | B) = P(A ∩ B) / P(B)
 
> If P(A | B) or P(A/B) =P(A), then A and B are independent events. 

Then probability of A not given B = P(A /~B) = P(A)",condit probabl,"['condit', 'probabl']",Conditional Probability
406,"Multiplication Rule

The probability that event A and B both occur is equal to the probability that Event B occurs times the probability that Event A occurs, given that B has occurred. 

P(A ∩ B) = P(B) * P(A | B)

If A and B are independent, 

P(A ∩ B) = P(B) * P(A)",multipl rule,"['multipl', 'rule']",Multiplication Rule
407,"Solving any set or probability problem

> Identify the event, subsets or groups: A, B

> Identify n(A) or P(A)

> Identify n(B) or P(B)

>  Identify n(A U B) or P(A U B) or P(A or B)

> Identify n(A ∩ B) or P(A ∩ B) or P(A and B)

> Understand the concept in totality with respect to experiment, sample space, observation, experience and dataset ",solv set probabl problem,"['solv', 'set', 'probabl', 'problem']",Solving any set or probability problem
408,"Difference between mutually exclusive and independent events

> A mutually exclusive event can simply be defined as a situation when two events cannot occur at same time whereas independent event occurs when one event remains unaffected by the occurrence of the other event.

> If two events are mutually exclusive, then they cannot be independent.",differ mutual exclus independ event,"['differ', 'mutual', 'exclus', 'independ', 'event']",Difference between mutually exclusive and independent events
409,"Basics of Summarizing Data

It is used for understanding large to very large dataset through different statistics

There are mainly three measures or statistics:

1. Measure of central tendency

2. Measure of spread

3. Measures of Symmetry",basic summar data,"['basic', 'summar', 'data']",Basics of Summarizing Data
410,"Numerical, Categorical, Dichotomous, and Ordinal Data

> A data can be of two types-Numerical and Categorical

> Numerical data are of two types-Discrete and Continuous

> Categorical data can be of three types-

1. Dichotomous(attribute like Yes or No), 

2. Nominal (Names like bus, train, car or red, yellow, blue) and 

3. Ordinal(order like hot, hotter, hotest)",numer categor dichotom ordin data,"['numer', 'categor', 'dichotom', 'ordin', 'data']","Numerical, Categorical, Dichotomous, and Ordinal Data"
411,"1. Measure of central tendency 

There are three measure of central tendency

Mean, Median, and Mode",1 measur central tendenc,"['1', 'measur', 'central', 'tendenc']",1. Measure of central tendency 
412,"Understanding Mean

Mean is denoted by x bar (x̄) or mu(μ) and it is the most common method to measure central tendency

> It is influenced by outliers

> Not applicable to categorical data",understand mean,"['understand', 'mean']",Understanding Mean
413,"Understanding Median

After sorting n observations in ascending or decending order,

If n is odd,then the median is the middle observation. 

If n is even, then the median is the mean of the middle two observations.

> It is robust or resistant to the effects of extreme observations (outliers)

> Not applicable to categorical data",understand median,"['understand', 'median']",Understanding Median
414,"Understanding Mode

Mode is defined as the value which occurs with the greatest frequency or the most typical value.

> Its use in practice is limited

> Not influenced by outliers

> Applicable to all types of data

> More than one mode possible",understand mode,"['understand', 'mode']",Understanding Mode
415,"2. Measure of spread

The measure of spread are:

Range, 

Variance, 

Standard Deviation, 

Interquartile Range",2 measur spread,"['2', 'measur', 'spread']",2. Measure of spread
416,"Understanding Range

Range is a very simple measure of spread defined as the difference between the largest and smallest observation in the data set.

Range is a poor measure of the spread of data as it relies on the extreme values, which aren’t necessarily representative of the data as a whole.",understand rang,"['understand', 'rang']",Understanding Range
417,"Understanding Variance

Variance is denoted by σ^2 or S^2

Variance is the expectation of the squared deviation of a random variable from its mean. 

In other words, it measures how far a set of numbers is spread out from their average value or mean.
",understand varianc,"['understand', 'varianc']",Understanding Variance
418,"Understanding Standard Deviation

The standard deviation is the positive square root of the variance.

A quantity expressing by how much the members of a group differ from the mean value

> A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range",understand standard deviat,"['understand', 'standard', 'deviat']",Understanding Standard Deviation
419,"Understanding Interquartile Range

IQR is another measure of spread which is like the range but not affected by the data extremes

> Q1, Q2 and Q3 are the quartiles that divide a set of data into four quarters. 

> Note that Q2 is just the median, while Q1 is called the lower quartile and Q3 the upper quartile

> The interquartile range is defined as (Q3  - Q1)",understand interquartil rang,"['understand', 'interquartil', 'rang']",Understanding Interquartile Range
420,"Steps to Calculate IQR

> Arrange the data in asc or desc order

> Q2= median of the data

> Q1=The median of data values below the main median

> Q3=The median of data values above the main median

> IQR = Q3-Q1

Here,

Q1=percentile 25

Q2=percentile 50

Q3=percentile 75

Q4=percentile 100

Percentile = (Number of values below ""X""/Total Number of all the Values) * 100",step calcul interquartil rang,"['step', 'calcul', 'interquartil', 'rang']",Steps to Calculate IQR
421,"Outliers with respect to IQR

Data below (Q1-1.5*IQR) and data above (Q3+1.5*IQR) are generally called outliers",outlier respect interquartil rang,"['outlier', 'respect', 'interquartil', 'rang']",Outliers with respect to IQR
422,"3. Measures of Symmetry

Coefficient of Skewness

> Positively skewed (Mean > Median > Mode) 

> Negatively skewed (Mean < Median < Mode)

> Normal or Symmetrical (Mean = Median = Mode)

Peak value of the distributioin is always the mode (highest frequency) for all of the above three categories",3 measur symmetri,"['3', 'measur', 'symmetri']",3. Measures of Symmetry
423,"Understanding skewness

When we plot graph between two conclusive parameters (e.g. house price range and number of houses), we can draw conclusion from the skewness.

> If the plot is positively skewed, there is highest no. of houses with lower price range-Its a village

> If the plot is negatively skewed, there is highest no. of houses with higher price range-Its a metro

> If the distribution is normal, there is highest no. of houses with medium price range-Its a town",understand skew,"['understand', 'skew']",Understanding skewness
424,"Libraries for Summarizing Data

from statistics import mode

mode(my_list)

from statistics import median

from statistics import mean

we can also calculate median and mean  using numpy library
 
Another way
from scipy import stats

stats.mode(a)",librari summar data,"['librari', 'summar', 'data']",Libraries for Summarizing Data
425,"Skewness and Kurtosis measurements

my_df['column_name'].skew()

my_df['column_name'].kurt()

",skew kurtosi measur,"['skew', 'kurtosi', 'measur']",Skewness and Kurtosis measurements
426,"Understanding random variable

A random variable (numerical feature or column name) is a numerical description of the outcome of a statistical experiment

Variable can store single data or multiple data as data structure. Thus random variable can store single random data or multiple random data as data structure.

Let X = number of heads when we toss a coin (X can take two values, 0 and 1)

Let Y = number that comes up when we roll a die (Y can take values 1, 2, 3, 4, 5 or 6)

It is conventional to denote the random variable by a capital letter and the possible values it can take by a small letter.

Let X = number of heads when we toss a coin, 
then x belongs to {0, 1}

Let Z = weight of a randomly selected student in a class, 
then z belongs to (0, ∞)

> We can use set data structure to store all possible outcomes of an experiment.

> A random variable is a rule for associating a number with each element in the set. Thus random variable is a function of the outcomes.

> Experiment: Tossing a coin

Sample space: S = {H, T}. 
X is the number of heads when we toss a coin.
Then, X(H) = 1 and X(T) = 0.

Random variable is introduced for graphical representation of the probability of outcomes. 

Because without the numerical values of the outcome, we can not plot them on an axis.",understand random variabl,"['understand', 'random', 'variabl']",Understanding random variable
427,"Python Code for Random Variables

import random
X=random.randint(1,6)
print(""You rolled"",X) 

returns any random number between 1 to 6

X=random.random()
print(""The random float is"",X)

> random library creates random data. Data structure can not be created using random library. Numpy is used in that case",python code random variabl,"['python', 'code', 'random', 'variabl']",Python Code for Random Variables
428,"Random seed

Repeat the same random numbers using seed (useful for presentation)

random.seed(11)
X=random.random()
print(""The random float is"",X)

NumPy random seed
np.random.seed(0) ; np.random.rand(4)

It is a pseudo-random number generator

np.random.rand() # generates a single random float between 0 and 1",random seed,"['random', 'seed']",Random seed
429,"Types of Random Variables

1. Discrete Random Variables (countable)

2. Continuous Random Variables (uncountable)",type random variabl,"['type', 'random', 'variabl']",Types of Random Variables
430,"Continuous and Discrete Random variables using numpy array

X=np.random.uniform(1,3)
returns a float between 1 and 3

X=np.random.uniform(1,3,5)
returns a numpy array with 5 floats between 1 and 3
 
X=np.random.randint(1,3)
returns an int between 1 and 3

X=np.random.randint(1,3,5)
returns a numpy 1D array with 5 int between 1 and 3",continu discret random variabl use numpi array,"['continu', 'discret', 'random', 'variabl', 'use', 'numpi', 'array']",Continuous and Discrete Random variables using numpy array
431,"Discrete Random Variables in Probability Distribution

1. Probability Distribution or Mass Function (pf or pdf or pmf)

The function fX(x) = P(X=x) for each x in the range of X is the probability function (pf) of X.

2. Cumulative Distribution Function(cdf or df)-summation

The cumulative distribution function (cdf) of X is FX(x) = P(X ≤ x). It gives the probability that X assumes a value that does not exceed x. CDFs are also known as “Distribution functions (df)""",discret random variabl probabl distribut,"['discret', 'random', 'variabl', 'probabl', 'distribut']",Discrete Random Variables in Probability Distribution
432,"Continuous Random Variables in Probability Distribution 

1. Probability Density Function

In case of continuous variables we always take intervals into account. 

The probability associated with an interval of values, (a, b), is represented as P(a < X < b) – and is the area under the curve of the probability density function (pdf) from a to b.

2. Cumulative Distribution Function-Integration

The cumulative distribution function (cdf) is defined to be the function: FX(x) = P(X ≤ x) For a continuous random variable, FX(x) is a continuous, non-decreasing function, defined for all real values of x.",continu random variabl probabl distribut,"['continu', 'random', 'variabl', 'probabl', 'distribut']",Continuous Random Variables in Probability Distribution 
433,"Mean of Random Variables

E[X] is a measure of the average/centre/location/level of the distribution of X. It is called the expected value of X, or mean of X, and is usually denoted as μ.

>> Mean of Discrete Random Variables

E(X) = ∑xP(X=x)

>> Mean of Continuous Random Variables by integration

The expected value of a random variable is the mean value over an infinite number of observations of the variable.",mean random variabl,"['mean', 'random', 'variabl']",Mean of Random Variables
434,"Variance of Random Variables

The variance σ² is a measure of the spread/dispersion/variability of the distribution. Specifically, it is a measure of the spread of the distribution about its mean.

Variance of Discrete Random Variables

Var(X) 
= ∑(x-μ)^2*P(X=x)
= E(X^2)- E(X)^2

For variance of Continuous Random Variables integration is used in place of summation",varianc random variabl,"['varianc', 'random', 'variabl']",Variance of Random Variables
435,"Point probability, cumulative probability

When we have a probability distribution function of a random variable and the definition of random variable is matching with our requirement, then we can answer point probability or cumulative probability for any outcome or mean and variance for the set of outcomes",point probabl cumul probabl,"['point', 'probabl', 'cumul', 'probabl']","Point probability, cumulative probability"
436,"Formulas for expectation and variance

E(aX+b)= a*E(X)+b

e.g.
E(5X - 2Y) = 5*E(X) - 2*E(Y)

Var(aX+b)= a^2*Var(X)

e.g. 
Var(5X - 2Y) =  5^2*Var(X) + (-2)^2*Var(Y)

",formula expect varianc,"['formula', 'expect', 'varianc']",Formulas for expectation and variance
437,"Basics of Probability distribution

Probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.

Simply, probability is the function of outcomes

Thus probability distribution is the graph between all possible numerical outcomes of an experiment vs. probability of occurance of the outcome.

The set of all possible numerical outcome is the random variable.

Thus probability distribution is the graph between random variable vs. probability",basic probabl distribut,"['basic', 'probabl', 'distribut']",Basics of Probability distribution
438,"Types of Discrete Probability Distribution

> Depending on definition of random variable,X, we can identify the pdf

1. Uniform Distribution

2. Bernoulli Distribution

3. Binomial Distribution

4. Geometric Distribution

5. Poisson Distribution",type discret probabl distribut,"['type', 'discret', 'probabl', 'distribut']",Types of Discrete Probability Distribution
439,"1. Uniform Distribution-discrete

-all outcomes are equally likely

X is the no. of outcome from 1 to k. Thus,

x= 1,2,3,...k

P(X=x)=1/k
 
E(X)=(k+1)/2

Var(X)=(k^2-1)/12

",1 uniform distributiondiscret,"['1', 'uniform', 'distributiondiscret']",1. Uniform Distribution-discrete
440,"2. Bernoulli Distribution

-Bernauli trial is an experiment which has only two possible outcomes ""success"" and ""failure""

X is the success or failure in a trial. Thus,

x= 0,1

P(X=1)= θ

P(X=0)= 1- θ

P(X=x)= θ^x*(1- θ)^(1-x)

E(X) = θ

Var(X) = θ - θ^2

> Bernoulli trials are also called as yes or no questions.",2 bernoulli distribut,"['2', 'bernoulli', 'distribut']",2. Bernoulli Distribution
441,"3. Binomial Distribution

-Binomial Distribution is the widely used probability distribution, derived from Bernoulli Process (Only two possible outcomes, i.e. success or failure).

Binomial distribution is one in which the probability of repeated number of trials (sequence of n bernoulli trials) are studied.

X is the number of success (or failure) in n trials. Thus 
x= 0,1,2...n

Also denoted as Bin(n,θ) or Bin(n,p)

P(X=x)= C(n,x)*θ^x*(1- θ)^(n-x)

E(X) = nθ

Var(X) = nθ(1-θ)",3 binomi distribut,"['3', 'binomi', 'distribut']",3. Binomial Distribution
442,"4. Geometric Distribution

-gives the probability of first success with number of trials.

X is the no. of trial on which the first success occurs. Thus,

x= 1,2,3,…

P(X=x)= θ*(1- θ)^(x-1)

E(X) = 1/θ

Var(X) = (1- θ)/θ^2

> The probability of success (successive trials are without replacement) changes from trial to trial in Hypergeometric Distribution 

> θ is also denoted by p",4 geometr distribut,"['4', 'geometr', 'distribut']",4. Geometric Distribution
443,"5. Poisson Distribution-

-models/trials the no. of events that occurs within a specified interval of time.

Thus, rate of occurance, λ=E(X)=Var(X)

Unlimited number of possible outcomes (can  be used for two outcomes-success or failure).

X is the number of specified outcome (success, failure or anything else) in a specified interval of time. Thus,

x= 0,1,2,....

P(X=x)= λ^x*e^(-λ)/x!

from scipy.stats import poisson",5 poisson distribut,"['5', 'poisson', 'distribut']",5. Poisson Distribution-
444,"Moments of probability distribution

1) The first moment is the mean, which indicates the central tendency of a distribution. 

2) The second moment is the variance, which indicates the width or spread 

3) The third moment is the skewness, which indicates any asymmetric 'leaning' to either left or right.

4) The fourth standardized moment is the kurtosis, which indicates the heaviness of the tail of the distribution.",moment probabl distribut,"['moment', 'probabl', 'distribut']",Moments of probability distribution
445,"Moments in Physics

> The total mass is the zeroth moment of mass.

> The first moment is the center of the mass, and

> The second moment is the rotational inertia.

From these moments we can understand how the physical quantity is arranged. Similarly, from the moments of pdf, we can understand its arrangement. ",moment physic,"['moment', 'physic']",Moments in Physics
446,"Basics of continuous distribution

Since continuous probability functions are defined for an infinite number of points over a continuous interval, the probability at a single point is always zero (e.g. P(weight=50.555)=0).

Hence, we will use P(X<=x) and P(X>=x)

P(X<=x) = P(X<x) and
P(X>=x) = P(X>x)

Since the sum of probabilities over the entire range of x is 1, hence,P(X>x) = 1 - P(X<x)",basic continu distribut,"['basic', 'continu', 'distribut']",Basics of continuous distribution
447,"Types of continuous probability distribution

1. Uniform Distribution

2. Normal Distribution 

3. Standard Normal Distribution

4. Exponential Distribution

5. Gamma Distribution

6. Chi-square Distribution

7. t-Distribution

8. F-Distribution
",type continu probabl distribut,"['type', 'continu', 'probabl', 'distribut']",Types of continuous probability distribution
448,"1. Uniform Distribution-continuous

X is the outcome between α (also denoted as a) and β (also denoted as b)

P(X<x)= 1/(β- α)

E(X) = (β+ α)/2

Var(X) = (β- α)^2/12",1 uniform distributioncontinu,"['1', 'uniform', 'distributioncontinu']",1. Uniform Distribution-continuous
449,"2. Normal or Gaussian Distribution

-It has a symmetrical “bell-shaped” density curve

X is the outcome between -∞ and 
+ ∞ and expectation is μ and standard deviation is σ

Normal distribution is the function of x, μ and σ 

X ~ N(μ, σ^2)

> It is asymptotic, means each end approaches the horizontal axis but never reaches it",2 normal gaussian distribut,"['2', 'normal', 'gaussian', 'distribut']",2. Normal or Gaussian Distribution
450,"3. Standard Normal Distribution- 

X is a normally distributed random variable with mean, μ=0 and standard deviation, σ=1. It will always be denoted by z_score",3 standard normal distribut,"['3', 'standard', 'normal', 'distribut']",3. Standard Normal Distribution- 
451,"4. Exponential Distribution

X is the outcome between 0 and ∞ and has a rate of occurance λ

P(X<x)= λ*e^-λx

E(X) = 1/λ

Var(X) = 1/λ^2

X ~ Exp(λ) means random variable X has an exponential distribution with rate parameter λ

> Exponential distribution is a special case of gamma distribution

> The Exponential distribution also describes the time between events in a Poisson process

> It is used to model the lifetime of any equipment.

> It also gives the distribution of the waiting time, T

P(T>t)= e^-λt

P(T<t)= 1- e^-λt",4 exponenti distribut,"['4', 'exponenti', 'distribut']",4. Exponential Distribution
452,"Gamma Function

Gamma Function, Γ(α) = ∫t^(α −1)* e^(−t) dt #  interval [0, ∞ ]

The gamma function is one commonly used extension of the factorial function to complex numbers.

> A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers, and i is a symbol called the imaginary unit, and satisfying the equation i² = −1",gamma function,"['gamma', 'function']",Gamma Function
453,"5. Gamma Distribution

Widely used distribution and its importance is largely due to its relation to exponential and normal distributions.

X is the outcome between 0 and ∞, rate of occurance, λ (also denoted as  θ)  and has a shape parameter α (also denoted as k)

P(X<x) = λ^α*x^(α-1)*e^-λx/Γ(α)
",5 gamma distribut,"['5', 'gamma', 'distribut']",5. Gamma Distribution
454,"6. Chi-square Distribution

The chi-squared distribution is a special case of the gamma distribution

X is the chi_square_score between 0 and ∞,  degrees of freedom= k 

P(X<x) = x^(k/2-1)*e^(-x/2)/(2^k/2*Γ(k/2))

chi_square_score= (n-1)*S^2/σ^2",6 chisquar distribut,"['6', 'chisquar', 'distribut']",6. Chi-square Distribution
455,"7. t-Distribution 

It is similar to the normal distribution, just with fatter tails. Have higher kurtosis than normal distributions

X is the t_score between -∞ and + ∞, degrees of freedom= k or γ

",7 tdistribut,"['7', 'tdistribut']",7. t-Distribution 
456,"8. F-Distribution

X is f_score between 0 and ∞, when two independent random samples of size n1 and n2 are taken respectively ",8 fdistribut,"['8', 'fdistribut']",8. F-Distribution
457,"log-normal distribution 

In probability theory, a log-normal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.",lognorm distribut,"['lognorm', 'distribut']",log-normal distribution 
458,"Exponential expressions

Exponential expressions are just a way to write powers in short form. The exponent indicates the number of times the base is used as a factor. Exponentiation is the Arithmatic Operation for power.

f(x) = e^x is called the natural exponential function  or in short, exponential function",exponenti express,"['exponenti', 'express']",Exponential expressions
459,"Euler's number

The number e, also known as Euler's number (like pi, is a transcendental number), is a mathematical constant approximately equal to 2.71828, and can be characterized in many ways. It is the base of the natural logarithm. 

> The reason Euler's number is such an important constant is that it has unique properties that simplify many equations and patterns.

> Apart from natural logarithm, other two logarithms are 

1. common logarithm-logarithm of base 10 

2. binary logarithm-logarithm of base 2

Logarithmic operation returns the power of the base. Thus, logarithms are a convenient way to express large numbers.",euler number,"['euler', 'number']",Euler's number
460,"Python Code for Continuous Distributions

from scipy.stats import uniform

from scipy.stats import norm

from scipy.stats import chi2

from scipy.stats import t

from scipy.stats import f

from scipy.stats import * (this imports all the modules)

lower_limit = 50

upper_limit = 150

range = upper_limit - lower_limit

# P(X < 74) is
uniform.cdf(74, lower_limit, range)

standard_deviation=math.sqrt(variance)

P(X<28) is norm.cdf(28, mean, standard_deviation)

P(X<z_score) is norm.cdf(z_score)

chi2.cdf(chi_square_score, degrees_of_freedom)

t.cdf(t_score, degrees_of_freedom)

f.cdf(f_score, degrees_of_freedom1, degrees_of_freedom2)",python code continu distribut,"['python', 'code', 'continu', 'distribut']",Python Code for Continuous Distributions
461,"Continuous distributions in ML

Different probability distributions serve different purposes and represent different data generation processes.

Use of continuous distribution are in the distribution of numerical input and output variables for models and in the distribution of errors made by models.",continu distribut machin learn,"['continu', 'distribut', 'machin', 'learn']",Continuous distributions in ML
462,"Most frequent types of distribution for data scientist:

1. Bernoulli 

2. Uniform 

3. Binomial 

4. Poisson 

5. Normal 

6. Exponential ",frequent type distribut data scientist,"['frequent', 'type', 'distribut', 'data', 'scientist']",Most frequent types of distribution for data scientist:
463,"Basics of Joint Distribution

For a given experiment, we are often interested not only in probability distribution functions of individual random variables but also in the relationships between two or more random variables.

The joint probability distribution can be expressed either in terms of a joint cumulative distribution function or in terms of a joint probability density function (in the case of continuous variables) or joint probability mass function (in the case of discrete variables).

>> Let A and B be the two events, joint probability is the probability of event B occurring at the same time that event A occurs. This can be written as P(A, B) or P(A ∩ B). Thus, the joint probability is also called the intersection of two or more events.",basic joint distribut,"['basic', 'joint', 'distribut']",Basics of Joint Distribution
464,"Two dimensional random vector

The combination of two random variable is called two dimensional random vector (discrete or continuous). Better to call it random matrix.",two dimension random vector,"['two', 'dimension', 'random', 'vector']",Two dimensional random vector
465,"Marginal Distribution and Conditional Distribution

> Marginal Distribution (discrete and continuous)-the probabilities for any one of the variables with no reference to any specific ranges of values for the other variables

> Conditional Probability Distribution- the probabilities for any subset of the variables conditional on particular values of the remaining variables. Conditional Distribution and Conditional expectation can be discrete or continuous.

Say the joint density function is f(x,y,z)

Then the marginal density functions are

fX(x)=∫∫f(x,y,z)dydz

fY(y)=∫∫f(x,y,z)dxdz

fZ(z)=∫∫f(x,y,z)dxdy

For independent random variables,

fX,Y,Z(x,y,z)= fX(x)*fY(y)*fZ(z)",margin distribut condit distribut,"['margin', 'distribut', 'condit', 'distribut']",Marginal Distribution and Conditional Distribution
466,"Independent Random Variables 

Two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one

Independent Random Variables (discrete and continuous) means outcomes are equally likely",independ random variabl,"['independ', 'random', 'variabl']",Independent Random Variables 
467,"Probability mass function

> Probability mass function (for discrete random variable) or joint probability mass function is always expressed as matrix",probabl mass function,"['probabl', 'mass', 'function']",Probability mass function
468,"Degree of association

Covariance and correlation are two important measures which describe the degree of association between two random variables X1 & X2

> Covariance gives the direction of relation

> Correlation give the strength of relation

> Covariance can vary between -∞ and +∞ 

> Correlation ranges between -1 and +1",degre associ,"['degre', 'associ']",Degree of association
469,"Understanding of Covariance 

Covariance  signifies  the  direction  of  the  linear  relationship between the two variables. By direction, we mean, if the variables are directly proportional or inversely proportional to each other. 

In general, it can be shown that a positive value of Cov(X1, X2) is an indication that X2 tends to increase as X1 does, whereas a negative value indicates that X2 tends to decrease as X1 increases. ",understand covari,"['understand', 'covari']",Understanding of Covariance 
470,"Understanding of Correlation

The strength of the relationship between X1 and X2 is indicated by the correlation between X1 and X2, a dimensionless quantity

Cov(X1,X2)= E(X1*X2)-E(X1)*E(X2)

Cov(X1,X1)= Var(X1)

Cov((X1+X2),X3)= Cov(X1,X3)+ Cov(X2,X3) 

Cov(aX, Y) = Cov(X,aY)

Corr(X1,X2)= Cov(X1,X2)/sqrt(Var(X1)*Var(X2))
 
Thus correlation refers to the scaled form of covariance.

> As correlation is dimensionless, it is not influenced by the change in scale.",understand correl,"['understand', 'correl']",Understanding of Correlation
471,"Basics of Sampling & Statistical Inference

> Population of Data or simply population indicates the full set of data or experience of any topic which is impossible to gain  for our practical experiments or problems.

> A  set  of  items  selected  from  a  parent  population  is  a random sample if the probability that any item in the population is included in the sample is proportional to its frequency in the parent population and the inclusion/ exclusion of any item in the sample operates independently 

> From the statistics of the sample, we can draw inference or conclusion about population.",basic sampl statist infer,"['basic', 'sampl', 'statist', 'infer']",Basics of Sampling & Statistical Inference
472,"Random sample

Random sample is a set of possible outcomes of sampling operation. 

Random sample is denoted by X=(X1,X2,X3,…Xn)

Thus random sample is nothing but the values of a single feature.",random sampl,"['random', 'sampl']",Random sample
473,"Understanding of Statistics

STATISTIC is the score or measurement of each individual data or experience. 

STATISTICS is therefore, the process of designing, comparing, interpreting and analysing data. 

Statistic is related to the sample and parameter is related to the population. A statistic is used to estimate a parameter.

A statistics are also  random variables that is a function of the random sample, but not a function of unknown parameters.

Therefore, Random variable is a function of random sample.

Thus, statistics means creating multiple  random variables (i.e. mean, mode, median, variance etc.) from the random sample data or experiences.

These random variables or statistics are the function of each feature of a dataset.

my_df.describe() # gives the statistics

my_df.describe(include='all') # to include all the categorical columns

> Descriptive statistics is responsible for examining trends or distributions",understand statist,"['understand', 'statist']",Understanding of Statistics
474,"Statistical Inference 

> Looking at the sample, concluding about the population

To reach the final conclusion about the population, we take the help of different probability distribution as probability distribution of the sample mean is the indicator of population probability distribution.
(we can find individual probability, marginal probability or conditional probability of the random variables) 

Thus, Statistical Inference is the theory, methods, and practice of forming judgements about the parameters of a population.

The reliability of statistical inference typically depends on the random sampling.",statist infer,"['statist', 'infer']",Statistical Inference 
475,"Sample Mean and Sample Variance

Sample Mean, 
X̄ = (1/n) * Σ Xi

Sample Variance, 
S^2 = (1/(n-1)) * Σ (Xi - X̄)^2


variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)

variance(my_sample_list)- returns the sample variance

Sample mean is denoted by x̄ and population mean is denoted by μ.

Sample standard deviation is denoted by S and population standard deviation is denoted by σ",sampl mean sampl varianc,"['sampl', 'mean', 'sampl', 'varianc']",Sample Mean and Sample Variance
476,"Independent and indentically distributed (IID) random variables

A collection of random variables is independent and identically distributed if each random variable has the same probability distribution (same μ and σ^2) as the others and all are mutually independent.

This holds true when we take multiple samples randomly from the same population",independ indent distribut iid random variabl,"['independ', 'indent', 'distribut', 'iid', 'random', 'variabl']",Independent and indentically distributed (IID) random variables
477,"Sampling with Replacement

Sampling ''with replacement'' means that when a unit selected at random from the population, it is returned to the population (replaced), and then a second element is selected at random.",sampl replac,"['sampl', 'replac']",Sampling with Replacement
478,"Central Limit Theorem (CLT) or z-results or z-score

> The Central Limit Theorem states that the  distribution of the sample means approaches a normal distribution as the sample size gets larger — no matter what the shape of the sample distribution. This fact holds especially true for sample size, n> 30 (more accuracy n>50)

As per CLT,
Expectation of the sample mean, E(x̄) = μ

Variance of the sample mean, Var(x̄) = σ^2/n

z-result of the sample mean, z_score = (x̄ – μ) / (σ/√n)

> Sample size is the total no. of observations/ rows in the set of samples

> One sample means a set with all observations for one feature of the samples space

> z-score (also called a standard score) gives us an idea of how far from the mean a data point (x) is. 
 z_score = (x – μ) /√σ^2

> We can find the cdf after getting the score (score is a generated statistic from the basic statistics like expectation and variance) of any probability distribution 

> CLT is is the bridge between statistics and probability

> CLT simply tells us that for large sample, population mean, μ is the expected value of sample mean, E(x̄) having  highest probability of occurance and rest of the probability distribution will be symmetrically decreasing from the mean",central limit theorem clt zresult zscore,"['central', 'limit', 'theorem', 'clt', 'zresult', 'zscore']",Central Limit Theorem (CLT) or z-results or z-score
479,"t-Result or t-Score

In most cases, we can get population mean but can not get population standard deviation. Thus, z-result cannot be used. We use the t-result in such cases.

> t-result is valid for samples from normal distribution only.

> The t-distribution is symmetrical about zero.

t_score = (x̄ – μ) / (S/√n)

Then we can calculate the probability for this t-score",tresult tscore,"['tresult', 'tscore']",t-Result or t-Score
480,"F-result or F-Score

> F-result is valid for samples from normal distribution only.

If two independent random samples of size n1 and n2 are taken respectively,

f_score= (sample_1_variance/sample_2_variance)*(population_2_variance/population_1_variance)",fresult fscore,"['fresult', 'fscore']",F-result or F-Score
481,"Point Estimation

> Method of moments- The basic principle is to equate population moments to  corresponding  sample moments and solve for the parameter(s)

> Maximum likelihood estimator- The method of maximum likelihood is widely regarded as the best general method of finding estimators

Point estimation is to estimate a single value of population parameter (mean or variance)",point estim,"['point', 'estim']",Point Estimation
482,"Mean and Expectation

> Mean of a random variable is the simple average of all the values, expectation of a random variable is the  probability-weighted average. 

> Thus, expectation is the value of random variable where the probability is maximum.

> For normal distribution, probability is maximum at mean. So, here mean= expectation

> The expected value of a random variable is the value that has the highest probability of occurrence

> Normal distribution is the plot for feature (height, age, price, temp, speed etc.) vs. its probability density 
Mean of the feature is the middle point of the bell. The  distribution is spreaded in both  side of μ with μ ± σ, μ ± 2σ, μ ± 3σ",mean expect,"['mean', 'expect']",Mean and Expectation
483,"Basics of Confidence interval

Confidence interval helps to determine the range of population parameters from sample parameters

Thus, a confidence interval provides an “interval estimate” of an unknown population parameter (as opposed to a “point estimate”)",basic confid interv,"['basic', 'confid', 'interv']",Basics of Confidence interval
484,"Calculating population parameters for one sample

> When σ is known, then assumming normal distribution of the sample mean, we are 95 % confident that 

μ= X̄ ± 1.96*σ*1/√n

or 
CI= X̄ ± 1.96*σ*1/√n

> When σ is unknown, then assumming t-distribution of the sample mean, we are 95 % confident that 

μ= X̄ ± t0.025, (n-1)*S*1/√n

95% means 100*(1-0.05), Here α=0.05, α/2=0.025

> When σ is unknown, then assumming Chi-square Distribution, we are 95% confident that 

σ^2= (n-1)*S^2/χ2  0.025,(n-1) and (n-1)*S^2/χ2  0.975,(n-1)

> We can calculate population proportion (p or θ) from sample proportion (p̂) with some confidence interval

It is valid only when the binomial distribution (categorical data) can be approximated by normal distribution.

p= p̂ ± 1.96*sqrt(p̂*(1-p̂)/n)",calcul popul paramet one sampl,"['calcul', 'popul', 'paramet', 'one', 'sampl']",Calculating population parameters for one sample
485,"Population proportion for binomial experiment

A population proportion is a fraction of the population that has a certain characteristic.

For example, we had 1,000 people in the population and 237 of those people have blue eyes. The fraction of people who have blue eyes is 237 out of 1,000, or 237/1000 or 23.7%",popul proport binomi experi,"['popul', 'proport', 'binomi', 'experi']",Population proportion for binomial experiment
486,"Calculating population parameters for two samples from difference population

> When σ1 and σ2 are known, then assumming normal distribution of the sample mean, we are 95 % confident that,

μ1- μ2= (X̄1-X̄2) ± 1.96*σp*√(1/n1+1/n2))

σp is the pooled population standard deviation

> When σ1 and σ2 are unknown, then assumming t-distribution of the sample mean, we are 95 % confident that 

μ1- μ2 = (X̄1-X̄2) ± t0.025, (n1+n2-2)*Sp*√(1/n1+1/n2)

Sp is the pooled sample standard deviation

> When σ1 and σ2 are unknown, then we can calculate 

σ1^2/σ2^2

> We can calculate the difference in population proportion from sample proportions  with some confidence interval

For 95% confidence interval,

p1-p2 = (p̂1- p̂2)  ± 1.96*sqrt(p̂1*(1-p̂1)/n1+p̂2*(1-p̂2)/n2)",calcul popul paramet two sampl differ popul,"['calcul', 'popul', 'paramet', 'two', 'sampl', 'differ', 'popul']",Calculating population parameters for two samples from difference population
487,"Pooled variance

Pooled variance is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same as Sp.

Sp^2 = [(n1-1)*S1^2+ (n2-1)*S2^2]/(n1+n2-2)",pool varianc,"['pool', 'varianc']",Pooled variance
488,"Confidence limits

Confidence limits are the values that mark the boundaries of the confidence interval.

95% confidence interval means, the process we used will capture the true parameter 95% of the time in the long run

e.g.
The statement, ""the 95% confidence interval for the population mean is (350, 400)"" means that 95% of the population values are between 350 and 400.

95% confidence level means the probability that the true value of the population parameter falls between the bounds of an already computed confidence interval is roughly 95%.",confid limit,"['confid', 'limit']",Confidence limits
489,"Sampling schemes from best to worst

i. simple random, 

ii. stratified, 

iii. convenience (or close to hand sample)

> A simple random sample randomly selects individuals from the population without any other consideration. 

> A stratified random sample, on the other hand, first divides the population into smaller groups, or strata, based on shared characteristics.",sampl scheme best worst,"['sampl', 'scheme', 'best', 'worst']",Sampling schemes from best to worst
490,"Basics of Hypothesis Testing

> A hypothesis can be defined as a proposed explanation for a phenomenon. It is not the absolute truth but a provisional working assumption.

> In statistics, a hypothesis is considered to be a particular assumption about a set of parameters of a population distribution

> It is called a hypothesis because it is not known whether  it is true or not

A hypothesis test is a standard procedure for testing a claim about a property of a population.

A statement whose validity is tested on the basis of a sample is called Statistical Hypothesis",basic hypothesi test,"['basic', 'hypothesi', 'test']",Basics of Hypothesis Testing
491,"Rare Event Rule for Inferential Statistics

If, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct",rare event rule inferenti statist,"['rare', 'event', 'rule', 'inferenti', 'statist']",Rare Event Rule for Inferential Statistics
492,"Components of a formal hypothesis test

> Given  a  claim,  identify  the  null  hypothesis  and  the  alternative hypothesis, and express them both in symbolic form

> Given a claim and sample data, calculate the value of the test statistic.

> Given a significance level, identify the critical value(s)

> Given a value of the test statistic, identify the P-value

> State  the  conclusion  of  a  hypothesis  test  in  simple,  non-technical terms",compon formal hypothesi test,"['compon', 'formal', 'hypothesi', 'test']",Components of a formal hypothesis test
493,"Null Hypothesis : Ho

The null hypothesis (denoted by Ho) is  a statement that the value of a population  parameter  (such  as  proportion,  mean,  or  standard deviation) is equal to or <=  or  >= some claimed value",null hypothesi ho,"['null', 'hypothesi', 'ho']",Null Hypothesis : Ho
494,"Alternative Hypothesis : HA

The alternative hypothesis (denoted by H1 or Ha or HA) is the statement that  the  statistic  has  a  value  that  somehow  differs  from  the  null hypothesis",altern hypothesi ha,"['altern', 'hypothesi', 'ha']",Alternative Hypothesis : HA
495,"Identifying the null and alternative hypothesis

If  we  are  conducting  a  study  and  want  to  use  a hypothesis test to support our claim, the claim must be worded so that it becomes the alternative hypothesis",identifi null altern hypothesi,"['identifi', 'null', 'altern', 'hypothesi']",Identifying the null and alternative hypothesis
496,"Test Statistic

With the assumption that the null hypothesis is true, we find test statistic from the sample statistics and hypothesised statistics

i. Test statistic for proportions 

z_score= (p̂ -p)/sqrt(p*(1-p)/n)

ii. Test statistic for mean

z_score = (x̄ – μ) / (σ/√n)

or,
t_score = (x̄ – μ) / (S/√n)

for two samples,
t_score= (X̄1-X̄2) - (μ1- μ2)/Sp*√(1/n1+1/n2))

iii Test statistic for variance

chi_square_score= (n-1)*S^2/σ^2

for two samples, 
when, Ho=σ1^2=σ2^2,
f_score=S1^2/S2^2",test statist,"['test', 'statist']",Test Statistic
497,"Significance Level

The significance level (denoted by α) defines how much evidence we require to reject H0 in favor of HA

> significance level is the lowest probability of null hypothesis in the population

> α  is the probability  at test statistic.

confidence level= 1-significance level

For 95% confidence level, significance level is 0.05.

0.95= 1-0.05",signific level,"['signific', 'level']",Significance Level
498,"Critical Region

The critical region (or rejection region) is  a set of values for the test statistic for which the null hypothesis is rejected.",critic region,"['critic', 'region']",Critical Region
499,"Critical Value

A critical value is any value that separates the critical region  from the values of the test statistic that do not lead to rejection of the null hypothesis.  

The critical values depend on the nature of the null hypothesis, the sampling distribution that applies, and the significance level

The two‐tailed critical value will be larger than one tailed test for same significance level.

Z= +1.96 and Z= -1.96 are the critical values for 0.05 significance level in normal distribution",critic valu,"['critic', 'valu']",Critical Value
500,"Two-tailed, Right-tailed, Left-tailed Tests

The tails in a distribution are the extreme regions bounded by critical values.

A two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater or less than a range of values

Right-tailed Tests is a method in which the critical area of a distribution is on the right side

Left-tailed Tests is a method in which the critical area of a distribution is on the left side",twotail righttail lefttail test,"['twotail', 'righttail', 'lefttail', 'test']","Two-tailed, Right-tailed, Left-tailed Tests"
501,"P-value or probability value

The P-value (or probability value) is the probability of observing results as extreme or more extreme than currently observed, given that the null hypothesis is true. 

The level of statistical significance is often expressed as a p-value between 0 and 1. The smaller the p-value, the stronger the evidence that we should reject the null hypothesis.

The null hypothesis is rejected if the P-value is very small, such as 0.05 or less.

If a P-value is small enough, then we say the results are statistically significant

1. Ha ≠ Ho- It is Two-tailed test

if cdf(test_statistic)> 0.5:

p_value=2*(1- cdf(test_statistic))

else:

p_value=2*cdf(test_statistic)

2. Ha> Ho- It is Right-tailed test

p_value= 1- cdf(test_statistic)

3. Ha< Ho- It is Left-tailed test

p_value= cdf(test_statistic)",pvalu probabl valu,"['pvalu', 'probabl', 'valu']",P-value or probability value
502,"Conclusions in Hypothesis Testing based on P-value

We always test the null hypothesis.  The initial conclusion will always be one of the following

i) Reject the null hypothesis - if the P-value ≤ α 

ii) Fail to reject the null hypothesis - if the P-value > α",conclus hypothesi test base pvalu,"['conclus', 'hypothesi', 'test', 'base', 'pvalu']",Conclusions in Hypothesis Testing based on P-value
503,"Type-I error

A Type-I error is the mistake of rejecting the null hypothesis when it is true.",typei error,"['typei', 'error']",Type-I error
504,"Type-II error

A Type-II error is the mistake of failing to reject the null hypothesis when it is false.

β (Beta) is the probability of Type-II error in any hypothesis test",typeii error,"['typeii', 'error']",Type-II error
505,"Power of a hypothesis test

The power of a hypothesis test is the probability (1 - beta) of rejecting a false null hypothesis (correct decision)

That is, the power of the hypothesis test is the probability of supporting an alternative hypothesis that is true.",power hypothesi test,"['power', 'hypothesi', 'test']",Power of a hypothesis test
506,"encoding and decoding

Encoding means the creation of a messages and decoding means listener or audience of encoded message. So, decoding means interpreting the meaning of the message",encod decod,"['encod', 'decod']",encoding and decoding
507,"Quantum computing

> Quantum computing is a type of computation that harnesses the collective properties of quantum states, to perform calculations (massive computing power).",quantum comput,"['quantum', 'comput']",Quantum computing
508,"Blockchain

> Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. ",blockchain,['blockchain'],Blockchain
509,"Degrees of Freedom

Degrees of Freedom refers to the number of logically independent values, which are values that have the freedom to vary, in the data sample. 

degrees of freedom= sample_size-1",degre freedom,"['degre', 'freedom']",Degrees of Freedom
510,"Understanding computer science

Computer science means the knowledge of computer. With the knowledge of computer we can create different functions, models or environments in computer.

> Artificial intelligence is an important part of Computer science

> Machine learning is an important part of Artificial intelligence

> Deep learning is an important part of Machine learning",understand comput scienc,"['understand', 'comput', 'scienc']",Understanding computer science
511,"Understanding convensional programming

Convensional programming is nothing but creating a function, model or environment based on different rules as per our understanding or according to the need.

Non-intelligent robot or collaborative robot is also an example of convensional programming.

",understand convension program,"['understand', 'convension', 'program']",Understanding convensional programming
512,"Types of Computer languages

1. Low level language (binary language)-Machine language (First Generation, 1GL) and Assembly language (Second Generation, 2GL)

2. High Level language (based on low level language, human like language)-3GL(C,C++,Java), 4GL(Perl, Javascript, PHP, Python, Ruby, and SQL), 5GL (contains visual tools to help develop a program)",type comput languag,"['type', 'comput', 'languag']",Types of Computer languages
513,"Programming language (1GL,2GL,3GL) vs scripting language(4GL,5GL)

> Programming language is compiler based and scripting is interpreter based.

> Programming language is difficult to write but scripting language is easy to write and use

> Programming language does not require host, scripting requires host (any computer that has the capability of permitting access to a network)

> C++ programming language needs a compiler (g++)

> Code editor (notepad, Turbo C, VS Code) is needed for writing  code and make .cpp file. Then complier converts .cpp file to .exe file",program languag 1gl2gl3gl vs script language4gl5gl,"['program', 'languag', '1gl2gl3gl', 'vs', 'script', 'language4gl5gl']","Programming language (1GL,2GL,3GL) vs scripting language(4GL,5GL)"
514,"AI technique

AI technique is nothing but implementing small fragments of human like intelligence in the model or environment created by convensional programming. Then the model or environment becomes intelligent model or environment.",artifici intellig techniqu,"['artifici', 'intellig', 'techniqu']",AI technique
515,"Basics of machine learning

The main feature of human intelligence is learning from the experiences. Thus an intelligent environment must have the capability of learning from experiences and machine learning algorithm serves this purpose.

> Human or Machine learning model learns the rule or true function from a noisy realworld experience set.",basic machin learn,"['basic', 'machin', 'learn']",Basics of machine learning
516,"Basics of deep learning

If we need the environment to be intelligent enough to learn from the raw experiences (directly from the sensors) like image, audio, video etc., then we need to implement raw experience processor (a algorithm) to convert raw experiences into organised experience table and deep learning algorithm to learn the rules from the experience table.
",basic deep learn,"['basic', 'deep', 'learn']",Basics of deep learning
517,"Basics of data science

> Data Science means the knowledge of data or reading the experiences

> Knowledge of data is required in the field of computer science, artificial intelligence, machine learning, deep learning and business management.",basic data scienc,"['basic', 'data', 'scienc']",Basics of data science
518,"Role of a data scientist

> A dataset is an experience set and a data scientist is able to understand the features (or Dharma in Sanskrit) of the dataset and their relation.

> Thus, a data scientist can decide the bestfit learning model (algorithm) for a particular problem statement.

> In simple words, a data scientist is like a teacher or guru (in Sanskrit) who is expert in understanding features (or Dharma) and thus expert in training and testing a model.",role data scientist,"['role', 'data', 'scientist']",Role of a data scientist
519,"Difference between Business Analyst and Data Scientist

While a business analyst typically focuses on finding trends in data and developing ways to leverage that information to improve an organization's operations, data scientists tend to look more at what drives those trends.",differ busi analyst data scientist,"['differ', 'busi', 'analyst', 'data', 'scientist']",Difference between Business Analyst and Data Scientist
520,"Predictive ML model

Predictive ML model is a machine learning algorithm which can predict the answer from a new problem statement on completion of training and testing of the model.",predict machin learn model,"['predict', 'machin', 'learn', 'model']",Predictive ML model
521,"Meaning of heuristic technique

A heuristic, or a heuristic technique, is any approach to problem-solving that uses a practical method or various shortcuts in order to produce solutions that may not be optimal but are sufficient given a limited timeframe or deadline.",mean heurist techniqu,"['mean', 'heurist', 'techniqu']",Meaning of heuristic technique
522,"Comparison between Heuristic technique and ML technique

After understanding the dataset for a particular problem statement, a data scientist may decide a heuristic technique (rule based or conventional model) or ML technique (data based or ML model) for problem solving.

Rule based model creates an environment in which data based models operate.",comparison heurist techniqu machin learn techniqu,"['comparison', 'heurist', 'techniqu', 'machin', 'learn', 'techniqu']",Comparison between Heuristic technique and ML technique
523,"Types of learning models

1. Supervised learning model (Regression and classification)

2. Unsupervised learning model (Clustering)

3. Reinforcement learning model (Q-learning, policy learning)",type learn model,"['type', 'learn', 'model']",Types of learning models
524,"Application of regression model

> Real estate prediction

> Weather forecasting

> Financial porfolio prediction

> ETA (Estimated Time of Arrival) etc.",applic regress model,"['applic', 'regress', 'model']",Application of regression model
525,"Application of classification model

> Credit card fraud detection

> Image classification

> Spam detection

> Insurance decisioning etc. ",applic classif model,"['applic', 'classif', 'model']",Application of classification model
526,"Application of Clustering model

> Document theme extraction

> Customer segmentation

> Insurance fraud detection

> Delivery store optimization etc. 

",applic cluster model,"['applic', 'cluster', 'model']",Application of Clustering model
527,"Components of reinforcement learning

> Environment

> Agent

> Action

> State and Reward

- It gathers its own experience from the environment and take action. A reward mechanism works inside and thus a reinforcement learning model learns from mistakes.",compon reinforc learn,"['compon', 'reinforc', 'learn']",Components of reinforcement learning
528,"Application of Reinforcement learning model

> Traffic light control

> Resource management

> Robotics

> Games

> Bidding and advertisement etc.",applic reinforc learn model,"['applic', 'reinforc', 'learn', 'model']",Application of Reinforcement learning model
529,"Supervised, parametric, regression algorithm

> Linear regression is a supervised, parametric, regression algorithm. 

> If the organised experience set have answers or labels, then it is called a supervised experience.

> If the answers are continuous numerical values, then it is regression problem.

> The linear regression model consists of independent variables and a dependent variable related linearly to each other. 

> We try to find the relationship between independent variable(input) and a corresponding dependent variable (output)",supervis parametr regress algorithm,"['supervis', 'parametr', 'regress', 'algorithm']","Supervised, parametric, regression algorithm"
530,"Basics of linear regression (OLS)

If the decesion or answer (dependent variable,Y) from an experience is linearly dependent on the dimensions (independent variables,X1,X2,X3,...Xn), then we can assume that the dependent variable is a function of all individual weights (W0, W1, W2,....Wn). 

Y=W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn+ E

Linear regression is also known as ordinary least squares (OLS) or linear least squares

> The term “least squares” is used because it is the smallest sum of squares of errors

The goal of linear regression is to create a trend line or best fit line

Best fit line is the spinal cord of whole experiences about which the pipeline weights are placed. It is also called the regression line.",basic linear regress ol,"['basic', 'linear', 'regress', 'ol']",Basics of linear regression (OLS)
531,"Error or residuals

The errors between the actual value and the predicted value is called the error or residuals.

> Reason of squaring the errors

Predicted value can have positive or negative error. If we don’t square the error, then the positive and negative points will cancel each other out during summation.  That is why we use SSE : Sum square error

SSE is also called Residual sum square (RSS)",error residu,"['error', 'residu']",Error or residuals
532,"Loss function and cost function

SSE= Σ(y-W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn)^2

SSE= Σ(y_test - y_pred)^2

This is called L2 loss. In this equation, error of prediction for one observation is squared and then it is summed up for all the observations. 

Absolute Error is called L1 loss which is expressed as  Σ|y_test - y_pred|

Loss function is used for single observation and cost fuction (as it provides the cost of  loss from interpreting the data using a linear regression) is used for the entire dataset",loss function cost function,"['loss', 'function', 'cost', 'function']",Loss function and cost function
533,"Types of loss function

Mean Square Error (MSE) or Root Mean Square Error (RMSE) is used instead of SSE to save space and avoid memory explosion with large no. of observations

MSE= 1/n*Σ(y_test - y_pred)^2
RMSE = sqrt(MSE)

Other errors

Mean Absolute Error, Mean Absolute Percentage Error(MAPE), R2 (R – Squared), 
adjusted_r2 =1 - (((n-1)/(n-k-1))*(1-r2_score(actual, predicted)))

> Residual Standard Deviation or Residual Standard Error (RSE)

> RSE is calculated based on RSS and degrees of freedom

> MAPE= np.mean(np.abs((actual - predicted) / actual)) * 100)

> From the RMSE, we can understand the amount of the error in the prediction by our model

> From r2_score, we can understand the perfection of our model (higher R-Squared is better)",type loss function,"['type', 'loss', 'function']",Types of loss function
534,"OLS method for finding out the model parameters

We differentiate loss function first with respect to β0(or W0) and then to β1(or W1) and βn(or Wn)

As per the rule of calculus, setting these partial derivatives equal to zero yields 'n' equation with 'n' unknowns to get the minimum values of β0, β1,....βn .

These equations are known as the normal equations.

Thus we can solve these 'n' equation with matrix method (OLS method).",ordinari least squar method find model paramet,"['ordinari', 'least', 'squar', 'method', 'find', 'model', 'paramet']",OLS method for finding out the model parameters
535,"
Gradient Descent Fundamentals

> Gradient is the slope of cost function and Descent means moving downward

> Gradient Descent is an optimization algorithm which runs on repeated steps (iteration) starting with a random prediction. It helps machine learning models to find the values of the parameters for which the error in prediction will be minimum. It is applied when Parametric ML model is fitted on training data.

> OLS (Non-iterative process) provides exact solution but gradient decent provides approximate but good enough (decent)solution.

> Gradient descent requires additional inputs like alpha and intial random prediction

> Learning rate, α (hyper-parameter) is the step size of moving downward in gradient descent.

> GD is suitable for datasets with large no. of features and observations",gradient descent fundament,"['gradient', 'descent', 'fundament']","
Gradient Descent Fundamentals"
536,"
Assumptions of regression

1.The relation between the dependent and independent variables should be almost linear.

2.Mean of residuals should be zero or close to 0 as much as possible. It ensures that our line is actually the line of “best fit” (running through the middle of the data).

3.There should be homoscedasticity or equal variance in a regression model. This assumption means that the variance around the regression line is the same for all values of the predictor variable (X).

4.There should not be multicollinearity in regression model. Multicollinearity generally occurs when there are high correlations between two or more independent variables.

> from the heatmap we can find the high positive or negative correlations between the independent variables",assumpt regress,"['assumpt', 'regress']","
Assumptions of regression"
537,"Multicollinearity issue

To solve multicollinearity issue:

i.Remove some of the highly correlated independent variables.

ii.Linearly combine the independent variables, such as adding them together.

iii.Perform an analysis designed for highly correlated variables, such as principal components analysis",multicollinear issu,"['multicollinear', 'issu']",Multicollinearity issue
538,"Heteroscedasticity issue

To solve heteroscedasticity issue:

i.Log-transformation of features

ii.Outlier treatment

iii.Try polynomial fit",heteroscedast issu,"['heteroscedast', 'issu']",Heteroscedasticity issue
539,"Properties of regression line

Regression line passes through the mean of independent variable (x) as well as mean of the dependent variable (y).

This means that for mean of x, the actual y and predicted y will be same (zero error).",properti regress line,"['properti', 'regress', 'line']",Properties of regression line
540,"Advantages of linear regression

1.Linear regression is simple to implement and easier to interpret the output coefficients

2.When we know the relationship between the independent and dependent variable is linear, linear regression is the best model, because it’s less complex as compared to other algorithms

3.It works well irrespective of data size",advantag linear regress,"['advantag', 'linear', 'regress']",Advantages of linear regression
541,"Limitations of linear regression

1.Outliers can have huge effect on the regression line.

2.Prone to underfitting (or Overgeneralizing) - Linear regression sometimes fails to capture the underneath pattern in data properly due to simplicity of the algorithm.
",limit linear regress,"['limit', 'linear', 'regress']",Limitations of linear regression
542,"Data preparation for linear regression

1. Linear Assumption: We may need to log transform ""generally natural logarithm"" (on X's) for an exponential relationship-It makes our skewed original data more normal. It improves linearity between our dependent and independent variables. 

2. Gaussian Distributions: Linear regression will make more reliable predictions if our independent and dependent variables have a Gaussian distribution or normal distribution.

3. Remove Collinearity

After analyzing the dataset, we need to find the best possible independent variables (have highest possitive or highest negative correlation with y, means the most linear relations) to make any future predictions about our dependent variable

4. Remove Outlier

5. Rescale Inputs",data prepar linear regress,"['data', 'prepar', 'linear', 'regress']",Data preparation for linear regression
543,"PyTorch

> PyTorch is a machine learning library developed and released by Facebook’s AI group (FAIR)",pytorch,['pytorch'],PyTorch
544,"Omission of relevant variable from a regression equation

If a relevant variable is omitted from a regression equation, the consequences would be

1. The standard errors would be biased

2. If the excluded variable is uncorrelated with all of the included variables, all of the slope and intercept coefficients will be inconsistent

> Including relevant lagged values of the dependent variable on the right hand side of a regression equation could lead to 
Biased but consistent coefficient estimates",omiss relev variabl regress equat,"['omiss', 'relev', 'variabl', 'regress', 'equat']",Omission of relevant variable from a regression equation
545,"Visualizing Linear Regression

We can visualize an experience as a pipeline which consists of indivisual straight pipelines in different dimension. As different dimension has weight, we can assume that the diameter of pipeline is different for different dimension. Thus weight per unit length will be different for different dimension.

Weight per unit length is the feature or property of the dimension. Here it is known as coefficient of dimension (W1,W2,...Wn).

W1, W2,…Wn are also called model parameters.

Here we are considering material and thickness of the pipeline are same for all the dimensions. 

One experience row tells us about different values in deifferent dimensions. Here the values we are assuming as length. Therefore, to get the weight of one indivisual straight pipeline, we need to multiply the weight/length with the length. To get the total weight(y), we will sum up the individual weights in all dimension (W1*X1+W2*X2+ W3*X3+...+Wn*Xn).

Linear regression model learns the properties of all dimension from the experience set.

Intercept, W0 is nothing but a fixed weight of the pipeline which is independent of observations in different dimensions. This also called Bias term. Linear regression model also learns this parameter.

f(X)=W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn is the equation of total predicted weight of the pipeline.

X1, X2,…Xn are the feature values

For building a supervised learning model, the actual total weight of any experience pipeline is known for the train or test data.

The model initially starts with the prediction of any random total weight of the experience pipeline in the the gradient descent. Then, it starts minimizing the error between actual and prediction. Finally the model learns the properties of all dimensions where the total error for total weight prediction is minimum.",visual linear regress,"['visual', 'linear', 'regress']",Visualizing Linear Regression
546,"Understanding of Feature scaling 

Feature scaling  must be performed to bring all data in certain range 

scikit-learn methods to preprocess data

-MinMaxScaler (min 0 and max  1)-a normalization technique 

(x-Min)/Range

-StandardScaler (mean 0 and standard deviation 1)-a standardization technique, 

(x-Mean)/Std. Dev

If required, we can perform MinMaxScaler operation after performing StandardScaler operation, when we want to see the data in same minimum (0) and maximum (1) range

> Unlike normalization, standardization does not have a bounding range.",understand featur scale,"['understand', 'featur', 'scale']",Understanding of Feature scaling 
547,"Difference bwteen Matrix and metric

Matrix is matrix while metric is a measure for something; a means of deriving a quantitative measurement or approximation for otherwise qualitative phenomena (especially used in software engineering)",differ bwteen matrix metric,"['differ', 'bwteen', 'matrix', 'metric']",Difference bwteen Matrix and metric
548,"Libraries for linear regression

from numpy import math

from sklearn.preprocessing import PolynomialFeatures

from sklearn.preprocessing import MinMaxScaler

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error",librari linear regress,"['librari', 'linear', 'regress']",Libraries for linear regression
549,"Importance of csv file

> csv format is used for importing and exporting data in different softwares.

> A csv file can be viewed through notepad, ms word, ms excel, google spreadsheet, sql, python etc.

> Thus a file which is required to be viewed through multiple different softwares, are saved in csv format

> Formats like xls, xlsx, json (JavaScript Object Notation) etc. are also used for import-export purpose",import csv file,"['import', 'csv', 'file']",Importance of csv file
550,"Implementation Steps of Linear Regression

1. Create dummy numerical variables for the catgeorical variable

> Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups. The dummy variables act like 'switches' that turn various parameters on and off in an equation.

dataset['NewYork_State'] = np.where(dataset['State']=='New York', 1, 0)

2. Create two separate dataset(numpy array) for independent variables and dependent variable

dependent_variable = 'Profit'
independent_variables = list(set(dataset.columns.tolist()) - {dependent_variable})

X = dataset[independent_variables].values

y = dataset[dependent_variable].values

3. Splitting both the datasets into the Training set and Test set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

4. Scaling and transforming both the training and test data

5. Training the ML model
regressor = LinearRegression()

regressor.fit(X_train, y_train)

6. Testing the ML Model
y_pred = regressor.predict(X_test)

7. Evaluating the ML Model
mean_squared_error(y_test, y_pred)

r2_score(y_test, y_pred)
> compare the prediction error both for train and test set

8. Explaining the model
regressor.coef_
regressor.intercept_",implement step linear regress,"['implement', 'step', 'linear', 'regress']",Implementation Steps of Linear Regression
551,"transform and fit_transform

>  Why we use fit_transform() on training data but transform() on the test data?

scaler= MinMaxScaler()

X_train_scaled=scaler.fit_transform(X_train)

X_test_scaled= scaler.transform(X_test)

.fit_transform() is the combined method for .fit() and .transform()

fit method calculates the minimum and maximum values for each of the features and transform method transforms all the features using the respective min and max.

Using the transform method we can use the same min and max (mean and variance for StandardScaler) as it is calculated from our training data to transform our test data.",transform fittransform,"['transform', 'fittransform']",transform and fit_transform
552,"Optimal Model

The learning model is called best fit model when the total error (Bias error+Variance error) in prediction on test data is optimal.

Bias and Variance are statistical quantities

Bias and variance are used to analyse the performance of the model on unseen data

> For the best performance of our ML models, there shall be great randomness in the selection of  experiences from the population. 

> The bias or influence in the selection of experiences shall be optimal. Then the sample data will be a true representative of the population data.",optim model,"['optim', 'model']",Optimal Model
553,"Underfit Model

1. When the model is very simple it is called underfit model. Here the total error in prediction is high due to high bias and low variance.

2. Simple model is not able to catch proper signal from the experience. It learns very low weight per unit length for the features and learns a very high fixed weight or intercept value (bias)

> We can call them overgeneralizing model

3. As the weight/length in all the dimensions are small enough, there is a very little impact of the dimensions of the experience. Thus for different experience there will be very little variation in the answer or prediction. This is called low variance.",underfit model,"['underfit', 'model']",Underfit Model
554,"Overfit Model

1. When the model is very complex it is called overfit model. Here the total error in prediction is high due to high variance and low bias.

2. Complex model catches the signal along with noise (outliers or exceptions) from the experience. It provides overimportance to each dimension of the experience and learns very high weight/length for the dimensions of the experience. Thus for different experience there will be huge variation in the prediction. This is called high variance.

3. For a complex model, as the weight/length is very high for the dimensions, the intercept value is very low. This is called low bias.",overfit model,"['overfit', 'model']",Overfit Model
555,"Understanding Estimator

Estimator is the rule or equation learned by the ML model which is a close approximation of the true function hidden or underlying in the experience set.

In other words, an estimator, models the relationship between independent and dependent variable

Thus an estimator is able to estimate or predict the dependent variable for a set of independent variables",understand estim,"['understand', 'estim']",Understanding Estimator
556,"Polynomial model 

In place of linear regression model we can choose polynomial model and train the model on training data

e.g.

list_of_polynomial_degrees = [1, 3, 10]
theta = {}
fit = {}

for degree_of_polynomial in list_of_polynomial_degrees:

  theta[degree_of_polynomial] = np.polyfit(X_train, y_train, degree_of_polynomial)
 
To view the model parameters

for degree_of_polynomial in list_of_polynomial_degrees:

  fit[degree_of_polynomial] = np.polyval(theta[degree_of_polynomial], x_grid)

> x_grid is the instance at which need to evaluate the function",polynomi model,"['polynomi', 'model']",Polynomial model 
557,"Conversion of Categorical column to numerical

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

for k in categorical_columns:

  X = my_df[k].values
  y = label_encoder.fit_transform(X)
  name = k+'_new'
  my_df[name] = y

or, 
my_df = my_df.join(pd.get_dummies(my_df['column_name'],drop_first=True))",convers categor column numer,"['convers', 'categor', 'column', 'numer']",Conversion of Categorical column to numerical
558,"Multi Label columns to Binary

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer(sparse_output=True)

df = df.join(
            pd.DataFrame.sparse.from_spmatrix(
                mlb.fit_transform(df.pop('genres_list')),
                index=df.index,
                columns=mlb.classes_))

> One topic_combination column to all individual topic columns",multi label column binari,"['multi', 'label', 'column', 'binari']",Multi Label columns to Binary
559,"Number-String to numerical value

my_df[list_of columns]=my_df[list_of columns].astype('float64')

or

pd.to_numeric(my_df[list_of columns])",numberstr numer valu,"['numberstr', 'numer', 'valu']",Number-String to numerical value
560,"Noise, Underfittiing, Overfitting and Overgeneralizing 

> Underfitting means giving less important to the features

> Overfitting means giving more importance to the features

> Overgeneralizing means giving more importance to the bias

> There is no noise-free or pure data (experience) in the world, because every data or experience is influenced by some surrounding factors. 

> So, larger the dataset, better will be the understanding of noise-free trend by the learning model. This is called optimal model 

> For smaller dataset, learning model can be simple (looking at small no. of features) or complex (looking at large no. of features). 

> With the increase in training data, Bias increases and Variance decreases for an overfitting model",nois underfitti overfit overgener,"['nois', 'underfitti', 'overfit', 'overgener']","Noise, Underfittiing, Overfitting and Overgeneralizing "
561,"Basics of Regularized Linear Regression

Regularized linear regression is the first option for linear regression problem on small dataset (<1lakh experiences)

When we fit normal linear regression model in our small training data with large no. of features, then there is a chance that the intercept value (bias) is very low and other model coeffients (variances) are high enough, then our model will become a complex model.

Regularized Linear Regression reduces the chance of model complexity

When we fit regularized linear regression with perfect tuning parameter λ, it increases the intercept value and reduces other model parameters towards zero. 

λ is denoted by alpha(α) in sklearn",basic regular linear regress,"['basic', 'regular', 'linear', 'regress']",Basics of Regularized Linear Regression
562,"Types of Regularization

i. Ridge

ii. Lasso

Regularized Liear Regression minimizes the sum of RSS and a ""penalty term""

Thus the cost function for Regularized LR model,

f(model parameters)= RSS+penalty term

Penalty term has a tuning parameter lambda(λ) multiplied with model coefficients. 

It is best to try both regularization and see which one works better. Usually L2 regularization can be expected to give superior performance over L1.

There's also a ElasticNet regression, which is a combination of Lasso regression and Ridge regression.

Lasso regression is preferred if we want a sparse model, meaning that we believe many features are irrelevant to the output.",type regular,"['type', 'regular']",Types of Regularization
563,"Ridge Regression (L2 Regularization)

Ridge means a long, narrow, elevated strip of land.

> Here the penalty term has sum of model coefficient Square

> It has closed form solution (fintie no. of operations)

> Ridge regression shrinks coefficients toward zero, but they rarely reach zero
",ridg regress l2 regular,"['ridg', 'regress', 'l2', 'regular']",Ridge Regression (L2 Regularization)
564,"Lasso Regression (L1 Regularization)

Lasso stands for least absolute shrinkage and selection operator. 

> Here the penalty term has sum of model coefficient Modulus

> Lasso regression shrinks coefficients all the way to zero, thus removing few features from the model.",lasso regress l1 regular,"['lasso', 'regress', 'l1', 'regular']",Lasso Regression (L1 Regularization)
565,"Libraries for Regularized Linear Regression

from sklearn.datasets import load_boston

The Boston housing dataset is a famous dataset from 1970s. It contains 506 observations and 13 features on housing prices around Boston.

from sklearn.linear_model import Ridge, RidgeCV

from sklearn.linear_model import Lasso, LassoCV

from sklearn.preprocessing import StandardScaler
",librari regular linear regress,"['librari', 'regular', 'linear', 'regress']",Libraries for Regularized Linear Regression
566,"Alpha value

my_model=Ridge(alpha=0.1, fit_intercept=True)

my_model=Lasso(alpha=0.1 , max_iter= 3000)

For a very small λ or alpha (near zero), Regularized LR model will act like a normal LR model

For a very large λ or alpha, large coefficients are not penalized 

> normal range of alpha is between 0 and 0.1

In Lasso and Ridge regression as alpha value increases, the slope of the regression line reduces and becomes horizontal (means only have bias, no slope or weight).",alpha valu,"['alpha', 'valu']",Alpha value
567,"Variance Inflation Factor (VIF) 

It quantifies the severity of multicollinearity

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):

    # Calculating VIF

    vif = pd.DataFrame()
    vif[""variables""] = X.columns
    vif[""VIF""] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

calc_vif(dataset[[i for i in dataset.describe().columns if i not in ['car_ID','price']]])

We need to bring the VIF for the final independent numerical feature <10, to get rid of multicolinearity.

By repeated checking of VIF and  heatmap, we remove the independent variables with multicolinearity",varianc inflat factor vif,"['varianc', 'inflat', 'factor', 'vif']",Variance Inflation Factor (VIF) 
568,"Cross validation Basics

Cross validation adds more randomness in the selection of training and testing experience set by subseting the main dataset, to make our model more accurate in prediction. 

Thus helps us to avoid overfitting.",cross valid basic,"['cross', 'valid', 'basic']",Cross validation Basics
569,"Simple Validation vs Cross Validation

> In simple validation we divide the data into two pieces.  80% for training and 20% for testing

> In Cross validation, we divide the data into 'k' pieces or buckets (files) and randomly select any one piece as testing data and the combined rest pieces as training data",simpl valid vs cross valid,"['simpl', 'valid', 'vs', 'cross', 'valid']",Simple Validation vs Cross Validation
570,"k-fold CV

Cross validation is also known as k-fold CV. In k-fold cross validation, we have multiple(k) train-test sets instead of 1. 

This basically means that in a k-fold CV we will be training our model k-times and also testing it k-times. (like multiple unit tests in a class)

Then, we check the cross_validation_score

> Other than k-fold there is stratified Cross Validation (Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label)

> If K=N, then it is called Leave one out cross validation, where N is the number of observations.",kfold cross valid,"['kfold', 'cross', 'valid']",k-fold CV
571,"Python coding for CV

from sklearn.model_selection import cross_val_score

results = cross_val_score(my_model,X_test,y_test, cv = 3)

print(f'[""accuracy"", ""precision"", ""recall""]: {results}')

> classification_report provides simple validation report whereas cross_val_score provides cross validation report

> For K-cross validation, smaller k implies less variance and larger k increases variance.",python code cross valid,"['python', 'code', 'cross', 'valid']",Python coding for CV
572,"yellowbrick CVScores

The results from each evaluation are averaged together for a final score, then the final model is fit on the entire dataset for operationalization

!pip install yellowbrick

from yellowbrick.model_selection import CVScores

visualizer = CVScores(model, cv=3, scoring='f1_weighted')

visualizer.fit(X, y)        
visualizer.show()",yellowbrick cvscore,"['yellowbrick', 'cvscore']",yellowbrick CVScores
573,"Fundamentals of hyperparameters

The parameters apart from model parameters whose values are used to control the learning process are called hyperparameters.

Examples:
n_iter

test_size

max_depth

random_state

n_neighbors

alpha

C

gamma

n_components

karnel

metric

n_folds

penalty

cv

> random_state hyperparameter of sklearn does similar thing as seed function",fundament hyperparamet,"['fundament', 'hyperparamet']",Fundamentals of hyperparameters
574,"Hyperparameters tuning

GridSearchCV is a method to tune the hyperparameters

It trains the model using different combinations of hyperparameters and gives the best combination based on the best k-fold cv score obtained 

GridSearch is very slow (alternate options may be RandomSearchCV or  Bayesian Hyperparameter Optimization)",hyperparamet tune,"['hyperparamet', 'tune']",Hyperparameters tuning
575,"Coding for Hyperparameters tuning

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import RandomizedSearchCV

parameters = {'model_param': list_of_values}

cross_validated_model = GridSearchCV(my_model, param_grid=parameters, scoring='r2', cv=3)

cross_validated_model.fit(X_train,y_train)

cross_validated_model.best_params_

cross_validated_model.best_score_

y_preds = cross_validated_model.predict(X_test)",code hyperparamet tune,"['code', 'hyperparamet', 'tune']",Coding for Hyperparameters tuning
576,"Steps of ML modelling

Step-1: Detailed commenting on problem statement at the begining of the code

Step-2: Installing latest libraries with pip install, if required

Step-3: Importing necessary libraries for using readymade functions

Step-4: Data server connection with notebook

Step-5: Data importing or reading as pandas dataframe

Step-6: Data inspection with pandas methods and functions

Step-7: Initial Data Preparation/Wrangling/ cleaning and saving the file as different csv

Data cleaning activities include:

> Dealing with missing values

> Dealing with outliers

> Correcting typos

> Grouping sparse classes

> Dropping duplicates

Step-8: Initial data exploration with pandas profiling 

Step-9: Final Data Preparation/wrangling  with transformation

Data Transformation activities and techniques include:

> Categorical encoding

> Dealing with skewed data

> Scaling

> Bias mitigation

> Rank transformation

> Power functions

Step-10: Feature Engineering by creating new feature based upon knowledge about current features and required task

Step-11: Final data exploration through data visualization tools

Step-12: Correct ML model selection or solution of problem statement from the understanding of data
 
Step-13: Train-Test spliting of the data

Step-14: Training the ML model for learning the pattern of the data on fitting training experience set

Step-15: Testing the ML model for evaluating its understanding of the trend on fitting testing experience set

Step-16: Improving the ML model through Cross validation and hyperparameter tuning

Step-17: Retraining and Retesting the model for further evaluation

Step-18: Checking the performance of the model in realtime deployment",step machin learn model,"['step', 'machin', 'learn', 'model']",Steps of ML modelling
577,"Understanding Warnings

Warnings are provided to warn the developer of situations that aren’t necessarily exceptions. Usually, a warning occurs when there is some obsolete of certain programming elements, such as keyword, function or class, etc. A warning in a program is distinct from an error. Python program terminates immediately if an error occurs. Conversely, a warning is not critical.

import warnings

warnings.filterwarnings('ignore')",understand warn,"['understand', 'warn']",Understanding Warnings
578,"Data preparation or data preprocessing

Data preparation (also referred to as “data preprocessing”) is the process of transforming raw data so that data scientists and analysts can run it through machine learning algorithms to uncover insights or make predictions.

Cross-validation gives a more accurate measure of model quality, which is especially important if we are making a lot of modeling decisions.

Before applying any transformation, we need to perform EDA to understand the shape of the distribution

Seaborn distplot or histogram plot may used to understand the probability distribution

To see all the shape of the data in a single graph, we can use pairplot and specify the kind of non-diagonal graphs, kind of diagonal graph and hue for target_feature",data prepar data preprocess,"['data', 'prepar', 'data', 'preprocess']",Data preparation or data preprocessing
579,"Linear Transformation or Scaling

MinMaxScaler or StandardScaler are the linear transformations.

Linear transformation only does scaling, it has nothing to do with skewness of data

For MinMax scaling:

X_new= (X- min(X)) /(Max(X)-Min(X))

For Standard scaling:

X_new= (X- mean(X)) / Std(X)",linear transform scale,"['linear', 'transform', 'scale']",Linear Transformation or Scaling
580,"Non-linear Transformations (for dealing with Sckewed data)

> square-root for moderate skew: 

sqrt(x) for positively skewed data, 

sqrt(max(x+1) - x) for negatively skewed data

> log for greater skew: 

log10(x) for positively skewed data, 

log10(max(x+1) - x) for negatively skewed data

np.log10(my_df['column_name']

> inverse for severe skew: 

1/x for positively skewed data 

1/(max(x+1) - x) for negatively skewed data

np.reciprocal(my_df['column_name']

> After the transformation, the distribution becomes more approximate to normal distribution.

> After taking log, zero values in some of columns are transformed with -Inf 

np.isinf(df[col_name]).values.sum()

> Normalization is good to use when we know that the distribution of our data does not follow a Gaussian distribution.

Linearity and heteroscedasticity:

First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable

If our data does the opposite – dependent variable decrease more rapidly with increasing independent variable – we can first consider a square-root transformation.",nonlinear transform deal sckew data,"['nonlinear', 'transform', 'deal', 'sckew', 'data']",Non-linear Transformations (for dealing with Sckewed data)
581,"Basics of Logistic regression 

Logistic regression  is the first option for linear classification problem on small dataset (<1lakh experiences)

Logistic regression is a supervised, parametric, classification algorithm (non-linear regression technique). 

Why it is called regression?

In both the Logistic regression and Linear regression, the predicted value of y, either continuously increases or decreases with the increase in independent variable.

But in Logistic Regression, the continuous target values are only a limited number (generally forced to give two target values)

>> Logistic regression does not need variables to be normally distributed for good performance",basic logist regress,"['basic', 'logist', 'regress']",Basics of Logistic regression 
582,"Meaning of odds and logit function in probability

The odds are defined as the probability that the event will occur (P) divided by the probability that the event will not occur (1-P)

Here, we are considering the probability of dependent variable,Y as P

Odds of event = P/(1-P)

Logit of event, Y= Log-odds= Log(P/(1-P))

Y= Log(P/(1-P)) is called the Logit fuction",mean odd logit function probabl,"['mean', 'odd', 'logit', 'function', 'probabl']",Meaning of odds and logit function in probability
583,"Logistic function or sigmoid function

In linear regression, the value of event, Y is a linear equation. 

Value of event, Y= w0+w1*x1+w2*x2+…wn*xn

In logistic regression, the logit of event, Y is a linear equation.

Logit of event, Y= w0+w1*x1+w2*x2+…wn*xn

or,
Log(P(Y)/(1-P(Y)))=w0+w1*x1+w2*x2+…wn*xn 

-This is called the logistic function or sigmoid function

Log(p/1-p)=W.T*x (By convention, we can assume that  x0=1)",logist function sigmoid function,"['logist', 'function', 'sigmoid', 'function']",Logistic function or sigmoid function
584,"Working of parametric model

Parametric algorithms involve two steps when ml_model_name.fit method is applied on the training data:

1. It selects a form for the function as called.

2. Learns the model parameters for the function by running a gradient descent (or maximum likelihood estimator) algorithm to make the total error minimum.",work parametr model,"['work', 'parametr', 'model']",Working of parametric model
585,"Benefits of Parametric ML models:

1. Simpler: These methods are easier to understand and interpret results.

2. Speed: Parametric models are very fast to learn from data.

3. Less Data: They do not require large training data ",benefit parametr machin learn model,"['benefit', 'parametr', 'machin', 'learn', 'model']",Benefits of Parametric ML models:
586,"Limitations of Parametric ML Models:

1. Constrained: By choosing a functional form these methods are highly constrained to the specified form.

2. Limited Complexity: The methods are more suited to simpler problems.

3. Poor Fit: A real data never perfectly follow a true function, but a data scientist call a ML function based on final EDA. Thus, the methods are unlikely to perfectly match the underlying  trend in the training data.

> Neural network is an expection",limit parametr machin learn model,"['limit', 'parametr', 'machin', 'learn', 'model']",Limitations of Parametric ML Models:
587,"Working of Non-parametric model 

Non-parametric algorithm does the following step when ml_model_name.fit method is applied on the training data:

It selects a method as called to best fit the training data in constructing the mapping function.",work nonparametr model,"['work', 'nonparametr', 'model']",Working of Non-parametric model 
588,"Benefits of Nonparametric Machine Learning Algorithms:

1. Flexibility: Capable of fitting a large number of functional forms.

2. Power: No assumptions (or weak assumptions) about the underlying function.

3. Performance: Can result in higher performance models for prediction.",benefit nonparametr machin learn algorithm,"['benefit', 'nonparametr', 'machin', 'learn', 'algorithm']",Benefits of Nonparametric Machine Learning Algorithms:
589,"Limitations of Nonparametric Machine Learning Algorithms:

1. More data: Require a lot more training data to estimate the mapping function.

2. Slower: A lot slower to train as they often have far more parameters to train.

3. Overfitting: More of a risk to overfit the training data",limit nonparametr machin learn algorithm,"['limit', 'nonparametr', 'machin', 'learn', 'algorithm']",Limitations of Nonparametric Machine Learning Algorithms:
590,"Classification model and probability

A probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to.

It is possible for every probabilistic method to simply return the class with the highest probability and therefore seem deterministic.",classif model probabl,"['classif', 'model', 'probabl']",Classification model and probability
591,"Generative models and Discriminative models 

Generative models (learns joint probability, p(x,y) and outputs conditional probability, p(y|x))-able to predict for unseen data point:

1. Linear Regression

2. Naive Bayes

3. Hidden Markov Models

Deterministic or Discriminative models (learns conditional probability, p(y|x) and outputs conditional probability, p(y|x))-Estimate parameters directly from training data:

1. Logistic Regression

2. KNN

3. Decision Tree

4. Neural Network (can also become generative)

5. SVM (Non-Probabilistic-does not give the probability for prediction)

> Generative models are less accurate than discriminative models 

> Generative models can work with missing data, and discriminative models generally can’t.

> Compared with discriminative models, generative models need less data to train.",generat model discrimin model,"['generat', 'model', 'discrimin', 'model']",Generative models and Discriminative models 
592,"Coding for Logistic regression 

from sklearn import metrics

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix

from sklearn.metrics import f1_score

from sklearn.metrics import log_loss

# log_loss is indicative of how close the prediction probability is to the corresponding actual/true value. A good model should have a smaller log_loss value (can never be negative).

# large log-likelihood statistic (parallel to F-test in OLS regression) indicates that the statistical model is a poor fit of the data.

# sklearn provides max 3 classes

# Training Logistic regression model

my_logistic_regress_mdl = LogisticRegression(fit_intercept=True, max_iter=10000)

my_logistic_regress_mdl.fit(X_train, y_train)

my_logistic_regress_mdl.coef_

my_logistic_regress_mdl.intercept_

# Evaluating the performance of Logistic regression model

train_accuracy = accuracy_score(y_pred_train,y_train)

test_accuracy = accuracy_score(y_pred_test,y_test)

my_cm = confusion_matrix(y_train, y_pred_train)

sns.heatmap(my_cm, vmin=0, cmap='Greens', annot=True)

f1_score(y_test, y_pred_test, average='macro')",code logist regress,"['code', 'logist', 'regress']",Coding for Logistic regression 
593,"Confusion Matrix

Also known as error matrix

It is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one

In unsupervised learning it is usually called a matching matrix

> Evaluation through confusion matrix

Confusion matrix finds, in how many cases our classification model is  correct or incorrect in predicting the target class (TP,TN,FP,FN). 

Thus we can interpret the performance of the model more easily.

Many times, accuracy of the model is not enough to judge the performace, then we also need to check precision and recall

condition positive (P): the number of real positive cases in the data

condition negative (N): the number of real negative cases in the data

true positive (TP): eqv. with hit

true negative (TN): eqv. with correct rejection

false positive (FP): eqv. with false alarm, type I error or underestimation

false negative (FN): eqv. with miss, type II error or overestimation

> For regression model evaluation, there is RMSE, MSE, R2 etc.",confus matrix,"['confus', 'matrix']",Confusion Matrix
594,"Accuracy, Precision, Recall and F1-Score

Accuracy= (TP+TN)/(TP+TN+FP+FN)

Precision= TP/(TP+FP)

Recall (sensitivity or hit rate)= TP/(TP+FN)

F1-Score 

This is defined as the harmonic mean of precision and recall.

F1=2*Precision*Recall/(Precision+Recall)",accuraci precis recal f1score,"['accuraci', 'precis', 'recal', 'f1score']","Accuracy, Precision, Recall and F1-Score"
595,"Importance of F1 Score

> F1 score penalizes the extreme values.

> Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial

> Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.

> In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric for evaluation.",import f1 score,"['import', 'f1', 'score']",Importance of F1 Score
596,"Checking Cross-validation scores

from sklearn.linear_model import LogisticRegressionCV

from sklearn.model_selection import cross_validate

The cross_validate function differs from cross_val_score in two ways: cross_validate function allows specifying multiple metrics for evaluation. It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.",check crossvalid score,"['check', 'crossvalid', 'score']",Checking Cross-validation scores
597,"List of ML models

1. For regression problems (supervised)

1A. Linear Regressors

i. Ridge,Lasso or ElasticNet (parametric)

ii. SVM (parametric)

iii. Neural Network-SGD (parametric)

1B. Non-linear Regressors

i. Ensembles of Decision Trees (non-parametric)

2. For Classification problems (supervised)

2A. Linear Classifers

i. SVM (parametric)

ii. Neural Network-SGD (parametric)

2B. Non-linear Classifiers

i. Logistic regression (parametric)-rarely used

ii. KNN (non-parametric)

iii. Naive Bayes (parametric)

iv. Ensembles of Decision trees (non-parametric)

v. Neural Network-SGD+Kernel approximation (parametric)

3. For Clustering problems (unsupervised)

i. K-Means Clustering (non-parametric)

ii. DBSCAN (non-parametric)

iii. Gaussian Mixture Model (GMM) (parametric)

iv. Hierarchical Clustering

Non-linear classifiers are the most important one among all groups of ML models. 

In real life, classifying a large experience set with large number of features is the most important one. 

And the underlying rule of this classification never follow a true function. 

Thus a non-linear classifier is employed.",list machin learn model,"['list', 'machin', 'learn', 'model']",List of ML models
598,"Important Terminology in Decision Tree ML Model

> Root node: It represents entire population or sample and this further gets divided into two sets.

> Decision node: When a sub-node splits further into sub-nodes, then it is called decision node.

> Leaf/Terminal node: Nodes do not split are called leaf or terminal nodes.

> Splitting: It is a process of dividing a node into two sub-nodes.

> Branch or Sub-tree: A sub section of entire tree is called branch or sub-tree.

> Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes whereas sub-nodes are the child of parent node.

> Pruning: When we remove sub-nodes of a decision node, this process is called pruning. we can say it the opposite process of splitting.",import terminolog decis tree machin learn model,"['import', 'terminolog', 'decis', 'tree', 'machin', 'learn', 'model']",Important Terminology in Decision Tree ML Model
599,"Steps of Decision tree algorithm

1. It takes the entire dataset as root node and finds the most important feature for the decision of classification of experiences and splits the whole sample dataset into two or more datasets (sub-node or child node)

2. Then each subset of data is further splitted in two or more sebsets, on the basis of second important feature.

3. The process of splitting goes on till the algorithm reach leaf or terminal node of most similar class of experiences. 
 
4. Thus the algorithm learns a treelike sequence and criteria of decisions from the training data.

5. Then the model is fitted with the testing experience set to check the performance scores.

6. When we give one real time experience to the decision tree model, it checks the decision criterion as learned and shows the class for the experience.

Decision tree is also referred to as Recursive (recurrence or repetition) partitioning",step decis tree algorithm,"['step', 'decis', 'tree', 'algorithm']",Steps of Decision tree algorithm
600,"Methods to measure the similarity of child nodes:

How a decision tree model finds the most important feature for the decision of spliting parent node into child nodes? 

Decision tree splits the parent node on all available features and then selects the split which results in most homogeneous or similar child nodes.

> Following methods are called splitting criterion in the syntax

Information Gain (criterion='entropy' in sklearn)

Gini (criterion='gini' in sklearn is the default criterion)

Chi Square (required to write the code)

Reduction in Variance (required to write the code)-used for continuous target variables that are used for regression problems

Gain ratio

>> Decision node illustrates Test specification in a decision tree

>>  Gini-Index and Entropy refer to ""Purity"" of our data points

>> Gini-Index = 1 signifies that all the elements are randomly distributed across various classes, and. Gini-Index = 0.5 denotes the elements are uniformly distributed into some classes.

>> The entropy of a node decreases as we go down a decision tree. 

>> Gini coefficient, Gini-Index and Gini ratio are the measure of statistical dispersion intended to represent the income inequality or the wealth inequality within a nation or a social group.",method measur similar child node,"['method', 'measur', 'similar', 'child', 'node']",Methods to measure the similarity of child nodes:
601,"Fitting Decision tree classifier

from sklearn.tree import DecisionTreeClassifier

X = dataset[independent_variables]

y = dataset[dependent_variable]

my_classifier = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=10, random_state=0)

my_classifier.fit(X_train, y_train)

min_sample_split is the minimum no. of sample required for a split. 

For instance, if min_sample_split = 5 and there are 7 samples in the node and say the split happens one with 1 sample and other with 6 samples

min_sample_leaf is the minimum no. of sample required to be a leaf node. 

In the above example, if min_sample_leaf = 2, then the split will not happen because here one node has 1 sample

> min_sample_leaf is positively correlated with overfitting. This is a similar effect like max_depth",fit decis tree classifi,"['fit', 'decis', 'tree', 'classifi']",Fitting Decision tree classifier
602,"Fitting Decision tree regressor

from sklearn.tree import DecisionTreeRegressor

my_regressor = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=10, random_state=0)

my_regressor.fit(X_train, y_train)

> Full form of CART  is Classification And Regression Trees",fit decis tree regressor,"['fit', 'decis', 'tree', 'regressor']",Fitting Decision tree regressor
603,"Coding for Visualizing Decision Tree

from sklearn.tree import DecisionTreeClassifier, export_graphviz

from sklearn import tree

from IPython.display import SVG

from graphviz import Source

from IPython.display import display

graph = Source(tree.export_graphviz(humidity_classifier, out_file=None, feature_names=X_train.columns, class_names=['0', '1'], filled = True))

display(SVG(graph.pipe(format='svg')))

>> Here, decision nodes are represented by square shape",code visual decis tree,"['code', 'visual', 'decis', 'tree']",Coding for Visualizing Decision Tree
604,"Advantages of Decision tree

1. Decision tree output is very easy to understand

2. Many times it is used as EDA or Data mining tool, as Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables.

3. Less data cleaning required as it is not influenced by outliers and missing values to a fair degree.

4. Possible Scenarios can be added",advantag decis tree,"['advantag', 'decis', 'tree']",Advantages of Decision tree
605,"Disadvantages of Decision tree

High chance of overfitting as the model is a true mapping of training experiences and it does not use any probability of events.

This single disadvantage can be overcome by making a decision through a group of decision trees.",disadvantag decis tree,"['disadvantag', 'decis', 'tree']",Disadvantages of Decision tree
606,"Basics of Ensembles of decision trees 

Ensembles of decision trees is the last option for non-linear regression/ classification problems on small dataset (<1lakh experiences) when KNN or Naive Bayes are not working well",basic ensembl decis tree,"['basic', 'ensembl', 'decis', 'tree']",Basics of Ensembles of decision trees 
607,"Ensemble techniques

Meaning of ensemble is group. The concept is, a group of weak learners (which are trained on less data) come together to form a strong learner.

There are three ensemble techniques:

1. Bagging

2. Boosting

3. Stacking

>> Decrease the fraction of samples to build base learners will result in decrease in variance",ensembl techniqu,"['ensembl', 'techniqu']",Ensemble techniques
608,"Bagging technique

It reduces the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same dataset.

This is similar to asking same question  to different persons and averaging the answers for the final answer.

Random Forest is an ensemble decision tree algorithm which uses bagging technique to reduce the chance of overfitting.

>> Individual tree is built on a subset of the features and subset of observations

>> It perform similar operations as dropout in a neural network",bag techniqu,"['bag', 'techniqu']",Bagging technique
609,"Boosting technique

There are many boosting algorithms which impart additional boost to model’s accuracy:

Gradient Boosting 

XGBoost

AdaBoost

LightGBM

CatBoost

In Random Forest, trees are side by side means parallel, whereas in Boosting, multiple trees are in sequence.

Boosting fit a sequence of weak learners (with sub-samples). More weight is given to examples that were misclassified by earlier rounds.",boost techniqu,"['boost', 'techniqu']",Boosting technique
610,"Extreme Gradient Boosting 

Extreme Gradient Boosting (XGBoost) is just an extension of gradient boosting. Advantages are:

1. Regularization,

2. Parallel Processing

3. High Flexibility

4. Handling Missing Values

5. Tree Pruning

6. Built-in Cross-Validation

7. Continue on Existing Model",extrem gradient boost,"['extrem', 'gradient', 'boost']",Extreme Gradient Boosting 
611,"Stacking technique

In stacking, trees are being arranged in two levels: 

Level 0 (Base models): These trees are sequential like boosting, but all trees are trained on same numbers of experiences like original dataset.

Level 1 (Meta model): This tree learns how to best combine the predictions of the base models. This tree is absent in boosting technique.

>> Stacking is best in case of limited training data, because each classifier is trained on all of the available data",stack techniqu,"['stack', 'techniqu']",Stacking technique
612,"Python coding for Ensembles of decision trees

from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import GradientBoostingClassifier

from xgboost import XGBClassifier

rf = RandomForestClassifier(n_estimators=100)

model = XGBRegressor(learning_rate =.1, n_estimators=10,
                     max_depth=2, min_child_weight=3, gamma=0, 
                     subsample=.8, colsample_bytree=.7, reg_alpha=1, 
                     objective= 'reg:linear')

>> Extra Trees (Extremely Randomized Trees) and Random Forest function does not use learning_rate hyperparameter

>> Increasing the value of max_depth may overfit the data

from sklearn.ensemble import StackingRegressor

estimators = [('lr', RidgeCV()), ('svr', LinearSVR(random_state=42))

reg = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=10, random_state=42))",python code ensembl decis tree,"['python', 'code', 'ensembl', 'decis', 'tree']",Python coding for Ensembles of decision trees
613,"Finding feature importance

from sklearn.ensemble import ExtraTreesRegressor

my_model=ExtraTreesRegressor()

my_model.fit(X_train,y_train)

my_model.feature_importances_

> It returns feature importance scores",find featur import,"['find', 'featur', 'import']",Finding feature importance
614,"Classification report

It is very useful for multiclass classification problem

from sklearn.metrics import classification_report

clf_rpt = classification_report(Y_validation,y_pred)

print(""classification report :"", clf_rpt)

> It gives precision,    recall,  f1-score etc.
",classif report,"['classif', 'report']",Classification report
615,"Ways to improve random forest accuracy

Algorithm Tuning

Add more data

Feature Selection",way improv random forest accuraci,"['way', 'improv', 'random', 'forest', 'accuraci']",Ways to improve random forest accuracy
616,"Basics of Model Explainability

Explainability in machine learning means that we can explain what happens in our model from input to output. It makes models transparent and solves the black box problem. Explainable AI (XAI) is the more formal way to describe this and applies to all artificial intelligence.",basic model explain,"['basic', 'model', 'explain']",Basics of Model Explainability
617,"Black Box Model vs. White Box Model

Black-box models, such as deep-learning (deep neural network), boosting, and random forest models, are highly non-linear by nature and are harder to explain in general. With black-box models, users can only observe the input-output relationship.

White-box models are the type of models in which one can clearly explain how they behave, how they produce predictions, and what the influencing variables are.",black box model vs white box model,"['black', 'box', 'model', 'vs', 'white', 'box', 'model']",Black Box Model vs. White Box Model
618,"Explainable AI

Explainable AI is about understanding ML models better. How they make decisions, and why. The three most important aspects of model explainability are:

1. Transparency

2. Ability to question

3. Ease of understanding",explartifici intelligencen artifici intellig,"['explartifici', 'intelligencen', 'artifici', 'intellig']",Explainable AI
619,"Importance of explainability

Explainability connects the data science team and non-technical team, improving knowledge exchange, and giving all stakeholders a better understanding of product requirements and limitations.

Reasons:

1. Accountability

2. Trust

3. Compliance

4. Performance

5. Enhanced control",import explain,"['import', 'explain']",Importance of explainability
620,"A. Scope of explainability

1. Global : This is the overall explanation of model behavior. It shows us a big picture view of the model, and how features in the data collectively affect the result.

2. Local : This tells us about each instance and feature in the data individually (kind of like explaining observations seen at certain points in the model), and how features individually affect the result.",scope explain,"['scope', 'explain']",A. Scope of explainability
621,"B. Approach of explainability

i) Model-Agnostic Approach: It works across all types of models

ii) Model-Specific Approach:  It is tailor-made for a particular class of algorithms

Explainable models

1. Linear models

2. Decision Tree Algorithms

3. Generalized Additive Models (GAM)-it is like linear model, but X is another polynomial function. Example is Splines
",b approach explain,"['b', 'approach', 'explain']",B. Approach of explainability
622,"Techniques or Libraries for Explainability in ML (mainly for black box models)

> LIME

> SHAP

> ELI5",techniqu librari explain machin learn main black box model,"['techniqu', 'librari', 'explain', 'machin', 'learn', 'main', 'black', 'box', 'model']",Techniques or Libraries for Explainability in ML (mainly for black box models)
623,"Local Interpretable Model-Agnostic Explanations (LIME)

It can give us the top most feature weights which are influencing (positively and negatively) the output

!pip install lime

import lime

import lime.lime_tabular

from __future__ import print_function

> This technique lie under model-agnostic approach and scope is local",local interpret modelagnost explan lime,"['local', 'interpret', 'modelagnost', 'explan', 'lime']",Local Interpretable Model-Agnostic Explanations (LIME)
624,"Shapley Additive Explanations (SHAP)

The concept is a mathematical solution for a game theory problem – how to share a reward among team members in a cooperative game?

1. TreeExplainer - for the analysis of decision trees

2. DeepExplainer - for the deep learning algorithms (less advance)

3. KernelExplainer - for most of the algorithms

> Shapely means having an attractive or well-proportioned shape (especially of a woman or part of her body).",shapley addit explan shap,"['shapley', 'addit', 'explan', 'shap']",Shapley Additive Explanations (SHAP)
625,"Implementing SHAP

!pip install shap

import shap

shap.initjs()

shap_values = shap.TreeExplainer(model_rf_final).shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=list(X.columns), plot_type=""bar"")

shap_values = shap.KernelExplainer(model_svr_final.predict, X_train[100:150], link='identity').shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=list(X.columns), plot_type=""bar"")

There are currently four types of Summary Plots in SHAP: dot, bar, violin, and compact dot

plot_type=""dot""

plot_type=""bar""

plot_type=""violin""

plot_type=""compact dot""

Other plots in SHAP

shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0])

for elem in top_features:
  shap.dependence_plot(elem, shap_values, X_train)",implement shapley addit explan,"['implement', 'shapley', 'addit', 'explan']",Implementing SHAP
626,"Explain Like I'm 5 (ELI5)

It is an unified API and very straight forward in giving the explanation.

!pip install eli5

import eli5 as eli

eli.explain_weights(model_rf_final,feature_names=list(X.columns))",explain like im 5 eli5,"['explain', 'like', 'im', '5', 'eli5']",Explain Like I'm 5 (ELI5)
627,"Other techniques for Explainability in ML

1. Partial Dependence Plots (PDP)

2. Individual Condition Expectations plots (ICE)

3. Leave One Column Out (LOCO)

4. Accumulated Local Effects (ALE)

5. Yellowbrick

6. Lucid

7. Anchors

8. Deep Learning Important Features (DeepLIFT)

9. Layer-wise relevance propagation (LRP)

10. Contrastive Explanations Method (CEM)

11. ProfWeight etc.",techniqu explain machin learn,"['techniqu', 'explain', 'machin', 'learn']",Other techniques for Explainability in ML
628,"Basics of KNN

kNN is the first option for non linear classification problems on small non-text dataset (<1lakh experiences)

May be applied for regression problems. Then it takes the mean value of k closest points.

> k-Nearest Neighbors is one of the simplest supervised learning algorithms and it is robust to the noisy training data

> kNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data",basic knearest neighbor,"['basic', 'knearest', 'neighbor']",Basics of KNN
629,"Euclidean Distance

The Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance.

Euclidean distance between two 'n' dimensional experiences or data points=sqrt((X11-X21)^2+(X12-X22)^2+...(X1n-X2n)^2)

Euclidean distance between two n-dimensional experiences treats each feature or dimension as equally important

> Manhattan distance can be used for continuous variables

> Other distance metrices for kNN

Tanimoto

Jaccard",euclidean distanc,"['euclidean', 'distanc']",Euclidean Distance
630,"Working of kNN

Step-1: Select the number K of the neighbors

Step-2: Calculate the Euclidean distances among all data points

Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.

Step-4: Among these k neighbors, count the number of the data points in each category.

Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.",work knearest neighbor,"['work', 'knearest', 'neighbor']",Working of kNN
631,"Ways to select the value of k in the kNN Algorithm

There is no particular way to determine the best value for ""k"", so we need to try some values to find the best out of them. The most preferred value for k is 5. 

Best value of k may be found out for best accuracy  score with the help of cross validation

>> The boundary becomes smoother with increasing value of K

>> When we increase value of k, the bias increases (simplification of the model)",way select valu k knearest neighbor algorithm,"['way', 'select', 'valu', 'k', 'knearest', 'neighbor', 'algorithm']",Ways to select the value of k in the kNN Algorithm
632,"Disadvantages of kNN Algorithm:

> Always needs to determine the value of K which may be complex some time.

> Computationally expensive because of calculating the distance between the data points for all the training samples.

>  kNN is very likely to overfit due to the curse of dimensionality (dimensionality reduction and feature selection shall be used beforing fitting kNN model)

> Accuracy largely depends on the quality of the data.",disadvantag knearest neighbor algorithm,"['disadvantag', 'knearest', 'neighbor', 'algorithm']",Disadvantages of kNN Algorithm:
633,"Python coding for KNN

from sklearn.neighbors import KNeighborsClassifier

my_knn = KNeighborsClassifier(n_neighbors=5)

my_knn.fit(X_train,y_train)

my_knn.score(X_test,y_test)

from sklearn.metrics import confusion_matrix,roc_curve

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test,test_preds)

> leaf_size is another important hyperparameter for KNN model",python code knearest neighbor,"['python', 'code', 'knearest', 'neighbor']",Python coding for KNN
634,"Receiver operating characteristic and AUC

The ROC is also known as a relative operating characteristic, because it is a comparison of two operating characteristics (TPR and FPR).

AUC means area under the curve. So to get ROC-AUC score we need to define ROC curve first.

AUC is also known as AOC (Area under operating characteristic curve)

The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.

> Thus, a good model should have roc_auc_score > 0.5",receiv oper characterist area curv,"['receiv', 'oper', 'characterist', 'area', 'curv']",Receiver operating characteristic and AUC
635,"Knn for recommender system

from scipy.sparse import csr_matrix

my_df_matrix = csr_matrix(X.values)

from sklearn.neighbors import NearestNeighbors

model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')

model_knn.fit(my_df_matrix)

query_index = np.random.choice(X.shape[0])

distances, indices = model_knn.kneighbors(np.array(X.iloc[query_index,:].values).reshape(1, -1), n_neighbors = 6)",knearest neighbor recommend system,"['knearest', 'neighbor', 'recommend', 'system']",Knn for recommender system
636,"Sparse matrix and Dense matrix

> We convert  normal 2D Array with mostly zeros to csr_matrix (sparse matrix)

> Sparse matrices are memory efficient data structures that enable us store large matrices with very few non-zero elements. It does not store the zero values.

> We can not view a sparse matrix type.

> Dense matrix are normal matrix which stores all the zero and non-zero values

> CSR stands for compressed sparse row.

> So, for converting sparse matrix  to dense matrix 

sparse_matrix.todense()

> NearestNeighbors() model can take X in the form of dense matrix but using sparse matrix  makes it more efficient in computation",spars matrix dens matrix,"['spars', 'matrix', 'dens', 'matrix']",Sparse matrix and Dense matrix
637,"Basics of Naive Bayes

Naive Bayes is the first option for non linear classification problems on small text dataset (<1lakh experiences)

> No need to convert categorical column to numerical column

> Naive means showing a lack of experience, wisdom, or judgement.",basic naiv bay,"['basic', 'naiv', 'bay']",Basics of Naive Bayes
638,"Bayes theorem

Perhaps the easiest Naive Bayes classifier to understand is Gaussian Naive Bayes. 

In this classifier, the assumption is that data for each label is drawn from a simple Gaussian distribution.

> The Naive Bayesian classifier is based on Bayes' theorem

Bayes' theorem provides a way to revise existing predictions or theories (update probabilities) given new or additional evidence.

P(Ej) -> Prior Probabilities

P(Ei | A) -> Posterior Probability

P(C / X)= P(X / C) *P(C) / P(X)

We calculate posterior probability with the help of prior probability and additional evidence event A.

Here, 
P(C / X) or P(yes/sunny)= postrior probability,

P(X / C) or P(sunny/yes) = likelihood

P(C) or P(yes)=class prior probability

P(X) or P(sunny)=predictor prior probability

> Baye's theorem or rule can be used for answering probabilistic query",bay theorem,"['bay', 'theorem']",Bayes theorem
639,"Understanding of Naive Bayes

Naive Bayes is a generative model and is very fast and have very few tunable parameters

For Naive Bayes model

>> attributes are equally important.

>> attributes are statistically independent of one another (This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems)

>> attributes can be nominal or numeric

>> when an attribute value in the testing record has no example in the training set, then the entire posterior probability will be zero.",understand naiv bay,"['understand', 'naiv', 'bay']",Understanding of Naive Bayes
640,"Text Pre-processing

> The classification algorithms need some sort of numerical feature vector.

> For the conversion of text to numerical value, the simplest method is the bag-of-words approach (CountVectorizer is also called bag-of-words), where each unique word in a text will be represented by one number.

Steps of Text Pre-processing (used in  Topic modeling)

1. Importing necessary libraries

import nltk

nltk.download('stopwords')

import string

from nltk.corpus import stopwords

2. Writing functions which removes punctuation and stopwords from our data

def text_process(text):
    nopunc =[char for char in text if char not in string.punctuation]
    nopunc=''.join(nopunc)
    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])

3. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens. Tokens are also known as lemmas

message['tokenized_message'] = message['message'].apply(text_process)

4. Vectorization in three steps

4.1. Count how many times does a word occur in each message (Known as term frequency,TF)

from sklearn.feature_extraction.text import CountVectorizer

del vectorizer
(del keyword in python is primarily used to delete objects in Python)

vectorizer = CountVectorizer(max_df = 0.9,min_df = 10)
vectorizer_matrix = vectorizer.fit_transform(message['tokenized_message'])

4.2. Weigh the counts, so that frequent tokens get lower weight

4.3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)",text preprocess,"['text', 'preprocess']",Text Pre-processing
641,"One hot encoding

One hot encoding is word representation without any context. It only tells about its presence in the vocabulary. It has nothing to do with the word meaning

Thus, by one hot encoding we are representing a word as point (or a line joining the origin and that point-means vector) in the vector space of vocabulary

One hot encoding is a process by which categorical variables (tokenized message) are converted into a form (matrix with zero/one value) that could be provided to ML algorithms to do a better job in prediction.

In one hot encoding there will be huge number of features (unigram words). So, it has a curse of dimensionality. It also can not count term frequency.

> Solution is CountVectorizer or TF-IDF vectorizer (better in most of the cases)",one hot encod,"['one', 'hot', 'encod']",One hot encoding
642,"Python coding for Naive Bayes

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import classification_report,confusion_matrix

X=vectorizer_matrix.todense()

y=my_df['label']

> Then split for X_train, X_test, y_train, y_test

classification_model = GaussianNB().fit(X_train,y_train)

Evaluating performance

test_preds = spam_detect_model.predict(X_test)

test_preds[:2]

y_test[:2]

print(confusion_matrix(y_test,test_preds))

print(classification_report(y_test,test_preds))

vectorizer_matrix or tfidf_matrix are sparse matrix. But Naive Bayes model needs dense matrix",python code naiv bay,"['python', 'code', 'naiv', 'bay']",Python coding for Naive Bayes
643,"Generalization Error

Generalization Error is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.",general error,"['general', 'error']",Generalization Error
644,"Basics of SVM

Support vector is the last option for linear regression/ classification problems on small dataset (<1lakh experiences) when regularized linear regressions is not working well

SVMs can also efficiently perform as non-linear classification model using the kernel trick. 

> SVM is also used for Outlier Detection and image classification Purposes on small dataset",basic support vector machin,"['basic', 'support', 'vector', 'machin']",Basics of SVM
645,"Kernel function 

Apart from SVM, other kernelizable algorithms are linear regression, PCA, K-means, neural network etc.

The meaning of Kernel is the edible part of a nut, seed, or fruit.

> Thus kernel function of a model softens the decision boundary to fit for non-linear classification

> Kernel function map low dimensional data to high dimensional space

> Kernel is a similarity function",kernel function,"['kernel', 'function']",Kernel function 
646,"Hinge loss function, hypersurface and hyperplane

The intuition of SVM is rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest point.

> Margin lines are the decision boundaries.

> SVM find a hyperplane to linearly separate the data by using the hinge loss function

> A decision boundary is a hypersurface that partitions the underlying vector space into two sets, one for each class. A general hypersurface in a small dimensional space is turned into a hyperplane in a space with much larger dimensions.",hing loss function hypersurfac hyperplan,"['hing', 'loss', 'function', 'hypersurfac', 'hyperplan']","Hinge loss function, hypersurface and hyperplane"
647,"Python coding for SVM

Without Karnel (for linear or binary classification)

from sklearn.svm import SVC 

my_model = SVC(gamma=0.23, C=3.20)

my_model.fit(X_train, y_train)

To generate blobs data points with a Gaussian distribution,

from sklearn.datasets.samples_generator import make_blobs

X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)

To generate circular data points,

from sklearn.datasets.samples_generator import make_circles

X, y = make_circles(100, factor=.1, noise=.1)

Kernel SVM

from sklearn.svm import SVC 

my_kernel_model = SVC(kernel='rbf', gamma=0.23, C=3.20)

my_kernel_model.fit(X, y)

> Different kernel functions are linear, nonlinear, polynomial (poly), radial basis function (rbf-it uses Gaussian transformation), and sigmoid.

> For high gamma value the model would consider only the points close to the hyperplane for modeling",python code support vector machin,"['python', 'code', 'support', 'vector', 'machin']",Python coding for SVM
648,"Tuning the SVM

The hardness of the margin is controlled by a tuning parameter, most often known as  C . For very large  C , the margin is hard, and points cannot lie in it. For smaller  C , the margin is softer, and can grow to encompass some points.

my_model = SVC(kernel='linear', C=0.1).fit(X, y)

The optimal value of the  C  parameter will depend on our dataset, and should be tuned using cv.",tune support vector machin,"['tune', 'support', 'vector', 'machin']",Tuning the SVM
649,"Advantages of SVM

Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.

Reason of high speed of SVMs 

1. Quadratic optimization (convex!)

2. They work in the dual, with relatively few points

3. The kernel trick",advantag support vector machin,"['advantag', 'support', 'vector', 'machin']",Advantages of SVM
650,"Disadvantages of SVM

The results are strongly dependent on a suitable choice for the softening parameter C. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.

If we have the CPU cycles to commit  training and cross-validating an SVM on our data, the method can lead to excellent results.

> We should always be careful about iteration for large dataset, otherwise our model will be computationally expensive

SVM will be effective when:

1. The data is linearly separable

2. The data is clean and ready to use

3. Proper Kernel is selected

4. Proper Kernel Parameters are selected",disadvantag support vector machin,"['disadvantag', 'support', 'vector', 'machin']",Disadvantages of SVM
651,"Basics of Artificial Neural Network

Artificial Neural Network (ANN) with SGD is the only option for linear Regression/ Classification problems on large dataset (> 1 lakh experiences)

ANN's are representative of the human brain. Human nervous system consists of billions of neurons. These neurons collectively process input received from sensory organs, process the information, and decides what to do in reaction to the input.

For simplicity, ANN is called Neural network

Neural network with SGD and kernel approximations is the only option for non-linear Classification problems on large dataset (>1 lakh experiences)",basic artifici neural network,"['basic', 'artifici', 'neural', 'network']",Basics of Artificial Neural Network
652,"Perceptron, neuron, node, unit

A neural network of single layer with single neuron which gives a single output from multiple inputs (i.e. multiple input features) with the help of activation function is called a perceptron.

Thus,
perceptron = neuron=node=unit 

Neural network can have multiple hidden layers and every hidden layer can have multiple neurons

The Perceptron is a linear classification algorithm. This means that it learns a decision boundary that separates two classes using a line (called a hyperplane) in the feature space.",perceptron neuron node unit,"['perceptron', 'neuron', 'node', 'unit']","Perceptron, neuron, node, unit"
653,"Model coefficients

The coefficients of the model are referred to as input weights and are trained using the stochastic gradient descent (SGD) optimization algorithm.",model coeffici,"['model', 'coeffici']",Model coefficients
654,"The components of neural network: 

1. Input layer (It includes all the feature values of the data for forward propagation)-This is nothing but inputs not a layer(defined as input_dim in the first hidden layer), 

2. Hidden layers (can have none, one or multiple) (defined with my_model.add method), 

3. Output layer (have 1 node for binary output) (defined with my_model.add method),

4. Neuron with Activation function (defined with Dense function).",compon neural network,"['compon', 'neural', 'network']",The components of neural network: 
655,"Backward propagation

We must adjust the weights to make the network fit the training data. The process of making these adjustments is known as backward propagation.",backward propag,"['backward', 'propag']",Backward propagation
656,"Understanding epochs

An epochs is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. (defined during model training)

",understand epoch,"['understand', 'epoch']",Understanding epochs
657,"Understanding of batch size

If the batch_size is the whole training dataset then the number of epochs is the number of iterations (batch_size is also defined during model training)",understand batch size,"['understand', 'batch', 'size']",Understanding of batch size
658,"Optimal accuracy in Neural Network

When we find the optimal number of neurons in multiple layers, we get optimal accuracy.

As a first attempt, we can try any one of the following:

> The number of hidden neurons should be between the size of the input layer (no. of features) and the size of the output layer (1 for binary classification).

> The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.

> The number of hidden neurons should be less than twice the size of the input layer. ",optim accuraci neural network,"['optim', 'accuraci', 'neural', 'network']",Optimal accuracy in Neural Network
659,"Applications of Neural Networks:

a. Classification of data

b. Anomaly detection

c. Speech recognition

d. Audio generation

e. Time series analysis

f. Spell checking

g. Character recognition

h. Machine translation (Converts one human language to another)

i. Image processing

> Neural networks have higher computational rates than conventional computers because a lot of the operation is done in parallel",applic neural network,"['applic', 'neural', 'network']",Applications of Neural Networks:
660,"Steps involved in a Neural Network

1. Take inputs

2. Add bias (if required)

3. Assign random weights to input features

4. Run the code for training (applying the learning function) in forward propagation.

5. Find the error in prediction.

6. Update the weight by SGD in back propagation.

7. Repeat the training phase with updated weights.

8. Make predictions.",step involv neural network,"['step', 'involv', 'neural', 'network']",Steps involved in a Neural Network
661,"Libraries for ANN model

from keras.models import Sequential

from keras.layers import Dense

from tensorflow.keras.optimizers import Adam

from keras.layers import Dropout

from keras import regularizers
",librari artifici neural network model,"['librari', 'artifici', 'neural', 'network', 'model']",Libraries for ANN model
662,"Define, Create and Compile ANN model

# define a function to build the keras model

def create_model(a,b):
    
# create model

model = Sequential()

  model.add(Dense(a, input_dim=13, activation='sigmoid'))
   
model.add(Dense(b, activation='sigmoid'))
    
model.add(Dense(1, activation='sigmoid'))
   
# compile model

adam = Adam(lr=0.001)

model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model

my_model = create_model(11,10)

print(my_model.summary())

> 'a' is the no. of neurons in the first hidden layer and 'b' is the no. of neurons in the second hidden layer ",defin creat compil artifici neural network model,"['defin', 'creat', 'compil', 'artifici', 'neural', 'network', 'model']","Define, Create and Compile ANN model"
663,"# fit the ANN model on the dataset

my_model.fit(X_train, y_train, epochs=150, batch_size=10)",fit artifici neural network model dataset,"['fit', 'artifici', 'neural', 'network', 'model', 'dataset']",# fit the ANN model on the dataset
664,"# evaluate the ANN model

result = my_model.evaluate(X_test,y_test,verbose=1)

print('Test loss: ', result[0])

print('Test accuracy: ', result[1])

train_pred=model.predict(X_train)>0.5

test_pred=model.predict(X_test)>0.5

print(confusion_matrix(y_train,train_pred))

print(confusion_matrix(y_test,test_pred))",evalu artifici neural network model,"['evalu', 'artifici', 'neural', 'network', 'model']",# evaluate the ANN model
665,"Plotting roc_curve

fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_pred)
auc_train = auc(fpr_train, tpr_train)

plt.figure

plt.plot([0, 1], [0, 1], 'k--')

plt.plot(fpr_train, tpr_train, label='Keras (area = {:.3f})'.format(auc_train))

plt.xlabel('False positive rate')

plt.ylabel('True positive rate')

plt.title('ROC curve')

plt.legend(loc='best')

plt.show()

Plotting roc_curve (easy way)

from sklearn import metrics

metrics.plot_roc_curve(my_model, X_train, y_train)  

Plotting precision_recall_curve (easy way)

metrics.plot_precision_recall_curve(my_model, X_train, y_train)",plot roccurv,"['plot', 'roccurv']",Plotting roc_curve
666,"Understanding Feature engineering

Feature engineering is a process of transforming the given data into a form which is easier to interpret.

We define feature engineering as creating new features from our existing ones to improve model performance.",understand featur engin,"['understand', 'featur', 'engin']",Understanding Feature engineering
667,"Importance of Feature Engineering

The intention of feature engineering is to achieve two primary goals:

1. Preparing an input dataset that is compatible with and best fits the machine learning algorithm.

2. Improving the performance of machine learning models",import featur engin,"['import', 'featur', 'engin']",Importance of Feature Engineering
668,"Basic EDA

1. Univariate Analysis

Ex:- CDF, PDF, Box plot, Violin plot.

2.  Bivariate analysis

Ex:- Scatter Plot, Box plot, Violin plot, Joint plot.

3. Multivariate Analysis

Ex:- Pair Plot, 3D Scatter Plot.",basic exploratori data analysi,"['basic', 'exploratori', 'data', 'analysi']",Basic EDA
669,"Understanding Outliers

Outliers are actually exceptional or abnormal experiences

> Finding out the true outlier experiences is very important

> In univariate or bivariate analysis of box plot, an observation may become outlier. There may be some other feature which is driving this excessive high or low value

> The true outlier experiences are those for which there is no feature in our dataset to catch the pattern

> When the feature is skewed, it can not show the true outlier. It has to be normal distribution to show the true outliers",understand outlier,"['understand', 'outlier']",Understanding Outliers
670,"Understanding Clustering Algorithm

KMeans is the only option for unsupervised machine learning with small data (< 10K experiences) with known no. of categories or clusters (need to define n_clusters=k)

MiniBatch Kmeans is the only option for unsupervised machine learning with large data (> 10K experiences) with known no. of categories (need to define n_clusters=k)

Variational Bayesian Gaussian mixture model (VB-GMM) is the best option for unsupervised machine learning with small data (<10K experiences) with unknown no. of categories)",understand cluster algorithm,"['understand', 'cluster', 'algorithm']",Understanding Clustering Algorithm
671,"Python coding for Kmeans

from sklearn.cluster import KMeans

my_model = KMeans(n_clusters=4)

my_model.fit(X)

y_preds = my_model.predict(X)

my_model.cluster_centers_

from sklearn.cluster import MiniBatchKMeans

my_model = MiniBatchKMeans(n_clusters=2, random_state=0, batch_size=6, max_iter=10)

my_model.fit_predict(X_final)

GMM

from sklearn.mixture import GaussianMixture

gaussian_model = GaussianMixture(n_components=2)",python code kmean,"['python', 'code', 'kmean']",Python coding for Kmeans
672,"Few issues of Kmeans

1. Although the expectation– maximization algorithm is guaranteed to improve the result in each step, there is no assurance that it will lead to the global best solution. 

2. Another common challenge with k-means is that we must tell it how many clusters we expect: it cannot learn the number of clusters from the data.

Ideally, we would not know how many clusters should we have, in the beginning of the algorithm

3. The algorithm will often be ineffective if the clusters have complicated geometries.

It always makes spherical clusters.",issu kmean,"['issu', 'kmean']",Few issues of Kmeans
673,"Expectation-Maximization approach 

For supervised learning model, we use error minimization algorithm like OLS, gradient descent as we have the actual value. 

For unsupervised learning model, we use expectation– maximization algorithm

In this algorithm, we assign the data points (data point means one observation in the feature space or n dimensional space) to the nearest cluster center in each Expectation step",expectationmaxim approach,"['expectationmaxim', 'approach']",Expectation-Maximization approach 
674,"Silhouette score for finding best no. of clusters 

1. Silhouette score tells how well samples are clustered with other samples that are similar to each other. 

from sklearn.metrics import silhouette_samples, silhouette_score

list_of_silhouette_score=[]

for k in range(2,15):

  my_kmeans_model = KMeans(n_clusters=k)

  my_kmeans_model.fit(X)

  y_preds = my_kmeans_model.predict(X)

  list_of_silhouette_score.append(silhouette_score(X, y_preds,  metric='euclidean'))

plt.plot(range(2,15), list_of_silhouette_score,'bo--',linewidth=2, markersize=8)

> Take the no. of cluster for the highest silhouette_score

> Same graph we can create by KElbowVisualizer with metric='silhouette'

from yellowbrick.cluster import SilhouetteVisualizer

my_model=KMeans(n_clusters=3)

visualizer = SilhouetteVisualizer(my_model)

visualizer.fit(X)

visualizer.poof() # Draw/show/poof the data",silhouett score find best cluster,"['silhouett', 'score', 'find', 'best', 'cluster']",Silhouette score for finding best no. of clusters 
675,"Elbow method for finding best no. of clusters 

2. The elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.

sum_of_sq_dist = {}

for k in range(1,15):

  km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)
      
  km = km.fit(X)

  sum_of_sq_dist[k] = km.inertia_
    
sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))

plt.xlabel('Number of Clusters(k)')
plt.ylabel('Sum of Square Distances')
plt.title('Elbow Method For Optimal k')

plt.show()

>> We can also do elbow visualization in yellowbricks",elbow method find best cluster,"['elbow', 'method', 'find', 'best', 'cluster']",Elbow method for finding best no. of clusters 
676,"Plotting Clusters

y_preds = my_model.predict(X)

centers = my_model.cluster_centers_

plt.scatter(X[:, 6], X[:, 7], c=y_preds, s=50, cmap='viridis')

plt.scatter(centers[:, 6], centers[:, 7], c='blue', s=200, alpha=0.5)

> cluster centers have same dimension as the data

> Cluster ploting is not useful for visualizing multidimensional clusters",plot cluster,"['plot', 'cluster']",Plotting Clusters
677,"Features of KMeans model

> Kmeans is an example of partition clustering method

> Kmeans is most sensitive to outliers 

> Kmeans is sensitive to missing values (Imputation with Expectation Maximization algorithm is the valid iterative strategy for treating missing values) 

> Kmeans is sensitive to multicollinearity of features 

> K-means is extremely sensitive to cluster center initializations (cluster seeds)

Bad initialization can lead to Poor convergence speed and bad overall clustering",featur kmean model,"['featur', 'kmean', 'model']",Features of KMeans model
678,"Density-based clustering

Density-based clustering is a non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together. Example: Density-based spatial clustering of applications with noise (DBSCAN) 

from sklearn.cluster import DBSCAN

dbscan_model = DBSCAN(eps=3, min_samples=2).fit(X)

dbscan_model.labels_

dbscan_model.predict(training_data)",densitybas cluster,"['densitybas', 'cluster']",Density-based clustering
679,"recency, frequency and monetary

>> RFM (recency, frequency and monetary) customer segmentation is very commonly used in marketing. It is a heuristic technique not a ml model",recenc frequenc monetari,"['recenc', 'frequenc', 'monetari']","recency, frequency and monetary"
680,"Basics of Hierarchical clustering

This is also applicable on small dataset (< 10K experiences) like Kmeans

Hierarchical clustering takes away the problem of pre-defining the number of clusters in Kmeans.

The optimal number of clusters can be obtained by the model itself, practical visualization with the dendrogram

Hierarchical clustering algorithms suffers from the problem of convergence at local optima",basic hierarch cluster,"['basic', 'hierarch', 'cluster']",Basics of Hierarchical clustering
681,"Types of hierarchical clustering:

1. Agglomerative hierarchical clustering (Bottom Up Approach)

We assign each data point (sample, observation or experience) to an individual cluster at the begining. 'n' clusters for 'n' observations

Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left.

It is also known as additive hierarchical clustering.

2. Divisive Hierarchical clustering (Top Down Approach)

Also called DIANA (DIvisive ANAlysis) is the inverse of agglomerative clustering

We start with a single cluster and assign all the data points to that cluster.

Now, at each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point.",type hierarch cluster,"['type', 'hierarch', 'cluster']",Types of hierarchical clustering:
682,"Proximity matrix

This is a mXm (m is the number of observations) square matrix which stores the euclidean distances between all the data points of the experience set.

In each iteration, proximity matrix is updated by considering a cluster as single observation.",proxim matrix,"['proxim', 'matrix']",Proximity matrix
683,"Understanding Linkage

Linkage is the method of determining the euclidean distance between two clusters.

1. Single linkage: 

Here two closest data points between the clusters is considered.

This approach can separate non-spherical clusters as long as the gap between two clusters is not small.

This approach cannot separate clusters properly if there is noise between clusters.

2. Complete linkage: 

Here two furthest data points between the clusters is considered.

This approach does well in separating clusters if there is noise between clusters.

3. Average linkage: 

Average of the distances among all the data points between the clusters is considered.

This approach also does well in separating clusters if there is noise between clusters

4. Ward’s Method:

Similar like average linkage except that Ward’s method calculates the sum of the square of the distances.

Ward’s method approach does best in separating clusters if there is noise between clusters.",understand linkag,"['understand', 'linkag']",Understanding Linkage
684,"Finding out no. of clusters from visualization

import scipy.cluster.hierarchy as sch

dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))

Find largest vertical distance we can, without crossing any other horizontal line",find cluster visual,"['find', 'cluster', 'visual']",Finding out no. of clusters from visualization
685,"Python coding for Hierarchical clustering

from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')

y_hc = hc.fit_predict(X)",python code hierarch cluster,"['python', 'code', 'hierarch', 'cluster']",Python coding for Hierarchical clustering
686,"Basics of PCA

PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization (in 2D or 3D plot), for noise filtering (does not eleminate the noise, but reduces), for feature extraction and engineering, data compression and much more.

Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction (to get rid of curse of dimensionality) in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset.

> PCA is mostly used for unsupervised learning

> Assumption: There is linear relationship between all variables.

Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.

> PCA's main weakness is that it tends to be highly affected by outliers in the data.

> If we use features of different scales, we get misleading directions.

Thus outlier treatment and feature scaling must be performed prior to PCA",basic princip compon analysi,"['basic', 'princip', 'compon', 'analysi']",Basics of PCA
687,"Drawback of PCA

1. Some information is lost, 

2. It can be computationally intensive.

3. Transformed features are often hard to interpret.",drawback princip compon analysi,"['drawback', 'princip', 'compon', 'analysi']",Drawback of PCA
688,"Python Coding of PCA

from sklearn.decomposition import PCA

my_model = PCA(n_components=3)

my_model.fit(X)

X_new = my_model.transform(X)

transform() function reduces dimensions according to the learnings from fit() function

> Now, X_new can be fitted in any ml model

PCA inside Pipeline function

from sklearn.pipeline import Pipeline

my_model = Pipeline([('norm', MinMaxScaler()), ('pca', PCA()), ('m', LogisticRegression())])

my_model.fit(X_train,y_train) or,

my_model.fit_transform(X_train)

y_pred = my_model.predict(X_test)",python code princip compon analysi,"['python', 'code', 'princip', 'compon', 'analysi']",Python Coding of PCA
689,"Understanding the important features in PCA

# get the number of components

n_pcs= my_model.components_.shape[0]

# get the index of the most important feature on each component

most_important = [np.abs(my_model.components_[i]).argmax() for i in range(n_pcs)]

feature_names = X.columns.tolist()

# get the names

most_important_names = [feature_names[most_important[i]] for i in range(n_pcs)]

list_of_list = [['PC{}'.format(i+1),most_important_names[i],'{}'.format(my_model.explained_variance_[i])] for i in range(n_pcs)]

# build the dataframe

df = pd.DataFrame(list_of_list,columns=['Principle-Components','Feature-Name','Explained-Variance'])",understand import featur princip compon analysi,"['understand', 'import', 'featur', 'princip', 'compon', 'analysi']",Understanding the important features in PCA
690,"Deeper Understanding of PCA

PCA does not change the distribution of datapoints. It only shifts the origin approximately to the center of gravity of the whole data points and selects the spinal cord of the datapoints as the first PC dimension and recalculate the coordinate of each datapoint. When we know the coordinate of any data point, that means we know the projection of the datapoint in all PC dimension. 

PC dimension-1 is the most significant extracted feature.

This is a techniques from the field of linear algebra. This is often called “feature projection” and the algorithms used are referred to as “projection methods.”

PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

This transformation from data axes to principal axes is an affine transformation, which basically means it is composed of three basic transformation: translation,  rotation and uniform scaling

An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation).

> Translation is a geometric transformation that moves every point of a figure, shape or space by the same distance in a given direction. A translation can also be interpreted as the addition of a constant vector to every point, or as shifting the origin of the coordinate system.

> If we don’t rotate (orthogonally) the components, the effect of PCA will diminish and we’ll have to select more number of components to explain variance in the training set.",deeper understand princip compon analysi,"['deeper', 'understand', 'princip', 'compon', 'analysi']",Deeper Understanding of PCA
691,"Math behind PCA

Step 1 : Take the whole dataset with dimension  d

Step 2 : Compute the mean of every dimension of the whole dataset.

Step 3 : Compute the covariance matrix (between the features) of the whole dataset

Step 4 : Compute Eigenvectors and corresponding Eigenvalues for the covariance matrix

Step 5 : Sort the eigenvectors by decreasing eigenvalues and choose  k  eigenvectors with the largest eigenvalues to form a  d×k  dimensional matrix  W .

Step 6 : Transform the samples onto the new subspace

> covariance matrix is a square matrix giving the covariance between each pair of elements of a given random vector

> Each Eigenvalue corresponds to a principal component. The highest eigenvalue corresponds to the 1st Principal Component and so on. The length of a principal component is its eigenvalue.

> Thus, if the eigenvalues are roughly equal PCA will perform badly",math behind princip compon analysi,"['math', 'behind', 'princip', 'compon', 'analysi']",Math behind PCA
692,"TruncatedSVD 

PCA is TruncatedSVD on centered data (by per-feature mean substraction). If the data is already centered, those two classes will do the same.

In practice TruncatedSVD is useful on large sparse datasets which cannot be centered without making the memory usage explode.

> Truncated meaning shortened in duration or extent",truncatedsvd,['truncatedsvd'],TruncatedSVD 
693,"PCA Visualization

A scree plot is a line plot of the principal components vs. eigenvalues. The scree plot is used to determine the number of factors to retain in an exploratory factor analysis or principal components to keep in a principal component analysis",princip compon analysi visual,"['princip', 'compon', 'analysi', 'visual']",PCA Visualization
694,"Trace of matrix

The Trace of a Matrix is defined only for a Square Matrix. It is sum of its diagonal elements from the upper left to lower right

Trace of a covariance matrix of shape (n X n) = n, because all the diagonal elements are one

>  The covariance between X1 and X2 indicates how the values of X1 and X2 move relative to each other.
Cov(X1,X1)=1",trace matrix,"['trace', 'matrix']",Trace of matrix
695,"Basics of Anomaly detection 

Anomaly detection is the process of identifying unexpected items or events in data sets, which differ from the norm. 

In contrast to standard classification tasks, anomaly detection (unsupervised anomaly detection) is often applied on unlabeled data, taking only the internal structure of the dataset into account.

Neural network (RNN, Deep Autoencoders) anomaly detection technique is used for large dataset (> 1lakh experiences)

> Data scaling must be done before performing anomaly detection",basic anomali detect,"['basic', 'anomali', 'detect']",Basics of Anomaly detection 
696,"Assumptions in Anomaly detection 

Anomaly detection has two basic assumptions:

1. Anomalies are the instances which occur very rarely.

2. Their features differ from the normal instances significantly.",assumpt anomali detect,"['assumpt', 'anomali', 'detect']",Assumptions in Anomaly detection 
697,"Finding global outliers

Isolation Forest is similar in principle to Random Forest and is built on the basis of decision trees. Isolation Forest, however, identifies anomalies or outliers (for small dataset) rather than profiling normal data points. Here, majority class is 1 and minority class is 0

> It Identifies anomalies as the observations with short average path lengths

> It Splits the data points by randomly selecting a value between the maximum and the minimum of the selected features.

Random Forest is a supervised learning  algorithm and Isolation Forest is an unsupervised learning algorithm

A global outlier is a measured sample point that has a very high or a very low value relative to all the values in a dataset.",find global outlier,"['find', 'global', 'outlier']",Finding global outliers
698,"1. Univariate Anomaly Detection

Here, anomaly detection is carried out on single column of pandas df, like sales column, profit column etc

Visualization of anomalies or outliers through scatter plot

plt.scatter(range(df.shape[0]), np.sort(df['column_name'].values))",1 univari anomali detect,"['1', 'univari', 'anomali', 'detect']",1. Univariate Anomaly Detection
699,"Python coding for Isolation forest

from sklearn.ensemble import IsolationForest

X=df1['Sales'].values.reshape(-1, 1) # for reshaping in 2D array

clf = IsolationForest(n_estimators=100, contamination=0.01, random_state=0)

clf.fit(X)

df['multivariate_anomaly_score'] = clf.decision_function(X)

df['multivariate_outlier'] = clf.predict(X)

> other hyperparameters of IsolationForest are max_samples and max_features",python code isol forest,"['python', 'code', 'isol', 'forest']",Python coding for Isolation forest
700,"2. Multivariate Anomaly Detection

Most of the analysis that we end up doing are multivariate due to complexity of the world we are living in. 

In multivariate anomaly detection, outlier is a combined unusual score on at least two variables.

X=df[list_of_two_columns]
Other codes are same as univariate

df[(df['outlier_univariate_sales'] == 1) & (df['outlier_univariate_profit'] == 1) & (df['multivariate_outlier'] == -1)] # gives the df of anomalies",2 multivari anomali detect,"['2', 'multivari', 'anomali', 'detect']",2. Multivariate Anomaly Detection
701,"Finding local outliers-LOF

from sklearn.neighbors import LocalOutlierFactor

my_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)

other codes are same as isolation forest

LOF performs well when the density of the data point isn't constant throughout the dataset.

It gives better results than the global approach to seek out outliers. But, as there's no threshold value of LOF, the choice of a data point as an outlier is user-dependent. 

> Therefore we can run first isolation forest and then LOF to check both

A local outlier is a measured sample point that has a value within the normal range for the entire dataset. It gives Local high or low values",find local outlierslof,"['find', 'local', 'outlierslof']",Finding local outliers-LOF
702,"Visual representation of univariate anomalies

xx = np.linspace(df['Sales'].min(), df['Sales'].max(), len(df)).reshape(-1,1)

anomaly_score = clf.decision_function(xx)

outlier = clf.predict(xx)

plt.figure(figsize=(7,7))
plt.plot(xx, anomaly_score, label='anomaly score')

plt.fill_between(xx.T[0],np.min(anomaly_score),p.max(anomaly_score),where=outlier==-1,color='r',alpha=.4, label='outlier region')",visual represent univari anomali,"['visual', 'represent', 'univari', 'anomali']",Visual representation of univariate anomalies
703,"Deeper Understanding of Anomalies

> Outlier are observations that are distant from the mean location of a distribution

> Outliers don’t necessarily represent abnormal behavior

> Anomalies are data patterns that are generated by different processes.

The most common reasons for the outliers are:

> Data errors

> Noisy data points

> Hidden patterns in the datasets

Identifying outliers in our dataset is probably one of the most difficult part of data cleanup, and it takes time to get right. Even if we have a deep understanding of statistics and how outliers might affect our data, it’s always a topic to explore cautiously.",deeper understand anomali,"['deeper', 'understand', 'anomali']",Deeper Understanding of Anomalies
704,"Natural Language Understanding (NLU) in five dimensions

Natural language can be oral (speech) or written (text) form

Oral Communication is an informal communication and written Communication is formal communication

The five dimensions of NLU are:

1. Lexical: Understanding meaning from vocabulary (understanding the meaning of word by analyzing the structure or parts of words such as stems, root words, prefixes, and suffixes)

> Phoneme-smallest unit of sound (e.g. Ch, Ph etc)

> Morpheme-smallest meaningful unit (e.g. unsual)

> Lexical is a brach of morphology.

2. Syntactic: Understanding meaning from syntax (understanding the meaning of word/ phrase according to syntax or grammer) 

3. Semantic: Understanding meaning from context (understanding the meaning of word/ phrase in relation to context)-used in language model

4. Pragmatic: Understanding meaning from general use (understanding the meaning of word/ phrase according to what people mean by the language they use)

5. NLU -informal has a fifth dimension: Understanding the feel of sound",natur languag understand nlu five dimens,"['natur', 'languag', 'understand', 'nlu', 'five', 'dimens']",Natural Language Understanding (NLU) in five dimensions
705,"Important Applications of NLP

1. Question & Answer (Chatbot, Voicebot etc.)

2. Summarizing a paragraph

3. Understanding genuine paragraph

4. Sentiment analysis etc.

> Modern NLP algorithms are based on machine learning, especially statistical machine learning.

> Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.

> For language model, conditional probability is rewritten as joint probability

P(A, B) = P(A)* P(B | A)",import applic natur languag process,"['import', 'applic', 'natur', 'languag', 'process']",Important Applications of NLP
706,"Feature engineering in NLP

1. Follow 'Steps of Text Pre-processing' as listed in Naive Bayes Classifier

2. During the steps of text pre-processing, along with removing punctuation and stop words, we can perform stemming/ lemmatization (generates the root form of the inflected words) for reducing no. of features/tokens

3. We can also reduce no. of tokens by taking all synonyms as single feature 

4. Removing high frequency words after calculating tf-idf weights",featur engin natur languag process,"['featur', 'engin', 'natur', 'languag', 'process']",Feature engineering in NLP
707,"n-gram in NLP

One word tokens called uni-gram, two words tokens called Bi-grams.

In the fields of computational linguistics and probability, an n-gram is a contiguous (touching) sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.",ngram natur languag process,"['ngram', 'natur', 'languag', 'process']",n-gram in NLP
708,"Understanding TF-IDF

> TF-IDF is better technique than CountVectorizer, because in TF-IDF all the document perspective is also considered

tfid_vectorizer = TfidfVectorizer(max_df = 0.9,min_df = 10)

tfidf_matrix = tfid_vectorizer.fit_transform(my_df['tokenized_message'])

dictionary = tfid_vectorizer.vocabulary_.items()

tf-idf weight is the product of two terms: 

The first term is the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; 

The second term is the Inverse Document Frequency (IDF), computed as the logarithm (ln) of the number of the documents in the corpus divided by the number of documents where the specific term appears.

> Reason of taking the Term Frequency but inverse of Document Frequency

This is because when a term is appearing in a document multiple times, we want to give more weightage, But when that term is appearing in multiple documents we want to give less weightage. ",understand tfidf,"['understand', 'tfidf']",Understanding TF-IDF
709,"RNN or LSTM in NLP

When the sequence of words matter, we use RNN or LSTM (Long Short Term Memory networks  – are a special kind of RNN). 

Now a day Transformers are used. Latest is GPT-3 (third generation Generative Pre-trained Transformer)

Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for NLP pre-training developed by Google.",recurr neural network long short term memori natur languag process,"['recurr', 'neural', 'network', 'long', 'short', 'term', 'memori', 'natur', 'languag', 'process']",RNN or LSTM in NLP
710,"Libraries for NLP

from sklearn import preprocessing

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.model_selection import train_test_split, KFold

from nltk.corpus import stopwords

from nltk.stem.snowball import SnowballStemmer

Famous open source libraries for NLP: Hugging Face, spaCy and nltk are used to run large scale NLP",librari natur languag process,"['librari', 'natur', 'languag', 'process']",Libraries for NLP
711,"Right order for a text classification 

1. Text cleaning

2. Text annotation

3. Text to predictors

4. Gradient descent

5. Model tuning
",right order text classif,"['right', 'order', 'text', 'classif']",Right order for a text classification 
712,"Measuring the complexity of a sentence

> Number of words in a sentence

> Average length of the words in the sentence",measur complex sentenc,"['measur', 'complex', 'sentenc']",Measuring the complexity of a sentence
713,"Basics of Topic modeling

Topic modeling refers to any technique that discovers the hidden (therefore called latent) semantic structure in a corpus which provides insights into the different themes present in the texts. 

Topic modelling refers to the task of identifying topics that best describes a set of documents. These topics will only emerge during the topic modelling process. And one popular topic modeling technique is known as Latent Dirichlet Allocation (LDA).

> Corpus means a collection of written texts, especially the entire works of a particular author

> Sentiment analysis is the process of identifying the emotions and opinions expressed in a particular text.

> LDA and PCA both does dimensionality reduction",basic topic model,"['basic', 'topic', 'model']",Basics of Topic modeling
714,"Basic assumptions of all topic models:

1. Each document is a collection of latent(hidden) topics

2. Each topic is a collection of words",basic assumpt topic model,"['basic', 'assumpt', 'topic', 'model']",Basic assumptions of all topic models:
715,"Libraries for Topic Modeling

from IPython.display import display

from tqdm import tqdm

from collections import Counter

import ast

import matplotlib.mlab as mlab

from sklearn.feature_extraction.text import CountVectorizer

from textblob import TextBlob

import scipy.stats as stats

from sklearn.decomposition import TruncatedSVD

from sklearn.decomposition import LatentDirichletAllocation

from sklearn.manifold import TSNE

output_notebook()",librari topic model,"['librari', 'topic', 'model']",Libraries for Topic Modeling
716,"Understanding LDA

LDA takes our document-term matrix as input and yield an  n×N  topic matrix as output, where  N  is the number of topic categories (which we supply as a hyperparameter).

LDA is a generative probabilistic process, designed with the specific goal of uncovering latent topic structure in text corpora.

As LDA is generative probabilistic process, we can think LDA as a machine generates document based on random probability distribution settings alpha and beta.

Then the machine finds the best fit of the generated document with the set of documents fitted into by adjusting the parameters alpha and beta.

Then it gives us the topic of the document

LDA is a computationally heavy process. So, we take random sample (say 1000 examples) from large text dataset  and then convert it into small_docment_term_matrix by applying countvectorizer/tf-idf vectorizer before passing the text dataset to LDA algorithm

In LDA, Dirichlet Distribution is used.

> Any set where the numbers sum up to 1 is called  Dirichlet Distribution. 

e.g. {0.6,0.4} is a dirichlet distribution or a probability simplex (a set where each point represents a probability distribution between a finite number of mutually exclusive events). Here 0.6 and 0.4 are the probability distributions

A document is a probability distribution of latent topics (alpha) and a topic is a probability distribution of words (beta)",understand latent dirichlet alloc,"['understand', 'latent', 'dirichlet', 'alloc']",Understanding LDA
717,"Plate Notation

Plate notation is a concise way of visually representing the dependencies among model parameters (alpha and beta).

Large rectangle,M denotes the entire set of documents. Small rectange, N denotes sub-set or sample set of examples. Theta and Phi are the multinomial distribution.",plate notat,"['plate', 'notat']",Plate Notation
718,"Python coding of LDA model 

lda = LatentDirichletAllocation()

grid_params={'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}

lda_model = GridSearchCV(lda,param_grid=grid_params)

lda_model.fit(small_document_term_matrix)

best_lda_model = lda_model.best_estimator_",python code latent dirichlet alloc model,"['python', 'code', 'latent', 'dirichlet', 'alloc', 'model']",Python coding of LDA model 
719,"Visualization of LDA

!pip install pyLDAvis

import pyLDAvis.sklearn

pyLDAvis.enable_notebook()

lda_panel = pyLDAvis.sklearn.prepare(best_lda_model, small_document_term_matrix,small_count_vectorizer,mds='tsne')
lda_panel",visual latent dirichlet alloc,"['visual', 'latent', 'dirichlet', 'alloc']",Visualization of LDA
720,"Cosine Similarity

Machine learning uses Cosine Similarity in applications such as data mining and information retrieval. This allows for a Cosine Similarity measurement to distinguish and compare documents to each other based upon their similarities and overlap of subject matter or context.

Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between the two vectors.

If the ange between the vectors is zero, then cosine similarity is 1, means the vectors (word, expression or sentence) are fully similar. Consine similarity 0 means no match

> By using cosine similarity,we can easily find how two words (word embedding vectors) are similar or opposite to each other.

from sklearn.metrics.pairwise import cosine_similarity

array_vec_1 = np.array([[12,41,60,11,21]])

array_vec_2 = np.array([[12,41,60,11,14]])

cosine_similarity(array_vec_1 , array_vec_2)

> Finding cosine similarity of whole matrix

cosine_similarity(document_term_matrix[-1:] , document_term_matrix[:-1])[0]

Execution time can reduced tremendously, say from 2000 ms to 50 ms using cosine similarity of whole matrix, in place of using for loop or list comprehension to find pairwise cosine similarity individually",cosin similar,"['cosin', 'similar']",Cosine Similarity
721,"Basics of Recommender System

The objective of a Recommender System is to recommend relevant items for users, based on their preference. Preference and relevance are subjective, and they are generally inferred by items users have consumed previously

Recommender systems have a problem known as user cold-start, in which it is hard to provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.",basic recommend system,"['basic', 'recommend', 'system']",Basics of Recommender System
722,"Popular Recommender Systems

1. Collaborative Filtering

2. Content-Based Filtering

3. Hybrid Approach",popular recommend system,"['popular', 'recommend', 'system']",Popular Recommender Systems
723,"Collaborative Filtering

This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from the group (collaborating)",collabor filter,"['collabor', 'filter']",Collaborative Filtering
724,"Content-Based Filtering

This method uses only information about the description and attributes of the items users has previously consumed to model user's preferences. 

In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present)

Here we are using a very popular technique in information retrieval (search engines) named TF-IDF.

We model an user profile through which we can find the top tokens and their relevance. 

To model the user profile, we take all the item profiles the user has interacted and average them. The average is weighted by the interaction strength, in other words, the items the user has interacted the most (eg. liked or commented) will have a higher strength in the final user profile.

Here, we compute the cosine similarity between the user profile and all item profiles

Thus we can recommend items for that user.",contentbas filter,"['contentbas', 'filter']",Content-Based Filtering
725,"Hybrid Approach

Recent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective than pure approaches in some cases. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.",hybrid approach,"['hybrid', 'approach']",Hybrid Approach
726,"Implementation strategies of Collaborative Filtering

Collaborative Filtering (CF) has two main implementation strategies:

1. Memory-based: This approach uses the memory of previous users interactions to compute users similarities based on items they've interacted (user-based or  User Neighbourhood-based approach) or compute items similarities based on the users that have interacted with them (item-based approach)

> Finds similar users based on cosine similariry or pearson correlation

2. Model-based: In this approach, models are developed using different machine learning algorithms to recommend items to users. There are many model-based CF algorithms, like Neural Networks, Bayesian Networks, Clustering Techniques, and Latent Factor Models such as Singular Value Decomposition (SVD) and Probabilistic Latent Semantic Analysis.

> Disadvantage of model-based approach: Inference is intracable because of hidden/latent factors",implement strategi collabor filter,"['implement', 'strategi', 'collabor', 'filter']",Implementation strategies of Collaborative Filtering
727,"Latent factor models

Latent factor models compress user-item matrix into a low-dimensional representation in terms of latent factors.

One advantage of using this approach is that instead of having a high dimensional matrix containing abundant number of missing values we will be dealing with a much smaller matrix in lower-dimensional space.",latent factor model,"['latent', 'factor', 'model']",Latent factor models
728,"Matrix Factorization

Like factorization of any number into prime numbers, matrix can also be factorized into set of matrices(U, sigma, Vt). 

This is called singular value dicomposition.

Singular matrix means a matrix with determinant zero

The higher the number of factors, the more precise is the factorization in the original matrix reconstructions. 

For model generalization (simplification), we need to decrease the no. of factors",matrix factor,"['matrix', 'factor']",Matrix Factorization
729,"Implementation of SVD

from sklearn.feature_extraction.text import TfidfVectorizer

from scipy.sparse.linalg import svds

Step-1: Data Munging (the procedure for transforming data from erroneous or unusable forms to usable form)

We define a dictionary for event strength and map all the user interactions.

For example, a comment in an article indicates a higher interest of the user on the item than a like, or than a simple view.

Step-2: Then, to model the user interest on a given article, we aggregate all the interactions the user has performed in an item by a weighted sum of interaction type strength and apply a log transformation to smoothen the distribution.

Step-3: Train-test splitting of the dataset

Step-4: Then, we apply pivot method on train df

users_items_pivot_matrix_df = interactions_train_df.pivot(index='personId', columns='contentId',  values='eventStrength')

Step-5: Pivot df to pivot matrix conversion

users_items_pivot_matrix = users_items_pivot_matrix_df.values

Step-6: Performs matrix factorization

U, sigma, Vt = svds(users_items_pivot_matrix, k =15)

where k is the number of matrix factorization

Step-7: After the factorization, we try to to reconstruct the original matrix by multiplying its factors. The resulting matrix is not sparse any more. It was generated predictions for items the user have not yet interaction, which we will exploit for recommendations.

all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 

Step-8: Writing the class for CFRecommender

class CFRecommender:
  
  def __init__(self, cf_predictions_df, items_df=None):
    .
    .
    return recommendations_df

Step-9: Evaluation

We choose to work with Top-N accuracy metrics, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.",implement singular valu decomposit,"['implement', 'singular', 'valu', 'decomposit']",Implementation of SVD
730,"Metrices used for evaluation of Recommender systems

The Top-N accuracy metric is Recall@N which evaluates whether the interacted item is among the top N items (hit) in the ranked list of recommendations for a user.

Other metrices for evaluation: Top-N Recall, Top-N Precision, MAP@K",metric use evalu recommend system,"['metric', 'use', 'evalu', 'recommend', 'system']",Metrices used for evaluation of Recommender systems
731,"Shortcoming of content-based recommender systems

Users will only get recommendations related to their preferences in their profile, and recommender engine may never recommend any item with other characteristics.",shortcom contentbas recommend system,"['shortcom', 'contentbas', 'recommend', 'system']",Shortcoming of content-based recommender systems
732,"Basics of Time Series

Time series is a series of data points indexed (or listed or graphed) in time order.

Time Series Analysis is to predict any future event

Time interval of time series data can be anything from millisecond level to year level or larger

This is a supervised or sudo supervised algorithm, because here the past data is considered as labels

> Graph of time series is called Historigram

> For checking randomness in time series autocorrelation plot is used",basic time seri,"['basic', 'time', 'seri']",Basics of Time Series
733,"Components of Time Series

Time series can be decomposed into four components (T, S, C, I)

1. Secular trend or Trend-'T' (the overall movement of a curve)

2. Seasonal variations or Seasonality-'S' (having period)-variation occurring within parts of a year

3. Cyclical fluctuations-'C' (which correspond to periodical but not seasonal variations)-e.g. Prosperity, Recession, and depression in a business

4. Irregular variations or Noise or Error-'I' (residual)",compon time seri,"['compon', 'time', 'seri']",Components of Time Series
734,"Multiplicative Model

The original time series is expressed as the product of trend, seasonal,cyclical and irregular components.

T*S*C*I",multipl model,"['multipl', 'model']",Multiplicative Model
735,"Ways to approach a Time Series Prediction Problem

1. Time Series Approach

Time Series Approaches are Naive Forecast, Moving Average (MA),  Weighted average, Exponential smoothing, Double exponential smoothing and Econometric approach

2. Machine Learning Approach",way approach time seri predict problem,"['way', 'approach', 'time', 'seri', 'predict', 'problem']",Ways to approach a Time Series Prediction Problem
736,"1. Naive Forecast

Estimating technique in which the last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors (unintended contributor to an incident).",1 naiv forecast,"['1', 'naiv', 'forecast']",1. Naive Forecast
737,"2. Moving Average (MA)

A simple moving average (SMA) is a prediction that takes the k historical points. 

This is one step advancement of Naive forecast.

MA may be used to smoothen a time series. (level of smoothening depends on the value of k)

> The moving average is free from the influences of seasonal and irregular variations

> Simple average method for finding out seasonal indices is good when the time-series has no trend and cyclic variation",2 move averag,"['2', 'move', 'averag']",2. Moving Average (MA)
738,"3. Weighted average 

It is a simple modification to the moving average. The weights sum up to 1 with larger weights assigned to more recent observations.

",3 weight averag,"['3', 'weight', 'averag']",3. Weighted average 
739,"4. Exponential smoothing

Instead of weighting the last  k  values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time.

Smoothening parameter alpha is used here instead of weight, w

Like Naive forecast, MA and weighted average, it also predicts one data point in the future.

When we try to predict multiple data points (say, forecasting for horizon, h = 8) in future, general tendency of our model is the accuracy of our prediction decreases with time. Lag 0 prediction accuracy will be very high and lag n prediction accuracy will be very low.",4 exponenti smooth,"['4', 'exponenti', 'smooth']",4. Exponential smoothing
740,"5. Double exponential smoothing

It is called Holt’s linear trend method

Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level or intercept and one for the trend)

Here, we use two smoothening paramaters, alpha and beta.",5 doubl exponenti smooth,"['5', 'doubl', 'exponenti', 'smooth']",5. Double exponential smoothing
741,"Assumption in Time Series

To apply any statistical technique (starting from Naive Forecast to Econometric approach) to forecast a time series, the main assumption is that the time seires is stationary.

Stationarity: If a process is stationary, that means it does not change its statistical properties over time, namely its mean, variance and auto-correlation (means correlating current value with previous value). 

Apart from visual inspection of time series to check stationarity, Dickey-Fuller test is there for stationarity check",assumpt time seri,"['assumpt', 'time', 'seri']",Assumption in Time Series
742,"6. Econometric approach

Realworld time seires are non-stationary in nature. 

So, we apply different transformation to make it stationary.",6 econometr approach,"['6', 'econometr', 'approach']",6. Econometric approach
743,"Understanding ARIMA

Autoregressive Integrated Moving Average model

ARIMA - Parameters

p: Trend autoregression order 
d: Trend difference order
q: Trend moving average order

> A statistical model is autoregressive if it predicts future values based on past values.",understand arima,"['understand', 'arima']",Understanding ARIMA
744,"Understanding SARIMA

Seasonal Autoregressive Integrated Moving Average model

It is used when there is seasonal component in the time series.

SARIMA - Parameters

P: Seasonal autoregressive order.

D: Seasonal difference order.

Q: Seasonal moving average order.",understand sarima,"['understand', 'sarima']",Understanding SARIMA
745,"Libraries for Time Series Analysis

from dateutil.relativedelta import relativedelta

from scipy.optimize import minimize

import statsmodels.formula.api as smf

import statsmodels.tsa.api as smt

import statsmodels.api as sm

import scipy.stats as scs

from itertools import product  
                 
from tqdm import tqdm_notebook",librari time seri analysi,"['librari', 'time', 'seri', 'analysi']",Libraries for Time Series Analysis
746,"Machine learning approach in Time series

To solve time series prediction problem by machine learning approach, we need to convert the two column ts dataset into a supervised learning dataset where there will be independent variables or features (X's).

We can use any supervised ml algorithm after creating the features in ts

'S' related features can be created from time stamp by extracting year, month etc.

Autoregression (AR) related features can be lag 1, lag 2, lag 3 and other lag values of y.

'I' related features by taking the differences between lag values

'MA' related features can be MA3, MA4 etc. 

External or exogenous factors, Target encoding, Forecasts from other models and many more ideas can be features",machin learn approach time seri,"['machin', 'learn', 'approach', 'time', 'seri']",Machine learning approach in Time series
747,"Downside of SARIMA

The biggest downside of SARIMA is we need to manually check if the data is stationary or not for applying SARIMA. But in reallife we need to attempt multiple time series, in the order of thousand to solve one ts problem. So, manual checking of stationarity is not feassible.

Thus, the machine learning approach came into picture.",downsid sarima,"['downsid', 'sarima']",Downside of SARIMA
748,"Understanding Leakage

In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time.

We must avoid leakage always by asking 'will I know the value of the feature during prediction?'",understand leakag,"['understand', 'leakag']",Understanding Leakage
749,"Prophet or Facebook Prophet

It is an open-source library for univariate (one variable) time series forecasting developed by Facebook.

> It converts a time series problem into machine learning problem

> In PROPHET, we can specify holidays and many more

https://facebook.github.io/prophet/docs/quick_start.html#python-api

from fbprophet import Prophet

model = Prophet()

> For seasonal smooth pattern generally we use prophet

> For non-seasonal smooth pattern we generally use ml algorithm",prophet facebook prophet,"['prophet', 'facebook', 'prophet']",Prophet or Facebook Prophet
750,"Cross Validation in Time Series

We can not randomly shuffle k-folds cross validation on ts, because here, y is highly dependent on sequential time.

So, here cross validation shall be done by k-folds sequentially. This is called temporal cross-validation.
",cross valid time seri,"['cross', 'valid', 'time', 'seri']",Cross Validation in Time Series
751,"Demand pattern classification in time series

1. Intermittent- The demand history shows very little variation in demand quantity but a high variation in the interval between two demands

2. Smooth-The demand is very regular in time and in quantity

3. Lumpy- The demand is characterized by a large variation in quantity and in time

4. Erratic- The demand has regular occurrences in time with high quantity variations",demand pattern classif time seri,"['demand', 'pattern', 'classif', 'time', 'seri']",Demand pattern classification in time series
752,"Product forecastability

To determine a product forecastability, we apply two coefficients:

1. Average Demand Interval (ADI). It measures the demand regularity in time by computing the average interval between two demands.

2. Square of the Coefficient of Variation (CV²). It measures the variation in quantities.

> Smooth demand (ADI < 1.32 and CV² < 0.49)

> Intermittent demand (ADI >= 1.32 and CV² < 0.49)

> Erratic demand (ADI < 1.32 and CV² >= 0.49)

> Lumpy demand (ADI >= 1.32 and CV² >= 0.49)",product forecast,"['product', 'forecast']",Product forecastability
753,"Basics of case interview

A case interview is a real client problem that we must solve in about 15-30 minutes during a job interview.

> We may think the scenario as expecting answer or decision from the trained data scientist",basic case interview,"['basic', 'case', 'interview']",Basics of case interview
754,"Categorization of case studies

1. Estimate the size of something

2. Fix something bad 

3. Take a strategic decision

4. Improve something 
",categor case studi,"['categor', 'case', 'studi']",Categorization of case studies
755,"Guesstimate (Problem Solving Approach)

1. Clarify the problem statement

> At first, we need to ask all the questions to make the problem statement clear

> Choose between supply side or demand side approach 
(if applicable)

> Confirm assumptions if any

2. Break down into sub-problems 

> Check for bottleneck if any (in case of demand side approach)

> Prepare a diagram considering the dependent variable and all independent variables

3. Solve sub-problems 

> Understand the filters if any (Geography, gender, life expectancy, income level etc.)

> guesstimate the values of all independent variables

> Take logical simple numbers for easy calculation

> Avoid zero error using exponents

4. Consolidate our findings 
(Create a learning function)",guesstim problem solv approach,"['guesstim', 'problem', 'solv', 'approach']",Guesstimate (Problem Solving Approach)
756,"Meaning of Diagnosis  

Diagnosis is the art or act of identifying a disease from its signs and symptoms. It needs a computer to analyse various relation and metrics together.",mean diagnosi,"['mean', 'diagnosi']",Meaning of Diagnosis  
757,"ML in Healthcare

The implementation of EHR i.e. Electronic health records started around 2010

Adoption of Machine learning in other domains like finance, retail etc is easier as compared to Healthcare

1. Use of Computer Vision 

i.Medical Imaging

ii.Nuclear Medicine

2. Use of NLP 

i.Physician Notes

ii.Nurse Notes

iii.Speech to text 

iv.Billing Denials annotation

3. Use of Tabular data based machine learning

i.Prediction of Length of Stay

ii.Readmission of a Patient

iii.Early detection of Disease",machin learn healthcar,"['machin', 'learn', 'healthcar']",ML in Healthcare
758,"Geometric Mean Length of Stay

GMLOS is the standard LOS (Length of Stay) metric determined and published by CMS (Centers for Medicare & Medicaid Services) for each diagnostic related grouper (DRG)",geometr mean length stay,"['geometr', 'mean', 'length', 'stay']",Geometric Mean Length of Stay
759,"Features for the prediction of LOS

1. Demography related features

i.Patient Age

ii.Gender

iii.Marital Status

iv.Ethnicity etc.

2. Vitals related features

i.Heart Rate

ii.Blood Pressure

iii.Oxygen level

iv.Temperature etc.

3. Utilization related features

i.Surgery Hours

ii.Number of Past visits to Hospitals

iii.Number of Surgeries etc.

4. Lab report related features

i.Creatinine

ii.Platelet Count 

iii.Magnesium 

iv.Phosporus etc. 

5. Diagnosis related features

i.Arthritis

ii.Heart failure 

iii.Diabetes mellitus 

iv.Acute kidney failure etc. ",featur predict length stay,"['featur', 'predict', 'length', 'stay']",Features for the prediction of LOS
760,"Understanding Fraud

“Fraud” is an intended action or set of actions by users to cause harm to any Business

1. Fraud in the context of Credit Risk

It is different from “Loan Default”, which is a genuine inability of the customer to complete a certain action or abide by certain rules

2. Fraud in the context of E-commerce

It is a genuine attempt by the customer to cause financial losses to the company by unnecessarily increasing their purchasing costs, forward and reverse logistics costs etc",understand fraud,"['understand', 'fraud']",Understanding Fraud
761,"WAYS TO CAPTURE FRAUDULENT BEHAVIOURS

For example, during bank account opening, typing speed, swipe patterns, and every click of the mouse tells a story – one of cybercriminal activity or genuine user behavior. 

BioCatch pulls together advanced behavioral insights to empower organizations with increased visibility into risk. 

Even when we have never seen a user before, BioCatch quickly spots trusted behaviors to create a smooth customer journey during account opening.",way captur fraudul behaviour,"['way', 'captur', 'fraudul', 'behaviour']",WAYS TO CAPTURE FRAUDULENT BEHAVIOURS
762,"Social Engineering Scams

Four ways behavioral biometrics can uncover a real time social engineering scam:

> Length of session: Sessions are longer and behaviors such as aimless mouse movements are common indicating a person is waiting for instructions

> Segmented typing: These patterns indicate dictation such as a cybercriminal reading off an account number to transfer funds

> Hesitation: Longer pauses before performing simple intuitive actions such as clicking on the submit button

> Displacement: Continuous movement of the phone suggests the user is picking the phone up to take instructions and placing it back down to perform the actions instructed by the cybecriminals",social engin scam,"['social', 'engin', 'scam']",Social Engineering Scams
763,"Synthetic Minority Over-sampling Technique

Classification for Modeling for frauds will require SMOTE as incidence rate is pretty low

SMOTE is an approach to the construction of classifiers from imbalanced datasets

SMOTE: Synthetic Minority Over-sampling Technique",synthet minor oversampl techniqu,"['synthet', 'minor', 'oversampl', 'techniqu']",Synthetic Minority Over-sampling Technique
764,"C4-Curious Case of Customer Credit

It means, we are egger to know the whole analysis for customer credit in some banking sector",c4curious case custom credit,"['c4curious', 'case', 'custom', 'credit']",C4-Curious Case of Customer Credit
765,"Overall Objective of Credit Risk

Wishes to optimise the portfolio by reducing the Risk

Solutions are

1. Predictive Analytics

2. Prescriptive Analytics",overal object credit risk,"['overal', 'object', 'credit', 'risk']",Overall Objective of Credit Risk
766,"Predictive Analytics of Credit Risk

Here, we build the model that would predict loan default

i.This is classification problem and

ii.Recall to be given more importance

Features of Predictive Analytics

1.Loan amount, terms

2.Customer default history

a.3 month, 6 month, 9 month

b.Customers outstanding debts/obligations etc

3.Customer demography

a.Age

b.Salary

c.Disposable income

d.PPP (public–private partnership) etc

4.City level features etc",predict analyt credit risk,"['predict', 'analyt', 'credit', 'risk']",Predictive Analytics of Credit Risk
767,"Prescriptive Analytics

Prescriptive Analytics helps us draw up specific recommendations by deeply looking into the ""what"" and ""why"" of a potential future outcome.

It utilizes complicated mathematical algorithms, artificial intelligence and machine learning.
",prescript analyt,"['prescript', 'analyt']",Prescriptive Analytics
768,"Metrics that help in evaluating our model’s accuracy

1. Confusion Matrix

2. F1 Score

3. Gain and Lift Charts

4. Kolmogorov Smirnov Chart (KS score)

5. ROC-AUC

6. Log Loss

7. Gini Coefficient

8. Concordant – Discordant Ratio

9. Root Mean Squared Error",metric help evalu model accuraci,"['metric', 'help', 'evalu', 'model', 'accuraci']",Metrics that help in evaluating our model’s accuracy
769,"Model Health using KS Scores

> Used for classification models. 

> KS is a measure of the degree of separation between the positive and negative distributions.

> The Sooner the Highest KS value occurs, the better the segregation of the goods and the bads

> Gains Chart study coupled with Confusion matrix study will govern final answer
",model health use ks score,"['model', 'health', 'use', 'ks', 'score']",Model Health using KS Scores
770,"Decision Making using Risk Bins

Depending on risk appetite and customer acquisition targets, we can choose the bins post which we wish to stop booking customers",decis make use risk bin,"['decis', 'make', 'use', 'risk', 'bin']",Decision Making using Risk Bins
771,"C4 -Curious Case of Customer Contacts 

It means, we are egger to know the whole analysis for customer contacts in some E-commerce sector",c4 curious case custom contact,"['c4', 'curious', 'case', 'custom', 'contact']",C4 -Curious Case of Customer Contacts 
772,"Important Features in E-commerce

1.Issue Category Wise analysis

2.Product type wise analysis

3.Customer demographic wise analysis

a.Area

b.Purchase history

c.Mode of payment

4.Price of product

5.Courier Mode

6.Time of order placement

7.Seller profile

a.Seller rating

b.Seller return history

8.Time taken to place the order

9.Return Logistics Partner wise",import featur ecommerc,"['import', 'featur', 'ecommerc']",Important Features in E-commerce
773,"Predictive Analytics in  E-commerce

1.Model that would predict a contact on a ticket and help solve it before contact happens

a.This is classification problem and

b.Recall to be given more importance

2.Model that predicts number of contacts to streamline workforce

a.This is regression problem and

b.It helps in WFM",predict analyt ecommerc,"['predict', 'analyt', 'ecommerc']",Predictive Analytics in  E-commerce
774,"Understanding UNIX Operating System

UNIX (also referred to as UNICS) is UNiplexed Information Computing System

● UNIX Operating System was first developed in the 1960s at AT&T Labs.

● Multi-user, multi-tasking system for servers, desktops and laptops.

● Most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X.",understand unix oper system,"['understand', 'unix', 'oper', 'system']",Understanding UNIX Operating System
775,"Kernel and shell of UNIX

Unix Operating System is made up of three parts: the kernel, the shell and the programs.

● Kernel is the main OS program to access the storage (local or remote)

● Shell is the application program for the users to access the storage through kernel.

● Program means all the data or installed application softwares.",kernel shell unix,"['kernel', 'shell', 'unix']",Kernel and shell of UNIX
776,"Understanding LINUX Operating System

LINUX stands for Lovable Intellect Not Using XP. Linux was developed by Linus Torvalds and named after him. 

Linux is an open-source and community-developed operating system for computers, servers, mainframes, mobile devices, and embedded devices.

Linux offers great speed and security, on the other hand, Windows offers great ease of use",understand linux oper system,"['understand', 'linux', 'oper', 'system']",Understanding LINUX Operating System
777,"Time Complexity and  Space Complexity

Performance parameters of any algorithm or application (be it Local or cloud) are as follows:

1. Time complexity or Speed (Time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the input.)

2. Space Complexity (Space complexity of an algorithm quantifies the amount of space or memory taken by an algorithm to run as a function of the length of the input)

3. Ease of use

4. Reliability

5. Security

Time and space complexity depends on lots of things like hardware, operating system, processors, etc. However, we don't consider any of these factors while analyzing the algorithm. We will only consider the time taken and space comsumed by an algorithm.

> Always we need to reduce the iterations in our algorithm to to reduce to run time. We have to think about the loop which is running behind any simple function or method

> We will always take worst case scienario to define the performance of any algorithm

> As a first step of reducing the iterations, we should sort the list or array. Then, we can ignore any part of the list as per our requirement. This will help us to cut the linkage of algorithm's speed with the length of input (length of input may be very large)

> Eficient alternative to loop is writing function for repitivite work. Within the function, we should try to avoid loop and get things done by numpy array and conditional operators

> When we call same function within itself, it is called recursion",time complex space complex,"['time', 'complex', 'space', 'complex']",Time Complexity and  Space Complexity
778,"Ways to connect to an EC2 instance

AWS means Amazon Web Services

AWS free tier EC2 is a free Cloud computer

One EC2 instance is like one remote computer (virtual machine) running generally on Linux and on which we can install whatever software we want.

EC means Elastic Compute. So, we can resize this remote computer as per our requirement.

There are many ways to connect to an EC2 instance

1. Putty

2. Git Bash

3. AWS console panel

4. Command prompt

5. Windows powershell or any other terminal

> windows PowerShell is a more advanced version of cmd (command prompt). It is not only an interface but also a scripting language that is used to carry out administrative tasks more easily.",way connect ec2 instanc,"['way', 'connect', 'ec2', 'instanc']",Ways to connect to an EC2 instance
779,"Security of Remote Computer

Since multi-user OS have several users accessing the system resources simultaneously, it's very important for the system administrators to implement security features within the system. 

These features could include account separation (by login through key pairs, a identity file), user groups, roles, and permissions.",secur remot comput,"['secur', 'remot', 'comput']",Security of Remote Computer
780,"Git Bash Understanding

Git is a revision control software (RCS) (created by the inventor of linux) for tracking changes in any set of files. Means it is a distributed version control system (DVCS).

Git Bash is a command line Windows application to navigate through local or remote computers having windows or linux environment. Through git bash, we can also enter into python environment to write codes.  

Bash means Bourne Again Shell. 

Git Bash is a command line environment used for creating and interacting with Git repository",git bash understand,"['git', 'bash', 'understand']",Git Bash Understanding
781,"Introduction to Repository

Repository means the storage location or the folder where codes are to be stored.

GitHub is a Git repository hosting service. It is usually used for coordinating work among programmers collaboratively developing source code during software development.

All files necessary for certain analysis can be held together and people can add in their code, graphs, etc. as the projects develop.

We can review other people’s code, add comments to certain lines or the overall document, and suggest changes. 

For collaborative projects, GitHub allows us to assign tasks to different users, making it clear who is responsible for which part of the analysis. ",introduct repositori,"['introduct', 'repositori']",Introduction to Repository
782,"
Steps to connect to an EC2 instance

1.  From AWS console, create an EC2 instance, Create & download key pairs and let the machine run

2. Through CLI, enter into the directory of local machine where the key pair is saved and login to the EC2 instance

ssh -i 'key_pair_name' user_name@public_IP_DNS

> ssh- means Secure Shell

> i- means identity",step connect ec2 instanc,"['step', 'connect', 'ec2', 'instanc']","
Steps to connect to an EC2 instance"
783,"Understanding IP address

An IP address is an unique address that identifies a device on the internet or a local network. IP stands for ""Internet Protocol,"" 

EC2 instance public IP address changes when we start stop an instance. Elastic IP's, if attached to the instance, does not change. So, people using the IP address will not get error related to IP address. But there is a charge for elastic IP's also in instance stopped condition.",understand ip address,"['understand', 'ip', 'address']",Understanding IP address
784,"Basic Bash or Linux commands

> Entering into the computer memory

cd folder_name -to enter into a directory

cd / -to enter into the root directory (cd /f/ -to enter intro f drive)

> To view the list of folders in the directory

ls shows the list of files and folders name

ls -l  shows long details of directory

ls -lh shows long details in more human readable forms

ls -al shows all folders including hidden folders

mkdir folder_name  -to create directory

touch file_name -to create empty file

rm file_name -to remove file

rm -r folder_name (rmdir folder_name may be used for emty directory)

mv old_name new_name -for renaming file/directory

mv file_name /directory - moving a file/directory

ln  -symbolic link (shortcut to the file / folder)

> Few other commands

wget web_address -to download any file from website

top/htop -to see current process uses for all the users

clear- to clear all the commands in git bash

> Reading Files (.txt, .csv or .py)

cat file_name -to print the file contents on the screen

vi file_name -to open the file in vi editor (:q to exit)

nano file_name -to open the file in nano editor

head / tail file_name -to look at top / bottom lines of the file

> Searching Files

grep -to search text in files / search file in the folder structure

> Reading directory path

pwd -to print the path of working directory

> To Compress

To zip single file

gzip file_name 

To zip multiple files

tar -cvf new_file_name.tar file_name1 file_name2

> To reduce the size further 

tar -cvzf file_name.tar.gz

> To Decompress 

gzip -d file_name 

tar -xvf tar_file_name

> file file_name -gives details of file type

>  up and down arrow to go to the command history

> after typing few letters of file name press tab, git bash will automatically type the file name

> Copying files

cp file_name directory_name- to copy in that folder

ftp -to copy file using FTP

> Environment Variables

export -to define the new environment variable

env -to print all environment variables

> command --help to see required argument for that command

ctrl+c for canceling or aborting any command

git clone 'http_address' -to download the code from github repository  in the current directory

bash bash_file_name.sh -to execute the whole code in that file

man command_name -to see the manual for any command for learning purpose (press q to escape the manual)-This command can only be used after logging in into a unix machine

uname to display the operating system name

uname -r to display the os version

du -gives information about how much disk space (data usage) each file in the current directory uses",basic bash linux command,"['basic', 'bash', 'linux', 'command']",Basic Bash or Linux commands
785,"Few UNIX commands during project setup

Writing into a new file

cat > my_file.csv

Ram, 80
Sham,30
Jadu,40
Ctrl+d

By '>' we, can redirect the output of any command to a file

who > names.txt saves all the users details connected to the server

wc -l < name.txt counts the no. of lines in the file

Pipe character

(pipe command or pipeline command means doing multiple operation in a single command)

who | wc -l

we can use multiple pipes in a single command

To copy file or folder from local to local

cp or cp -r

To copy file from local to remote computer

scp -i 'key_file_name' /local_file_path username@public_IP_DNS:/remote_directory_path

To copy folder from local to remote computer

scp -i 'key_file_name' -r /local_file_path username@public_IP_DNS:/remote_directory_path

To see the python version in the server (for colab)

import sys

sys.version",unix command project setup,"['unix', 'command', 'project', 'setup']",Few UNIX commands during project setup
786,"File Permission Handling for security

> File permission are of three types read (r), write (w), execute (x) and it is divided among three groups

> Check left most side of the long list (run !ls -l in colab)

> The first (1) character represents file or directory (_ for file, d for directory)

Group-1: The first three (2-4) character represents the permission for the file owner (users,u)

Group-2: Second group of three character (5-7) represents permission for the group (g)

Group-3: The last group of three character (8-10) for everyone else (o)

Change file Permission
(change file mod bits)

chmod u+x o+wx file_name 
or 

chmod 102 file_name

-Three numbers represents three group permission

0- No permission

1- Execute permission

2- Write permission

3- Execute and write permissioin

4- read permission

5- read and execute

6- read and write

7- read+write+execute",file permiss handl secur,"['file', 'permiss', 'handl', 'secur']",File Permission Handling for security
787,"UNIX command in colab notebook with ! Mark

Example: !man cd

> Google colab is a cloud computing coding software hosted on Google cloud server. It has no direct access to our local drives.

> However, Colab provides various options to connect to almost any cloud storage we can imagine. 

> We can either clone an entire GitHub repository to our Colab environment or access individual files from their link",unix command colab notebook mark,"['unix', 'command', 'colab', 'notebook', 'mark']",UNIX command in colab notebook with ! Mark
788,"Creating a Simple Module

> We can save a single module_name.py file in the session and call the function inside the module with import statement

Suppose add() is defined in a module adder.py. 

To import and use the add() function

from adder import add
result = add(2, 3)

or
import adder
result = adder.add(2, 3)

** If we just create a blank __init__.py file within a folder and keep the module_name.py files in that folder, the folder becomes a python library (folder name=library name).

** The library has to be in the same directory (session) where it is called with import statement",creat simpl modul,"['creat', 'simpl', 'modul']",Creating a Simple Module
789,"Basics of modular programming

A module allows us to logically organise our python code. For example, 

1. UI

2. Training

3. API

> Module is a single python file with set of codes that we can import in python

> Modular Programming Application Layout means packaging or foldering procedure

> Main folder or directory is called a python library

> Subdirectories are called python packages.

> All the .py files either inside the main directory or subdirectories are called python modules (module_name.py).

> Layouts are:
1. One-Off Script, 

2. Installable Package and 

3. App with Internal Packages",basic modular program,"['basic', 'modular', 'program']",Basics of modular programming
790,"One-Off Script Layout

> Here is only one directory with the application_name. This directory includes following files:

1. .gitignore file (available in github. This ensures that certain file types are never committed to the local Git repository)

2. application_name.py file (this script houses all the main codes)

3. LICENSE file (available in github) 

4. README.md file (a markdown document with purpose and usage of the application)

5. requirements.txt file (lists down dependencies)

6. setup.py file (This script houses all the installation realted codes. This is the most crucial files in the folder directory.)

7. tests.py file (This script houses all the test related codes)",oneoff script layout,"['oneoff', 'script', 'layout']",One-Off Script Layout
791,"Installable Package Layout

The only difference here is that our application code is now all held in  application_name subdirectory. This subdirectory includes following files:

1. application_name.py file (this script houses all the main codes)

2. __init__.py file (It sets up how packages or functions will be imported into our other files)

3. helpers.py file

Tests are written in a separate subdirectory",instal packag layout,"['instal', 'packag', 'layout']",Installable Package Layout
792,"App with Internal Packages Layout

Here the subdirectories are:

1. bin (houses executable file if any.  If our package is a pure python one, there is nothing to put here)

2. docs (houses all the.md files)

3. application_name

4. data

5. tests (module_1_test.py, module_2_test.py etc.)

> Sub-packages may be formed if necessary

> src- This is the folder that contains our python package codes (module_1.py, module_2.py etc.).",app intern packag layout,"['app', 'intern', 'packag', 'layout']",App with Internal Packages Layout
793,"Data Science Project Layout

Data Science Project Layouts can vary depending on what all
components it contains. Some of the components are:

a. Data Engineering Pipelines (airflow / celery / API / SQL)

b. Model Training Pipeline (CLI)

c. Model Serving (API)

d. Model Validation (CLI)

e. Visualization

f. Report Generation",data scienc project layout,"['data', 'scienc', 'project', 'layout']",Data Science Project Layout
794,"Using cookiecutter for packaging

We can either manually do the packaging or can do the same with few libraries like cookiecutter

pip install cookiecutter (can be used both for CLI and colab)

from cookiecutter.main import cookiecutter (for colab)

> Navigate to a directory where we want to create the new project, then run following in CLI

cookiecutter gh:claws/cookiecutter-python-project (we can run this command in colab with ! mark)

> Full form of CLI is command line interface",use cookiecutt packag,"['use', 'cookiecutt', 'packag']",Using cookiecutter for packaging
795,"stdin, stdout and stderr

All applications have three unique streams that connect them to the outside world. These are referred to as Standard Input, or stdin; Standard Output, or stdout; and Standard Error, or stderr.

1. Standard input is the default mechanism for getting input into an interactive program. This is typically a direct link to the keyboard when running directly in a terminal, and isn’t connected to anything otherwise.

2. Standard output is the default mechanism for writing output from a program. This is typically a link to the output terminal but is often buffered for performance reasons. 

3. The standard error is an alternative mechanism for writing output from a program for errors. ",stdin stdout stderr,"['stdin', 'stdout', 'stderr']","stdin, stdout and stderr"
796,"Reading a static data file from inside a Python package

import pkgutil   # provides binary data

from io import StringIO # for binary to high level data conversion

bytes_data = pkgutil.get_data(__name__, ""my_data.csv"")

s=str(bytes_data,'utf-8')
data = StringIO(s) 
my_df_clean=pd.read_csv(data)",read static data file insid python packag,"['read', 'static', 'data', 'file', 'insid', 'python', 'packag']",Reading a static data file from inside a Python package
797,"Code for Creating python library 

 For final packaging or creating distributable version (wheel file) of python model,

> At first do the basic packaging 

creating README.txt, LICENCE.txt, setup.py and __init__.py files

- Always mension the version of dependencies in the setup.py file install_requires

> Run the following code in CLI (powershell/anaconda prompt/gitbash etc.) in the project file where setup.py file is present, not in the package file where __init__.py file is present

pip install wheel (required during first run)

python setup.py sdist bdist_wheel

 # This will create a dist folder with .whl file for uploading in pypi. 

> Then upload as follows

 python -m pip install --upgrade twine (required during first run)

twine upload --repository testpypi dist/* (shall be used for testing)

twine upload dist/* (for actual deployment in pypi.org)

>  In case hanging problem arise during the connection with pypi.org in any CLI, try any other terminal",code creat python librari,"['code', 'creat', 'python', 'librari']",Code for Creating python library 
798,"Testing during packaging of our own python library

Test-1: Initial packaging without setup.py file

> create a folder 'package_name' in the session with __init__.py file

> upload data_file (.csv,.xlsx etc.) file in the session (if needed by the code)

> !pip install libraries # all dependencies

> from package_name import my_function

> call the my_function

Test-2: Installation through setup.py file

> create a folder 'package_name' in the session with __init__.py file

> upload README, LICENSE and setup.py file in the session

> upload data_file (.csv,.xlsx etc.) file in the session (if needed by the code)

> !pip install package_name # Installing our own python library

> from package_name import my_function

> call the my_function

Test-3: Installation through wheel file

Must contain __init__.py file and data_file (.csv,.xlsx etc.)

> upload the wheel file in the session

> Checking files inside the source distribution file

import tarfile

tar = tarfile.open('my_package-0.0.1.tar.gz')

tar.getnames() 

> Checking files inside final distribution file or wheel file

import pprint

from zipfile import ZipFile

path = 'my_package-0.0.1-py3-none-any.whl'

names = ZipFile(path).namelist()
pprint.pprint(names)

> !pip install package_name-0.0.1-py3-none-any.whl

> from package_name import my_function

> call the my_function

Test-4: Installation through testpypi

> !pip install -i https://test.pypi.org/simple/ package_name

> from package_name import my_function

> call the my_function

Test-5: Installation through pypi (final testing)

> !pip install package_name

> from package_name import my_function

> call the my_function",test packag python librari,"['test', 'packag', 'python', 'librari']",Testing during packaging of our own python library
799,"CVCS and DVCS

Version control allows us to keep track of our work and helps us to easily explore the changes we have made, be it data, coding scripts, notes, etc.

Manual version control with time stamps in the folder is prone to error. That is why DVCS and CVCS which keeps the track of file history.

> CVCS is old technology. Example: SVN (SubVersioN)

> DVCS is most popular because of empowering offline work by cloning the repository

> There aren’t really revision numbers in DVCS

> Every repo has its own revision numbers depending on the changes.

> We can tag releases with meaningful names.

Git bash+Github together make a DVCS.",central version control system distribut version control system,"['central', 'version', 'control', 'system', 'distribut', 'version', 'control', 'system']",CVCS and DVCS
800,"GitHub workflow

The GitHub workflow can be summarised by the “commit-pull-push” mantra.

1. Commit-

Once we’ve saved our files, we need to commit them - this means the changes we have made to files in our repo will be saved as a version of the repo

2. Pull (git pull)-

Now, before we send our changes to Github, we need to pull, i.e. make sure we are completely up to date with the latest version

3. Push (git push)-

Once we are up to date, we can push our changes ",github workflow,"['github', 'workflow']",GitHub workflow
801,"Steps of creating local git repository

1. mkdir my_project

2. cd my_project

3. git init (initializes a hidden git repository which tracks all changes)

4. Copy all the project files here

5. git add files_or_folder_name

6. git status

7. git config --global user.name 'user name'

8. git config --global user.email 'emailid'

9. git commit file_name -m 'my_message'

10. git log -to see the log of the history (git log -n to show limited number of commits)

Git repository never forgets its history. So, we can restore any deleted file or older version file.

git reset commitID",step creat local git repositori,"['step', 'creat', 'local', 'git', 'repositori']",Steps of creating local git repository
802,"Concept of Branch

Master branch (for releasing)- ready to deploy codes

Branch (developer/feature branch)- experimentals codes

> Git commands for creating, activating and entering into the branch

git branch branch_name

git checkout branch_name

git branch",concept branch,"['concept', 'branch']",Concept of Branch
803,"Synchronising local git repository with Github (remote)

git remote add origin github_repository_address

git push --set-upstream origin master",synchronis local git repositori github remot,"['synchronis', 'local', 'git', 'repositori', 'github', 'remot']",Synchronising local git repository with Github (remote)
804,"Merge Conflict

When multiple people want to commit code file with different changes, in the same workflow, then merge conflict occurs.",merg conflict,"['merg', 'conflict']",Merge Conflict
805,"Continuous Integration/ Continuous Deployment or Delivery (CI/CD)

Through github Actions we can build, test and deploy our codes continuously or regularly.

> Concept of master branch and branch introduced for this purpose",continu integr continu deploy deliveri cicd,"['continu', 'integr', 'continu', 'deploy', 'deliveri', 'cicd']",Continuous Integration/ Continuous Deployment or Delivery (CI/CD)
806,"Understanding API

> An API (Application Programming Interface) is a set of functions that allows applications to access data
and interact with external software components, operating systems, or microservices.

> Application means the production deployment of software development. There are two types of Apps: Mobile Apps and Web Apps

> API delivers a user request to a system and sends the system’s response back to a user. Thus, API makes a web page dynamic (web app)

Popular Examples

• YouTube API

• Twitter API

• Facebook API

• Microsoft 365 Graph API",understand applic program interfac,"['understand', 'applic', 'program', 'interfac']",Understanding API
807,"Types of web pages

1. Static webpage (can be created using only html or html+css). It is also called website

2. Dynamic webpage (can be created using html+ flask/JavaScript). This is also called web app

> A website provides visual and text content that users can view and read.  Web application is designed for interaction with end users.

HTML tags-These are added to web documents to control their appearance

HTTP- The rules governing the conversion between a web client and a web server

> Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.",type web page,"['type', 'web', 'page']",Types of web pages
808,"Machine Learning Services

All Machine Learning Services are available using API Interface:

○ Azure Cognitive Services

○ Google Cloud AI and Machine Learning Services

○ AWS AI Services",machin learn servic,"['machin', 'learn', 'servic']",Machine Learning Services
809,"Types of API (Package for creating Apps)

SOAP API -Simple Object Access Protocol, XML based protocol, heavy weight

REST API- Representational State Transfer, HTTP based protocol, light weight",type applic program interfac packag creat app,"['type', 'applic', 'program', 'interfac', 'packag', 'creat', 'app']",Types of API (Package for creating Apps)
810,"Basics of Flask

Flask is a python based framework (microframework) to build web application. 

> It is a very light weight, REST API

> It is based on Werkzeug and Jinja2

> Flask supports database powered application (RDBMS)

Django is a similar web api python package like flask. But Flask is easy to learn.
",basic flask,"['basic', 'flask']",Basics of Flask
811,"Benefits of using the Flask framework

1. It supports Unicode.

2. It has an inbuilt development server.

3. It has a tiny API and can be quickly learned by a web developer.

4. It has vast third-party extensions",benefit use flask framework,"['benefit', 'use', 'flask', 'framework']",Benefits of using the Flask framework
812,"Popular HTTP Requests

1. GET – Gathers information (Pulling all Coupon Codes)

2. PUT –  Updates pieces of data (e.g. Updating Product pricing)

3. POST – Creates (e.g. Creating a new Product Category)

4. DELETE – (e.g. Deleting a blog post)

We can send api request through postman like app or directly through python interpreter with following command:

import requests

response=requests.get('https://api.github.com')

response.text

-Requests library  allows us to send HTTP/1.1 requests using Python
",popular http request,"['popular', 'http', 'request']",Popular HTTP Requests
813,"Use of Virtual Environment

A virtual environment is a tool that helps to keep dependencies required by different projects separate by creating isolated python virtual environments for them. 

As a best practice, create a virtual environment for any new project and install packages (dependencies) as required through pip install command

",use virtual environ,"['use', 'virtual', 'environ']",Use of Virtual Environment
814,"Creating a virtual environment

1. Creating a virtual environment in local machine (through CLI)

> To create a virtual environment, python must be installed in that computer.

> make a project directory, enter into that directory and then run following command

python -m venv env

> Activate the environment

source env/Scripts/activate

2. Creating a virtual environment in EC2 instance (through CLI)

> Here python3 is installed, 

> make a project directory, enter into that directory and then run the following command

python3 -m venv env

> Then activate the environment by sourcing the activate file in the  directory.

source env/bin/activate",creat virtual environ,"['creat', 'virtual', 'environ']",Creating a virtual environment
815,"Understanding pip install

pip install package_name

pip stands for ""preferred installer program"".

This is the Python Package Installer (also used for uninstalling)

Usually, pip is automatically installed if we are working in a virtual environment.

pip show package_name (to see the version of package installed)

> colab notebook is a python virtual environment, so we can use pip without creating a virtual environment

If the project directory has setup.py file, that means we can install the package.

1. For installing a static package, as a tester,

pip install package_name

2. For installing an editable package, as a developer (keep the  setup.py and README.txt, LICENCE.txt files in the session),

pip install -e .

This will install the dependencies
or 

Direct installation from wheel file

pip install package_name-0.0.1-py3-none-any.whl",understand pip instal,"['understand', 'pip', 'instal']",Understanding pip install
816,"Creating a Web App in Flask

> after activating virtual environment, enter into the project directory and run following command

!pip install flask

> write the code for the Web App in app.py file and save in the project directory, 

from flask import Flask, redirect, url_for, render_template, request

app=Flask(__name__)
@app.route('/')
def home_page():
    return 'prediction=200'
app.run() # gives the link for the webapp

> We can integrate html pages in the above coding. Then we need to return render_template('my_file.html')

> url_for -used for dynamic url generation

if total_score>=50:
res='success'
else:
res='fail'
return redirect(url_for(res,score=total_score))

> Creating a dynamic button

@app.route('/submit', method= ['POST','GET'])

> Adding mailing feature in the Flask Application

pip install Flask-Mail

> Flask default port is 5000

> Flask default host is a localhost (127.0.0.1)

> FastAPI default port is 8000",creat web app flask,"['creat', 'web', 'app', 'flask']",Creating a Web App in Flask
817,"Jinja techniques

> Jinja tag is written as 

{% ... %} -For any statement
{{  }} -expressions to print output
{#...#} -for internal comments

> Jinja techniques are used to integrate dynamic coding within html",jinja techniqu,"['jinja', 'techniqu']",Jinja techniques
818,"Ways to edit script code (.py file) in Colab notebook (alternative to vs code)

> Make a copy the script file in google drive

> Mount drive with colab

> !cat 'file_path'

> copy the output code and paste in a new colab file and start editing

> Then, download the .py file

> Colab notebook is a development server, we should not use it in making app.

> VS Code desktop app is very useful for offline editing of python files",way edit script code py file colab notebook altern vs code,"['way', 'edit', 'script', 'code', 'py', 'file', 'colab', 'notebook', 'altern', 'vs', 'code']",Ways to edit script code (.py file) in Colab notebook (alternative to vs code)
819,"Common file formats used in Data Science:

.csv

.txt

.json

.xlsx

.sas

.sql

.pickle

.dta

.h5

.xml

.html

.zip

.pdf

.docx

.jpeg, .png, .gif etc.",common file format use data scienc,"['common', 'file', 'format', 'use', 'data', 'scienc']",Common file formats used in Data Science:
820,"Flaskr as a basic blog application

Flaskr is a basic blog application inside Flask package. Here, users are be able to register, log in, create posts, and edit or delete their own posts (basic five actions).

Flaskr directory has following files and sub-directories:

1. flask app object defined in __init__.py

2. table intilization in db.py

3. table structure is defined in schema.sql

4. user authentication is defined in auth.py

5. creating, editing or deleting a post is defined in blog.py

6. template sub-directory contains all the .html template files

7. static directory contains all the static .css files",flaskr basic blog applic,"['flaskr', 'basic', 'blog', 'applic']",Flaskr as a basic blog application
821,"Hyper Text Markup Language

The Hyper Text Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by  Cascading Style Sheets (CSS) and JavaScript.
We can create static html page as follows:

> we can simply use notepad (or vs code) and write the code and save the file with file_name.html or we can copy the html source code and paste it in notepad/ vs code and start editing

Template inheritance allows us to build a base “skeleton” template that contains all the common elements of our site and defines blocks that child templates can override.",hyper text markup languag,"['hyper', 'text', 'markup', 'languag']",Hyper Text Markup Language
822,"Understanding FastAPI

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.

FastAPI is similar to Flask but FastAPI is really fast and very usefull for ml engineers to present their work in a working way through ml app.

> after activating virtual environment, enter into the project directory

pip install fastapi[all]
pip install uvicorn[standard]

> write the code for the Web App in app.py file and save in the project directory, 
example python code:

from fastapi import FastAPI
app=FastAPI()
@app.get('/')
async def root():
    return 'prediction=200'

> Then run the server with

uvicorn app:app --reload # app(is the file name):app(is the object name)",understand fastapi,"['understand', 'fastapi']",Understanding FastAPI
823,"WSGI vs ASGI

WSGI stands for Web Server Gateway Interface. It is synchronous. Fask, Django uses WSGI

ASGI stands for Asynchronous Server Gateway Interface. It is asynchronous. FastAPI use ASGI",web server gateway interfac vs asynchron server gateway interfac,"['web', 'server', 'gateway', 'interfac', 'vs', 'asynchron', 'server', 'gateway', 'interfac']",WSGI vs ASGI
824,"Features of FastAPI

● Interactive API Docs (Swagger UI)

url/docs # url stands for Uniform Resource Locator

● Data Type Validation",featur fastapi,"['featur', 'fastapi']",Features of FastAPI
825,"Pydantic Data Model

from pydantic import BaseModel

class Item(BaseModel)

name:str
description: Optional[str] = None

price: float

tax: Optional[float] = None

> pydantic guarantees the types and constraints of the output model, not the input data",pydant data model,"['pydant', 'data', 'model']",Pydantic Data Model
826,"FastAPI Working with SQL (RDBMS)

> FastAPI works with any database and any style of library to talk to the database.

> A common pattern is to use an ""ORM"": an ""object-relational mapping"" library. e.g. SQLAlchemy",fastapi work structur queri languag rdbms,"['fastapi', 'work', 'structur', 'queri', 'languag', 'rdbms']",FastAPI Working with SQL (RDBMS)
827,"VS Code for python coding

> Open VS Code and then open a folder (newly created)  in any location of the computer

> Then inside that workspace (folder), create a file with .py extension

> Write or copy paste the code

> Create and activate a virtual environment in that folder and pip install all the required packages through gitbash

> from command palette> select python interpreter> select recommended virtual env

> right click on the code and click Run python file in terminal

>> From view tab> terminal (Ctrl+`), we can open the CLI, here we can write all the git/terminal commands

> Different extensions are used in vs code to give us multiple extra features. We can also build vs code extension and contribute",vs code python code,"['vs', 'code', 'python', 'code']",VS Code for python coding
828,"Ways to use python in local machine through git bash (when anaconda installed)

conda init bash 
(in anaconda prompt)

python --version
(checking the version of python)

ipython

exit()

",way use python local machin git bash anaconda instal,"['way', 'use', 'python', 'local', 'machin', 'git', 'bash', 'anaconda', 'instal']",Ways to use python in local machine through git bash (when anaconda installed)
829,"Docker Basics

> Docker file helps in packaging our project as a container (packages our project like setup.py)

> This is standardized packaging for software and dependencies

> Docker is the company which is pioneer in container technology. So container and docker is used interchangeably

> This is very light weight

> This is the most popular packaging technique

> Solomon Hykes developed Docker in 2013",docker basic,"['docker', 'basic']",Docker Basics
830,"Image or Docker Image

It is a read only binary file which defines all the step for running the app including instruction for installling  packages and libraries as a dependency of the application. 

> Image has the task to create a container.

> We can create multiple containers from the same image

> Docker file is a text document containing few lines of codes (command line) for creating a docker image

Docker file-> Docker image-> Docker Container-> Installation of Packages and libraries
",imag docker imag,"['imag', 'docker', 'imag']",Image or Docker Image
831,"Container or Docker Container

It is the running instance of the docker image

Container is similar as virtual machine (instance) but very different from VM.

Instance is heavy weight but container is very light weight.

In one heavy instance with high RAM, high processor or core, high storage and docker installed, we can run multiple applications in multiple dockers.

All the docker containers share same resources (i.e computational resources like OS kernel, RAM, CPU and Hard disk)

Thus docker container creates boundaries in the instance for separating multiple applications",contain docker contain,"['contain', 'docker', 'contain']",Container or Docker Container
832,"Introduction to dockerhub

Just like github there is dockerhub for shipping docker image for the application

The easiest way to make our images available for use by others inside or outside any organization is to use a Docker registry, such as Docker Hub, or by running our own private registry. 

> Docker registry uses port 5000

> Docker Hub allows only one private repository

> Docker engine is the docker software (an open source containerisation technology)",introduct dockerhub,"['introduct', 'dockerhub']",Introduction to dockerhub
833,"Ways to install docker in EC2 instance

> Before installing, check if the docker is already installed

docker --version

> Then run the following commands

sudo yum update -y

sudo amazon-linux-extras install docker

sudo service docker start

sudo usermod -a -G docker ec2-user",way instal docker ec2 instanc,"['way', 'instal', 'docker', 'ec2', 'instanc']",Ways to install docker in EC2 instance
834,"Basic Docker Commands

$ docker pull python:3.7

$ docker images

$ docker run –d –p 5000:5000 –name node node:latest

$ sudo docker run -i -t alpine /bin/bash (for running images as container)

$ docker ps -a

$ docker stop container_id (or docker kill)

$ docker rm container_ id (for removing the containers)

$ docker rmi image_id (for removing the images)

$ docker build –t node:2.0

$ docker push node:2.0

$ docker ps (to see all running container in Docker)

$ docker --help

> A node is a device or data point in a larger network.

> Multiple nodes means that there are multiple virtual machines.",basic docker command,"['basic', 'docker', 'command']",Basic Docker Commands
835,"Docker or Container volume

> Volumes mount a directory on the host into the container at a specific location

> Mount local source code into a running container",docker contain volum,"['docker', 'contain', 'volum']",Docker or Container volume
836,"Docker Bridge Networking and Port Mapping

 docker container run -p 8080:80 

# 8080 (host_port):80(container_port)",docker bridg network port map,"['docker', 'bridg', 'network', 'port', 'map']",Docker Bridge Networking and Port Mapping
837,"Docker Compose

It is a Multi Container Application",docker compos,"['docker', 'compos']",Docker Compose
838,"Docker swarm

Docker swarm allows the user to manage multiple containers deployed across multiple host machines (native clustering). ",docker swarm,"['docker', 'swarm']",Docker swarm
839,"Understanding Microservices

Microservices are used when the application is huge (generally for e-commerce application)

Earlier there were monolithic applications implemented using a single development stack

Microservices is the architecture of an entire software project. ML application is only one part of that main application (main service). 

> The knowledge of microservices is important for ML engineer, not for data scientist

In microservices architecture, all the part applications are loosely coupled, so their deployment schedule can be independent.

Loosely coupled means, all the part applications are individually complete application and they interact with each other through api service

All the micro services don't need to share the same technology stack, libraries, or frameworks",understand microservic,"['understand', 'microservic']",Understanding Microservices
840,"API Gateway

All the part applications do not directly interact with each other, they interact with each other through a coordinator, called API Gateway",applic program interfac gateway,"['applic', 'program', 'interfac', 'gateway']",API Gateway
841,"Development of Microservices

Use domain analysis to define our microservice boundaries. 

In this stage we list down the requirement of api's for every part application. 

Here, we actually understand the inputs and outputs of every part application and their interrelation.

> Domain means the part applications or microservices

> Design service includes Scheduler service",develop microservic,"['develop', 'microservic']",Development of Microservices
842,"Kubernetes

Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It uses Docker

It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.",kubernet,['kubernet'],Kubernetes
843,"Python code for creating a Streamlit file

Streamlit is a python library. The fastest way to build Web App

# import module

import streamlit as st # All the text cell will be displayed after this import statement

# Title

st.title(""Hello GeeksForGeeks !!!"")

# Header

st.header(""This is a header"")

# Subheader

st.subheader(""This is a subheader"")

# Text

st.text(""Hello GeeksForGeeks!!!"")

# Markdown

st.markdown(""### This is a markdown"")

# success

st.success(""Success"")

# success

st.info(""Information"")

# success

st.warning(""Warning"")

# success

st.error(""Error"")

# Write text

st.write(""Text with write"")

# Writing python inbuilt function 

range()
st.write(range(10))

# Display Images

# import Image from pillow to open images

from PIL import Image

img = Image.open(""streamlit.png"")

# display image using streamlit

# width is used to set the width of an image

st.image(img, width=200)

# checkbox

# check if the checkbox is checked

# title of the checkbox is 'Show/Hide'
if st.checkbox(""Show/Hide""):

# display the text if the checkbox returns True value

st.text(""Showing the widget"")

# radio button

# first argument is the title of the radio button

# second argument is the options for the ratio button

status = st.radio(""Select Gender: "", ('Male', 'Female'))

# conditional statement to print

# Male if male is selected else print female

# show the result using the success function

if (status == 'Male'):
 st.success(""Male"")
else:
 st.success(""Female"")

# Selection box

# first argument takes the titleof the selectionbox

# second argument takes options

hobby = st.selectbox(""Hobbies: "",
     ['Dancing', 'Reading', 'Sports'])

# print the selected hobby

st.write(""Your hobby is: "", hobby)

# multi select box

# first argument takes the box title

# second argument takes the options to show

hobbies = st.multiselect(""Hobbies: "",
      ['Dancing', 'Reading', 'Sports'])

# write the selected options

st.write(""You selected"", len(hobbies), 'hobbies')

# Create a simple button that does nothing

st.button(""Click me for no reason"")

# Create a button, that when clicked, shows a text

if(st.button(""About"")):
 st.text(""Welcome To GeeksForGeeks!!!"")

# Text Input

# save the input text in the variable 'name'

# first argument shows the title of the text input box

# second argument displays a default text inside the text input area

name = st.text_input(""Enter Your name"", ""Type Here ..."")

# display the name when the submit button is clicked

# .title() is used to get the input text string

if(st.button('Submit')):
 result = name.title()
 st.success(result)

# slider

# first argument takes the title of the slider

# second argument takes the starting of the slider

# last argument takes the end number

level = st.slider(""Select the level"", 1, 5)

# print the level

# format() is used to print value

# of a variable at a specific position

st.text('Selected: {}'.format(level))

> streamlit is for demo development not for api development.

> Not only for demo but streamlit is also very useful for our ml model validation

> Using streamlit we can deploy any machine learning model and any python project with ease and without worrying about the frontend
",python code creat streamlit file,"['python', 'code', 'creat', 'streamlit', 'file']",Python code for creating a Streamlit file
844,"Creating a file for streamlit inside the session

%%writefile file_name.py

import streamlit as st

st.title('The Taste of My Mango')

!pip install streamlit

!streamlit run file_name.py",creat file streamlit insid session,"['creat', 'file', 'streamlit', 'insid', 'session']",Creating a file for streamlit inside the session
845,"Creating a Streamlit Project

> Commit the following code as app.py along with requirements.txt, README and data file (may also be accessed via url) in github and deploy the demo app in https://share.streamlit.io/

from drona import text_process

import streamlit as st 

st.title(""Welcome to 'drona' text_process"")

text = st.text_input(""Provide your text"")

text=text.title() # .title() is used to get the input string

ans = text_process(text)

if(st.button('Submit')):   # display the processed text when the submit button is clicked
  st.success(ans)",creat streamlit project,"['creat', 'streamlit', 'project']",Creating a Streamlit Project
846,"Computer Networking

In computer networking, a port is a communication endpoint. At the software level, within an operating system, a port is a logical construct that identifies a specific process or a type of network service. A port is identified for each transport protocol and address combination by a 16-bit unsigned number, known as the port number. 

> A port number is always associated with an IP address of a host and the type of transport protocol used for communication

192.168.0.1:80 (Socket address)

192.168.0.1 (IP address)

80 (port number)",comput network,"['comput', 'network']",Computer Networking
847,"Deploying streamlit app in Google Cloud

> At first we need to create an account in GCP.

> It needs following files:

1. app.py

2. requirements.txt

3. README (md or txt file)

4. Dockerfile (available in github)

> Command to build the application (change the ProjectID and AppName):

ProjectID needs to be copied from gcp console

gcloud builds submit --tag gcr.io/<ProjectID>/<AppName>  --project=<ProjectID>

> Command to deploy the application:

gcloud run deploy --image gcr.io/<ProjectID>/<AppName> --platform managed  --project=<ProjectID> --allow-unauthenticated",deploy streamlit app googl cloud,"['deploy', 'streamlit', 'app', 'googl', 'cloud']",Deploying streamlit app in Google Cloud
848,"Deploying streamlit app in AWS EC2

> Create an AWS account

> Create an instance in AWS console (Config custom TCP port 8501) and download the key pairs (.pem file)

> Enter into the local project directory where the .pem file is present and login into the instance through CLI

ssh -i 'key_pair_name' ec2-user@public_IP_DNS

> Check python version in EC2 to confirm python is installed in that machine

python3

> Copy all the files (app.py, README, requirements.txt) from github

sudo yum install git

git clone https://repository_path

> Install all dependencies 

sudo pip3 install -r requirements.txt

In case need to install or uninstall dependencies individually, enter into the ec2 home directory,

sudo pip3 install package_name

> Then, check streamlit version to confirm the installation of streamlit in the ec2 instance

streamlit version

> Use Tmux to keep the app running

sudo yum install tmux

tmux new -s st_instance

> Enter into the directory where app.py file is present and run the app

streamlit run app.py

> Attaching the session

tmux ls

tmux attach-session -t st_instance

streamlit run app.py

> Config custom domain name is done during actual deployment, not for demo app deployment",deploy streamlit app aw ec2,"['deploy', 'streamlit', 'app', 'aw', 'ec2']",Deploying streamlit app in AWS EC2
849,"ngrok

> ngrok is a cross-platform application that enables developers to expose a local development server to the Internet for end user with minimal effort.",ngrok,['ngrok'],ngrok
850,"4 Phases of ML Lifecycle

Phase 1 is Project Planning and Project Setup

Phase 2 is Data Collection and Data Labeling

Phase 3 is Model Training, Testing and Model Debugging (model explainability check)

Phase 4 is Model Deployment (API>Docker>Cloud) and Model Real Time Testing",4 phase machin learn lifecycl,"['4', 'phase', 'machin', 'learn', 'lifecycl']",4 Phases of ML Lifecycle
851,"Challenges with ML during development

> Development, training and deployment environment can be different

> Tools, libraries and dependencies can complicate deployment

> Tracking and analyzing experiment can become tedious to handle

> Difficult to reproduce experiment as input data changes

As a result, ML Code may end up in a spaghetti jungle.",challeng machin learn develop,"['challeng', 'machin', 'learn', 'develop']",Challenges with ML during development
852,"Challenges with ML in production

> Live data is not equal to training data

> Feature engineering pipeline must match between training and serving infrastructure

> Seamlessly scale up and scale down deployed model

> Continuous training and champion challenger model deployment

> Different technology landscape between development and deployment (check versions of the libraries)",challeng machin learn product,"['challeng', 'machin', 'learn', 'product']",Challenges with ML in production
853,"Data drift and Model drift

In model deployment, there can be data drift or model drift issues. 

Data drift is the change in model input data that leads to model performance degradation

Model drift refers to the degradation of model performance due to changes in data and relationships between input and output variables.

So, drift analysis must be done.

",data drift model drift,"['data', 'drift', 'model', 'drift']",Data drift and Model drift
854,"Machine learning engineering

MLE is the process of using software engineering principles with analytical and data science knowledge together.

This is to take a ML model that’s created and making it available as a product for the end users.
",machin learn engin,"['machin', 'learn', 'engin']",Machine learning engineering
855,"Cloud computing advantages

● Reliability: Depending on the service-level agreement that we choose, our cloud-based applications can provide a continuous user experience with no apparent downtime even when things go wrong.

● Scalability: Applications in the cloud can be scaled in two ways, while taking advantage of autoscaling:

>> Vertically: Computing capacity can be increased by adding RAM or CPUs to a virtual machine.

>> Horizontally: Computing capacity can be increased by adding instances of a resource, such as adding more virtual machines.

● Elasticity: Cloud-based applications can be configured to always have the resources they need.

● Agility: Cloud-based resources can be deployed and configured quickly as our application requirements change.

● Geo-distribution: Applications and data can be deployed to regional datacenters around the globe, so our customers always have the best performance in their region.

● Disaster recovery: By taking advantage of cloud-based backup services, data replication, and geo-distribution, we can deploy our applications with the confidence that comes from knowing that our data is safe in the event of disaster.",cloud comput advantag,"['cloud', 'comput', 'advantag']",Cloud computing advantages
856,"Cloud service models

IaaS-Infrastructure as a Service (Provides storage, computational power and networking (like DNS, Azure VMs and storage services)

PaaS-Platform as a Service (along with IaaS services, it provides OS and runtime)-This is the most popular one

SaaS-Software as a Service. (along with PaaS services, it provides applications as needed. Example is Office 365)",cloud servic model,"['cloud', 'servic', 'model']",Cloud service models
857,"runtime system

A runtime system refers to the collection of software and hardware resources that enable a software program to be executed on a computer system. 

Ex. Docker is a runtime system",runtim system,"['runtim', 'system']",runtime system
858,"CPU vs GPU

The main difference between CPU and GPU architecture is that a CPU is designed to handle a wide-range of tasks quickly, a GPU (Graphics processing unit) is designed to quickly render high-resolution images and video concurrently.

CPU reads Cache Memory data before reading RAM.

RAM is a hardware element where the data being currently used is stored. It is a volatile memory.",central process unit vs graphic process unit,"['central', 'process', 'unit', 'vs', 'graphic', 'process', 'unit']",CPU vs GPU
859,"Azure machine learning platform 

-It has end to end service means it provides all services staring from development to deployment

-It is getting very popular day by day because it helps us to build, train and deploy our ML models faster and foster team collaboration.",azur machin learn platform,"['azur', 'machin', 'learn', 'platform']",Azure machine learning platform 
860,"Azure ML Workspace and Azure ML piplines

> In azureml every run is called an Experiment

> Environment file includes the list of all the dependent libraries and packages for the deployment of our application

> from the environment file azure creates the docker image

> Azure Resource Manager and Azure Portal are used to manage resources.",azur machin learn workspac azur machin learn piplin,"['azur', 'machin', 'learn', 'workspac', 'azur', 'machin', 'learn', 'piplin']",Azure ML Workspace and Azure ML piplines
861,"Connecting to azure ml

We can access azureml through web interface or through python coding by using azureml web api

> !pip install azureml-sdk (enables us to connect to azureml from python) # SDK means Software Development Kit


> import azureml.core

> from azureml.core import Environment, Workspace, Experiment, ScriptRunConfig

experiment_name = 'my_experiment'

ws = Workspace.from_config()

experiment = Experiment(workspace=ws, name=experiment_name)

myenv = Environment.get(workspace=ws, name=experiment_name)

src = ScriptRunConfig(source_directory=project_folder, script='train.py',
compute_target=my_compute_target, environment=myenv)",connect azur machin learn,"['connect', 'azur', 'machin', 'learn']",Connecting to azure ml
862,"Blob storage

Blob storage-Blob stands for Binary Large Object",blob storag,"['blob', 'storag']",Blob storage
863,"import os

The OS module in Python provides functions for interacting with the operating system. OS comes under Python's standard utility modules.",import oper system,"['import', 'oper', 'system']",import os
864,"Different cloud deployment models

1. Private cloud: Privately owned and manged with restricted access

2. Public Cloud: Service provider owned and managed.

3. Hybrid cloud: combination of private and public cloud",differ cloud deploy model,"['differ', 'cloud', 'deploy', 'model']",Different cloud deployment models
865,"MLOps and DevOps

(MLOps)

MLOps is one of the field in ML lifecycle, mainly a part of ML engineering.

MLOps in simple term is DevOps for Machine Learning

MLOps reduces technical debt across machine learning models

ML Lifecycle management tools  are MLFlow, Azure ML, AWS Sagemaker and Google Cloud ML

> Technical debt (or design debt or code debt) is reflects the implied cost of additional rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer.

(DevOps)

DevOps meaning Software development and IT operation

Software development process includes Plan, Code, Build (an executable program that can be used to carry out proper machine level outputs. Building is also called programming) and Test

IT operation process includes Release, Deploy, Operate and Monitor

DevOps breaks the wall of confusion between the development team and operation team by automating the process of delivery or deployment through CI/CD

> When we only use github we can not compare metrices between different versions of our model. But when we use github along with ML lifecycle management tools, we can compare metrices",ml lifecycl devop,"['ml', 'lifecycl', 'devop']",MLOps and DevOps
866,"Agile software development

Agile software development refers to a group of software development methodologies based on iterative development, where requirements and solutions evolve through collaboration between self-organizing cross-functional teams.

MLOps enables the application of agile principles to machine learning projects.

> Agile is a process of colaboration and DevOps is an automation of colaboration",agil softwar develop,"['agil', 'softwar', 'develop']",Agile software development
867,"Key components of MLOps

1. Machine learning

2. DevOps (IT)

3. Data Engineering",key compon ml lifecycl,"['key', 'compon', 'ml', 'lifecycl']",Key components of MLOps
868,"Advantages of MLOps

MLOps is more vibrant than DevOps (ML + DevOps = MLOps), however MLOps is simpler than DevOps

1. Data/Schema versioning apart from code versioning

2. Experiment tracking (Model hyperparameters, Data Distribution, Model performance, feature importance etc)

3. Model artifacts versioning

4. Monitor continuously for data and model drift

5. Continuous re-training of model

6. Capture sensitivity of key features to target",advantag ml lifecycl,"['advantag', 'ml', 'lifecycl']",Advantages of MLOps
869,"Key outcomes of MLOps

1. Model Packaging and Validation

2. Model Reproducibility

3. Model deployment 

4. Model Explainability, Monitoring and Re-training",key outcom ml lifecycl,"['key', 'outcom', 'ml', 'lifecycl']",Key outcomes of MLOps
870,"Model registry and Metadata 

Model registry is for model versioning

(There are three kinds of versioning: Data versioning, model versioning and source code versioning)

ML metadata store is for tracking experiments

> Metadata is a set of data that describes and gives information about other data.",model registri metadata,"['model', 'registri', 'metadata']",Model registry and Metadata 
871,"MLFLow Basics

MLFLow starts from modeling to deployment.

>> It is an open source platform for the machine learning lifecycle. 

>> It has four components: Tracking (experiment, code, data, config, results), Projects (packaging), Models (deployment) and Model registry

>> We can log different metrices using mlflow library and each run of our ml project is saved as individual version. So, we can compare the metrices of different version.

!pip install mlflow

import mlflow

import mlflow.sklearn

import logging

logging.basicConfig(level=logging.WARN)

logger = logging.getLogger(__name__)

mlflow.log_metric(""rmse"", rmse)",mlflow basic,"['mlflow', 'basic']",MLFLow Basics
872,"Apache Spark (PySpark)

Apache Spark is an open-source unified analytics engine for large-scale data processing which requires multiple virtual machines. 

It is very popular analytics tool. 

Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm

> Spark was built on the top of Hadoop MapReduce module and adds superb speed

Hadoop is an open-source software framework for storing and processing big data in a distributed computing environment. The core of Hadoop consists of a storage part “HDFS” (Hadoop Distributed File System) and a processing part “MapReduce”. ",apach spark pyspark,"['apach', 'spark', 'pyspark']",Apache Spark (PySpark)
873,"Advantages of Spark

1. Speed: Spark is 100 times faster than MapReduce (because spark uses RAM)

2. Ease of use: allows to quickly write applications in Java, Scala, R and Python

3. Advanced analytics: support sql quaries, streaming data and advanced analytics",advantag spark,"['advantag', 'spark']",Advantages of Spark
874,"Spark Modules

1. Spark Core: It is the underlying general execution engine. It provides an RDD (Resilient Distributed Dataset) and in-memory (RAM) computing capabilities.

2. Spark SQL and DataFrame

3. Streaming 

4. Mllib: MLlib is a scalable machine learning library

Spark Core is responsible for:

● Memory management and fault recovery

● Scheduling, distributing and monitoring jobs on a cluster

● Interacting with storage systems",spark modul,"['spark', 'modul']",Spark Modules
875,"RDD operations

A Cluster manager breaks the main csv file into multiple RDD blocks for parallel processing in multiple nodes. Then different transformation occurs on RDD's and generates new RDD's. Then through actions we extract our required datasets

> Here the concept of driver node and worker node (can run application code in the cluster) comes in.

> RDD is an immutable and fault-tolerant system",resili distribut dataset oper,"['resili', 'distribut', 'dataset', 'oper']",RDD operations
876,"Anatomy of Spark Application

• Driver

• Application Master

• Spark Context (This is the object for entry point of a PySpark program. This object allows us to connect to a Spark cluster and create RDDs)

• Resource Manager (Cluster Manager)

• Executors",anatomi spark applic,"['anatomi', 'spark', 'applic']",Anatomy of Spark Application
877,"Spark application coding (in colab)

spark dataframe functions and methods are very similar as pandas dataframe (but they are distributed and immutable)

!pip install pyspark

import pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master(""local"")\
        .appName(""Colab"")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

spark  # To check the version

my_spark_df = spark.read.csv(""file_path"", header=True, inferSchema=True)

my_spark_df.show(5)

> Git command for linking the Python API to Spark Core and initializing SparkContext

PySpark Shell",spark applic code colab,"['spark', 'applic', 'code', 'colab']",Spark application coding (in colab)
878,"Directed Acyclic Graph (DAG)

When a task is defined with simple pandas df like commands, spark builds a logical flow of operations that can be represented as directed acyclic graph (DAG) where node represents a RDD partition and the edge represents a data transformation.

> We can check the DAG for job details till the deepest level in a spark web interface

> Task is a defined unit of work within a DAG; it is represented as a node in the DAG graph, and it is written in Python.

> DAG is a collection of all the tasks we want to run, organized in a way that reflects their relationships and dependencies.

> In Graph view, we can visualize each and every step of our workflow with their dependencies and their current status",direct acycl graph dag,"['direct', 'acycl', 'graph', 'dag']",Directed Acyclic Graph (DAG)
879,"Azure Databricks

Azure Databricks is a data analytics platform optimized for the Microsoft Azure cloud services platform. 

Azure Databricks offers three environments for developing data intensive applications: 

1. Databricks SQL, 

2. Databricks Data Science & Engineering, and 

3. Databricks Machine Learning.

> Databricks was built on top of Spark and adds High reliability ",azur databrick,"['azur', 'databrick']",Azure Databricks
880,"Spark application deploy modes

There are two deploy modes that can be used to launch Spark applications on Apache Hadoop YARN - client mode and cluster mode",spark applic deploy mode,"['spark', 'applic', 'deploy', 'mode']",Spark application deploy modes
881,"Apache Airflow

This platform is to programmatically author, schedule and monitor workflows

Luigi, Jenkins, AWS Step Functions, and Pachyderm are the most popular alternatives and competitors to Airflow.

> Airbnb created Apache Airflow",apach airflow,"['apach', 'airflow']",Apache Airflow
882,"Introduction to Jenkins

Jenkins is an open source automation server. It helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration (CI) and continuous deployment or delivery (CD).

Jenkins creates workflows using Declarative Pipelines, which are similar to GitHub Actions workflow. ",introduct jenkin,"['introduct', 'jenkin']",Introduction to Jenkins
883,"Use of Apache Airflow

1. Manage scheduling and running jobs and data pipelines

2. Provides mechanisms for tracking the state of jobs and recovering from failure

3. Ensures jobs are ordered correctly based on dependencies

>> The strongest point of airflow is it can trigger any workflow including spark",use apach airflow,"['use', 'apach', 'airflow']",Use of Apache Airflow
884,"Airflow Components

Airflow Components are Web server, Scheduler, Executor and Metadata database",airflow compon,"['airflow', 'compon']",Airflow Components
885,"Web Server and Scheduler

● Web Server and Scheduler: The Airflow web server and Scheduler are separate processes run (in this
case) on the local machine and interact with the database mentioned above.

Scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete (scheduling the execution of DAGs). 

● airflow.cfg is the Airflow configuration file which is accessed by the Web Server, Scheduler, and
Workers.

● DAGs refers to the DAG files containing Python code, representing the data pipelines to be run by Airflow. The location of these files is specified in the Airflow configuration file, but they need to be accessible by the Web Server, Scheduler, and Workers.

> By Airflow we can create variables where we can store and retrieve data at runtime in the multiple DAGS",web server schedul,"['web', 'server', 'schedul']",Web Server and Scheduler
886,"Metadata Database

● Metadata Database: Airflow uses a SQL database to store metadata about the data pipelines being run.
Postgres is extremely popular with Airflow. Alternate databases supported with Airflow include MySQL.",metadata databas,"['metadata', 'databas']",Metadata Database
887," Executor, Worker and Operator

● The Executor is shown separately in architecture but runs within the Scheduler.

Executors are the mechanism by which task instances get run. e.g. SequentialExecutor, Celery Executor, Kubernetes Executor

● The Worker(s) are separate processes which also interact with the other components of the Airflow architecture and the metadata repository

● Operators determine what actually gets done by a task. e.g. Bash Operator, Python Operator

> Azure Kubernetes Service (AKS) and few others offers serverless capabilities

> By Task duration we will be able to compare the duration of our tasks run at different time intervals",executor worker oper,"['executor', 'worker', 'oper']"," Executor, Worker and Operator"
888,"Applications of Airflow

Apache Airflow can be used to schedule and execute complex workflows

● ETL pipelines that extract data from multiple sources and run Spark jobs or any other data
transformations

● Training machine learning models

● Report generation

● Backups and similar DevOps operations",applic airflow,"['applic', 'airflow']",Applications of Airflow
889,"Understanding Celery

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

Celery instance / Celery application or just celery app is used as the entry-point for everything we want to do in Celery, like creating tasks and managing workers, it must be possible for other modules to import it.",understand celeri,"['understand', 'celeri']",Understanding Celery
890,"Basic of Deep Neural Networks

Neural Network/Deep learning models are supervised learning models. Means we need labeled data for it's functioning.

A small neural network can also beat the performance of any other ML model. That is why deep learning is gaining its popularity, but it is a blackbox model.

> To get better performance we must have large neural networks and more data.

> Neuron applies some forms of non-linearity on the inputs

> The hidden layers of the neural networks actually does some feature engineering inside (learns different weights), about which we don't know. We only get the output

> It is called a blackbox model means, there is no interpretability

> Deep learning is a type of machine learning based on artificial neural networks in which multiple layers of processing are used to extract progressively higher level features from data.

In deep learning, at a very basic level, we convert an unstructured data (text, audio, image) into structured data (matrix)",basic deep neural network,"['basic', 'deep', 'neural', 'network']",Basic of Deep Neural Networks
891,"SNN, CNN and RNN

Types of Neural Network

1. Standard Neural Network (used for normal classification and regression problems on large dataset (>1 lakh experiences))

2. CNN-Convolutional Neural Network (used for image related inputs)

3. RNN-Recurrent Neural Network (used for sequencial input like text, audio)

4. Custom NN (for solving complex problems, like autonomous driving)",snn convolut neural network recurr neural network,"['snn', 'convolut', 'neural', 'network', 'recurr', 'neural', 'network']","SNN, CNN and RNN"
892,"Channels of RGB image

An RGB image has three channels: red, green, and blue. RGB channels roughly follow the color receptors in the human eye, and are used in computer displays and image scanners.

R channel gives us a matrix with the intensity of red in every pexel.

G channel gives us a matrix with the intensity of green in every pexel.

B channel gives us a matrix with the intensity of blue in every pexel.

Seven colours of Rainbow:

Red, Orange, Yellow, Green, Blue, Indigo and Violet.

Red, Green and Blue are additive primary colours because they can make all other colors, even yellow. When mixed together, red, green and blue colours make white. ",channel rgb imag,"['channel', 'rgb', 'imag']",Channels of RGB image
893,"Writing sigmoid function

def sigmoid(z):
   
    s= 1/(1+np.exp(-z))
   
    return s",write sigmoid function,"['write', 'sigmoid', 'function']",Writing sigmoid function
894,"Deeper understanding of deep neural network

We can think a neural network as ensembles of linear regression with kernel and does feature engineering in multiple layers. 

As we initialize the weights of the features randomly, every node of the first layer generates different activations means every node is generating a new feature with the combination of input features with feature importance. This process goes on in multiple layers for generating higher level feature. 

In backpropagation feature importances (weights) are adjusted to get the desired label.

Regularization can be used to train models that generalize better on unseen data, by preventing the algorithm from overfitting the training dataset.",deeper understand deep neural network,"['deeper', 'understand', 'deep', 'neural', 'network']",Deeper understanding of deep neural network
895,"Writing initialize weight function 

> for single neuron(output layer)

def initialize_with_zeros(dim):
        
        w = np.full((dim,1),0)
    b = 0.0
    
    return w, b

> For one hidden and one output layer

def initialize_parameters(n_x, n_h, n_y):

    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
    
    parameters = {""W1"": W1,
                  ""b1"": b1,
                  ""W2"": W2,
                  ""b2"": b2}
    
    return parameters",write initi weight function,"['write', 'initi', 'weight', 'function']",Writing initialize weight function 
896,"Writing optimization function

def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):
    
        w = copy.deepcopy(w)
    b = copy.deepcopy(b)
    
    costs = []
    
    for i in range(num_iterations):
        
        # Cost and gradient calculation 
        
        grads, cost = propagate(w, b, X, Y)
     
         # Retrieve derivatives from grads
        dw = grads[""dw""]
        db = grads[""db""]
        
        # update rule 
        w = w - learning_rate* dw
        b = b - learning_rate* db
       
        # Record the costs
        if i % 100 == 0:
            costs.append(cost)
        
            # Print the cost every 100 training iterations
            if print_cost:
                print (""Cost after iteration %i: %f"" %(i, cost))
    
    params = {""w"": w,
              ""b"": b}
    
    grads = {""dw"": dw,
             ""db"": db}
    
    return params, grads, costs

>> For multi-layer network, optimization function is generally written as update_parameters(parameters, grads, learning_rate) function which returns the updated parameters",write optim function,"['write', 'optim', 'function']",Writing optimization function
897,"Loss function and Cost Function in DNN

when Loss function is denoted by L(ŷ,y) and 'm' is the number of observations or examples

Cost Function, 

J(w,b)=1/m∑L(ŷ(i),y(i)), i=1,m

'w' is the weight and 'b' is the intercept

Partial derivative of J, dJ/dw is referred as 'dw' and dJ/db is referred as 'db'

Loss function for logistic regression

L(ŷ,y)= -[ylogŷ-(1-y)log(1-ŷ)]

Hyperparameter tuning for neural network is an iterative process, for minimizing the cost fuction(loss or error)",loss function cost function deep neural network,"['loss', 'function', 'cost', 'function', 'deep', 'neural', 'network']",Loss function and Cost Function in DNN
898,"Writing propagate function

def propagate(w, b, X, Y):
    
    m = X.shape[1]
    
    # FORWARD PROPAGATION (FROM X TO COST)
    A = sigmoid(np.dot(w.T, X) + b) # use of previously written sigmoid function
    cost = -((np.sum(np.dot(Y, np.log(A).T)+np.dot((1-Y), (np.log(1-A)).T)))/m)

    # BACKWARD PROPAGATION (TO FIND GRAD)
    dw = (np.dot(X,((A-Y).T)))/m
    db = np.sum((A-Y))/m

    cost = np.squeeze(np.array(cost)) # to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array)
    
    grads = {""dw"": dw,
             ""db"": db}
    
    return grads, cost

>> Backpropagation in two layer network

    dZ2 = A2 -Y
    dW2 = (np.dot(dZ2,A1.T))/m
    db2 = np.sum(dZ2, axis=1, keepdims=True)/m
    dZ1 = np.dot(W2.T, dZ2)*(1 - np.power(A1, 2))
    dW1 = (np.dot(dZ1,X.T))/m
    db1 = np.sum(dZ1, axis=1, keepdims=True)/m

>> For multi-layer, propagate function is generally broken into three functions e.g. linear_activation_forward(A_prev, W, b, activation), compute_cost(AL, Y), linear_activation_backward(dA, cache, activation)",write propag function,"['write', 'propag', 'function']",Writing propagate function
899,"Coputation graph

A computational graph is defined as a directed graph where the nodes correspond to mathematical operations.

> We can create the computation graph for linear regression or logistic regression or deep learning model

> feed_dict feeds external data into computational graphs",coput graph,"['coput', 'graph']",Coputation graph
900,"Calculation behind gradient descent

> It calculates 'z' values, 'a' values and cost function in forward propagation considering random weights

> It calculates da, dz, dw and db in back propagation 

> It calculates new weight of w and b for learning rate alpha and the gradient of cost function dw and db. Then another forward propagation begins

> Process continues till we reach global minima for the cost function (cost function means the average error in prediction for all the observations)

> In every step weight is updated by w= w -α*dw

Gradient at a given layer is the product of all gradients at the previous layers.

Weight between input and hidden layer have a constant input in each epoch of training a Deep Learning model

In FeedForward ANN information flow is unidirectional

Effect of false minima is reduced by stochastic update of weights

For a non-continuous objective during optimization in deep neural net Subgradient method is used",calcul behind gradient descent,"['calcul', 'behind', 'gradient', 'descent']",Calculation behind gradient descent
901,"Need of vectorization

> Z=W-transpose.X+b 

> CPU or GPU has the ability of parallel processing (which is called SIMD, single instruction multiple data). Numpy utilizes this ability of the computer

> for Z calculation in non-vectorized form, we use explicit for loop (Explicit describes something that is very clear), which is series calculation and very slow. Thus the need of vectorization comes into picture. However, in a deeper network, we cannot avoid a for loop iterating over the layers.

>> During numpy dot product of two vectors W-transpose and X, SIMD is utilized making the caculation very fast

>> X is a matrix (or vector) in which each row is one feature and each column is one training example. (For other ml algorithm it is transposed)",need vector,"['need', 'vector']",Need of vectorization
902,"Vectorized implementation of forward propagation for layer l

Z [l]=W [l]A [l - 1]+b [l]

A [l]=g [l] (Z [l])

> Input layer is called the activation zero. A[0]=X.

> First hidden layer is the first layer and output layer is the final layer

> A neural network with 1 input layer and 2 hidden layers and 1 output layer have total 3 layers

> Shape of X is (nx,m) where nx= total no. of features and m= total no. of observations",vector implement forward propag layer l,"['vector', 'implement', 'forward', 'propag', 'layer', 'l']",Vectorized implementation of forward propagation for layer l
903,"Notations in DNN

> When we represent a single observation, we use small letter notation (z,w,x)

> When we represent multiple observations, we use capital letter notation (Z,W,X). So, Z,W and X are all vectors or matrices, not single numbers",notat deep neural network,"['notat', 'deep', 'neural', 'network']",Notations in DNN
904,"SIMD units

SIMD units refer to hardware components that perform the same operation on multiple data operands concurrently. Typically, a SIMD unit receives as input two vectors (each one with a set of operands), performs the same operation on both sets of operands (one operand from each vector), and outputs a vector.",simd unit,"['simd', 'unit']",SIMD units
905,"Writing predict function

def predict(w, b, X):
        
    m = X.shape[1]
    Y_prediction = np.zeros((1, m))
    w = w.reshape(X.shape[0], 1)
    
    # Compute vector ""A"" predicting the probabilities of a cat being present in the picture
    
    A = sigmoid(np.dot(w.T, X) + b)
   
    for i in range(A.shape[1]):
        
               if A[0, i] > 0.5 :
            Y_prediction[0,i] = 1
        else:
            Y_prediction[0,i] = 0
        
    return Y_prediction

> We must avoid using for loop in our algorithm by using following code in predict function

def predict(parameters, X):

    A2, cache = forward_propagation(X, parameters) # use of propagate or forward_propagation function

    predictions = np.where(A2 > 0.5, 1, 0)

return predictions",write predict function,"['write', 'predict', 'function']",Writing predict function
906,"Element-wise matrix multiplication

It is different from common matrix product

a = np.random.randn(3, 3)

b = np.random.randn(3, 1)

c = a * b

c.shape=(3,3)

In another case,

a = np.random.randn(1, 3)

b = np.random.randn(3, 3)

c = a * b is not possible because it is not possible to broadcast more than one dimension",elementwis matrix multipl,"['elementwis', 'matrix', 'multipl']",Element-wise matrix multiplication
907,"Notation for multiple layers

In the superscript of Z, W, X or b if we write in parenthesis, it denotes the observation no.

Z(1) means the Z value for the first observation or training example

In the superscript of Z, W, X or b if we write in square bracket, it denotes the layer no.

Z[1] means the Z value in the first layer

Neurons are also called nodes or units

In the subscript we denote the node number

Say, layers no. = l

g[l]= activation function of layer l

> Multiple layer neural network is called deep neural network

parameters (which the model learns) for a neural network are

W[1], b[1], W[2], b[2],…. W[l], b[l]",notat multipl layer,"['notat', 'multipl', 'layer']",Notation for multiple layers
908,"Activation functions

The activation function maps linear regression function, z(x) (z=y_pred in linear regression) and generates an activation/output value f(z) 

> In general activation function is denoted by 'g'

1. sigmoid function

Sigmoid function is only used for binary classification problems

if z is large in positive side then σ(z) or g(z)≈1

if z is large in negative side then σ(z) or g(z)≈0

*** a=g(z), g(z) is called the 'a' value

2. Tanh function

This activation function is similar as sigmoid, but the range is -1 to 1

But tanh function is better than sigmoid function, because it centralizes the data and it is useful for second layer learning. (Hyperbolic Tangent activation function also centralizes the data)

3. ReLU and Leaky ReLU function

Rectified Linear Unit (ReLU) is function where the output is non-linear for a certain period and then becomes linear.

ReLU is better activation function than Sigmoid and Tanh, because learnig becomes very slow for larger values of z as the gradient vanishes (flatens) in the case of sigmoid and tanh

The range of ReLU is 0 to z

The range of Leaky ReLU is -0.01z to z

> In different layer we can use different activation functions if required",activ function,"['activ', 'function']",Activation functions
909,"Linear and nonlinear activation function

We apply linear activation function (similar to not applying any activation) in neural network for regression problems

Non linear activation functions (Sigmoid, Tanh, ReLU or Leaky ReLU) are called kernel approximation or kernel functions.",linear nonlinear activ function,"['linear', 'nonlinear', 'activ', 'function']",Linear and nonlinear activation function
910,"Keras and TensorFlow

Keras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. 

TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs.  Keras is built in Python which makes it way more user-friendly than TensorFlow.

Keras is a deep learning wrapper on TensorFlow

Wrapper methods measure the “usefulness” of features based on the classifier performance

>> TensorFlow architecture works in 3 parts (Data pre-processing, Model building and Train & estimate the model)",kera tensorflow,"['kera', 'tensorflow']",Keras and TensorFlow
911,"Problem of zero initialization

> When all the intial weights are considered as zero, all the nodes in a particular layer will learn the same function even after multiple iterations of gradient descent. This is same as taking single node.

>> That is why random weights are initialized but very small values. This breaks the symmetry and allows different nodes to learn independently of each other 

>> Also for other ml model if all the weights are initiallized as same, after many iterations the model learns same weight for all the features. This is called the “symmetry”",problem zero initi,"['problem', 'zero', 'initi']",Problem of zero initialization
912,"Initializing parameters for the model (for deep network)

Suppose we store the values of n[l] in layer_dims

layer_dims=[6,4,3,1] # 6 is the number of input features

parameters = {}

for l in range(1, len(layer_dims)):

    parameters['W'+str(l)]= np.random.randn(layer_dims[l],layer_dims[l-1])*0.01

    parameters['b'+str(l)]= np.zeros((layer_dims[l], 1))",initi paramet model deep network,"['initi', 'paramet', 'model', 'deep', 'network']",Initializing parameters for the model (for deep network)
913," Deep Learning capabilities:

1. Classification Only (C)

2. Classification with Memory (CM)

3. Classification with Knowledge (CK)

4. Classification with Imperfect Knowledge (CIK)

5. Collaborative Classification with Imperfect Knowledge (CCIK)-ensembles of weak learners

> As range of hidden layers boom, model capability will increase",deep learn capabl,"['deep', 'learn', 'capabl']", Deep Learning capabilities:
914,"RNN for machine translation

Why is an RNN used for machine translation?

> It can be trained as a supervised learning problem.

> It is applicable when the input/output is a sequence (e.g., a sequence of words).",recurr neural network machin translat,"['recurr', 'neural', 'network', 'machin', 'translat']",RNN for machine translation
915,"Layer dimension notation in Neural Network

n[l] means no. of nodes in l th layer

n[0]= no. of nodes in the input layer (=no. of features in the dataset)

n[1]=no. of nodes in the first hidden layer",layer dimens notat neural network,"['layer', 'dimens', 'notat', 'neural', 'network']",Layer dimension notation in Neural Network
916,"Weight notation in Neural Network

For a two layer network, say there are three input nodes (X1,X2 and X3) and one hidden layer with two nodes, h1[1] and h2[1] and one output layer, h1[2]

> then the weights learned by h1 will be w11,w21,w31 and weights learned by h2 will be w12,w22,w32 for the same set of inputs

Initialized weight vector, W= [[w11,w12], [w21,w22], [w31,w32]] or 

[list_of_weight_for_input_variable_1, list_of_weight_for_input_variable_2, list_of_weight_for_input_variable_3,]

Thus, the shape of W is (3,2)

Weight vector for the first layer,

W[1]=W.T= [[w11,w21,w31],[w12,w22,w32]]

Thus, the shape of W[1] is (2,3) or shape of W[l] is (n[l], n[l-1])",weight notat neural network,"['weight', 'notat', 'neural', 'network']",Weight notation in Neural Network
917,"Bias notation in Neural Network

b[1]= [[b1],[b2]]

shape of b[l] is (n[l],1)

for all the training example, m, shape of b[l] is (n[l],m)

> Shape of Z and A will be same as shape of b",bias notat neural network,"['bias', 'notat', 'neural', 'network']",Bias notation in Neural Network
918,"Network notation for i th experience

A two layer network, say there are three input nodes (X1,X2 and X3) and one hidden layer with two nodes, h1[1] and h2[1] and one output layer, h1[2] is denoted as follows:

X1(i)--a1[1]--
X2(i)--a2[1]--a1[2]--y(i)
x3(i)--",network notat th experi,"['network', 'notat', 'th', 'experi']",Network notation for i th experience
919,"Forward propagation and backpropagation

During forward propagation, in the forward function for layer II we need to know what is the activation function in a layer.

During backpropagation, the corresponding backward function also needs to know what is the activation function for layer II, since the gradient depends on it.

> backpropagation uses reversed range to calculate the gradients

for l in reversed(range(L-1)): # L is the number of layers

We use cache to pass variables computed during forward propagation to corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives

 >> linear_cache stores the values of (A, W, b) as a tuple and activation_cache stores the value of Z as numpy.ndarray",forward propag backpropag,"['forward', 'propag', 'backpropag']",Forward propagation and backpropagation
920,"shallow"" neural network

""shallow"" neural network is a term used to describe NN that usually have only one hidden layer (Standard NN) as opposed to deep NN which have several hidden layers, often of various types.

If we need a large shallow network (large no. of nodes or logic gates) to compute any function, using deep NN will be exponentially smaller.",shallow neural network,"['shallow', 'neural', 'network']","shallow"" neural network"
921,"Basics of Improving Deep Neural Networks

We can split the dataset into three parts: Training, Development and Testing (fully untouched one, used as unseen data) as an improvement method

In case of big data, there is no need to follow 70-30 or 80-20 train-test split (old way of spliting data). We may take 98 % in training, 1% in development and 1% in testing, because here 1% is also a huge number and testing data does not contribute in learning, it is only for evaluation.

> But for human learning model, testing data also contribute in learning when we perform error analysis. This is the case like reinforcement learning model

> Always make sure that our train,dev and test have similar distribution of data",basic improv deep neural network,"['basic', 'improv', 'deep', 'neural', 'network']",Basics of Improving Deep Neural Networks
922,"Basic 'recipe' for all machine learning models

> Build our first simple model or system quickly and then slowly keep on increasing complexity

> Then we should use Bias/Variance analysis and error analysis to prioritize next step 

1. check the model has high bias or not (comparing trainig error with expected error) and take action.

2. Check the model has high variance or not (comparing training and dev error) and take action

In deep learning we do not concentrate on bias variance trade off. But our model should not have 

1. High bias, low variance

2. Low bias, high variance

3. High bias, high variance (also possible)

Our optimal model must have

1. low bias and low variance (optimum)",basic recip machin learn model,"['basic', 'recip', 'machin', 'learn', 'model']",Basic 'recipe' for all machine learning models
923,"DNN model complexity

> Big neural network may make our model overfitting (so, we can easily control model complexity by adjusting the layers and nodes. We can also reduce model complexity of big neural network by applying regularization)

>  Less data with large no. of dimensions can also make our model overfitting

For normal machine learning models, after a threshold limit of data, model performance saturate. 

But for deep neural network there is no limit of data. More data means more accuracy.

> Regularization prevent overfitting in neural network

For extreamly large lambda, most of the weights will become zero and the model will become very simple.

> For classification problem, every neuron in a neural network learns a classification boundary.

More the classification boundary becomes non-linear, more the model becomes complex. ",deep neural network model complex,"['deep', 'neural', 'network', 'model', 'complex']",DNN model complexity
924,"Dropout regularization

Apart from L1 and L2 regularization, there is Dropout regularization (here we randomly make some node zero to make the model simpler)

> Weight decay is a regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration",dropout regular,"['dropout', 'regular']",Dropout regularization
925,"Implementing dropout ('inverted dropout'): 

Here in a certain hidden layer (l=3), if we define keep prob=0.8. It means, with probability 0.2 the nodes will become zero in layer 3. So, if in layer 3 there were 10 nodes, 2 nodes will be deleted randomly

> There is no implementation of dropout during test, it is only implemented during training

> Generally used in computer vision (CNN architecture)

> Dropout can not be used in input and output layers

> forward_propagation_with_dropout

 D1 = np.random.rand(A1.shape[0], A1.shape[1])      # Step 1: initialize matrix D1
D1 = (D1 < keep_prob).astype(int)                  # Step 2: convert entries of D1 to 0 or 1
A1 = np.multiply(A1, D1)                           # Step 3: shut down some neurons of A1
A1 = A1/keep_prob                                  # Step 4: scale the value of neurons that haven't been shut down

> backward_propagation_with_dropout

dA2 = np.multiply(dA2, D2)     # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob          # Step 2: Scale the value of neurons that haven't been shut down",implement dropout invert dropout,"['implement', 'dropout', 'invert', 'dropout']",Implementing dropout ('inverted dropout'): 
926,"Data Augmentation

When there is less data and more data can not be made available (generally in case of images), we generate more data by twisting, rotating or zooming the orginal iamge. This is an useful technique to reduce variance",data augment,"['data', 'augment']",Data Augmentation
927,"Early stopping technique to stop overfitting

When the cost function for train and dev set diverge after certain no. of iterations, then we need to stop training at that iteration.

> This is not a very good technique to deal with overfitting",earli stop techniqu stop overfit,"['earli', 'stop', 'techniqu', 'stop', 'overfit']",Early stopping technique to stop overfitting
928,"Importance of normalized inputs

When cost function is drawn from normalized inputs, the curve becomes symmetric and gradient descent reaches minima faster",import normal input,"['import', 'normal', 'input']",Importance of normalized inputs
929,"Vanishing/exploding gradients

If there is very large no. of layers, 
Then lower (<1) weights becomes very very small or vanishes when multiplied in every layer to reach final activation and thus the gradient also vanishes.(curve of y or J becomes parallel to x or weight axis). Means with the change in feature value there is no change in prediction. 

And higher (>1) weights becomes very very large or explodes when multiplied in every layer to reach final activation and thus the gradient also explodes. (curve of y or J becomes perpendicular to x or weight axis)",vanishingexplod gradient,"['vanishingexplod', 'gradient']",Vanishing/exploding gradients
930,"Gradient checking

Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).

Gradient checking is slow, so we don't want to run it in every iteration of training. We would usually run it only to make sure our code is correct, then turn it off and use backprop for the actual learning process.",gradient check,"['gradient', 'check']",Gradient checking
931,"Batch size and iteration

Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.

Iteration is one time processing (forward + backward) for a batch of images (say one batch is defined as 16 in a CNN, then 16 images are processed in one iteration). 

> Epoch is once all the  training examples are processed one time individually of forward and backward to the network.",batch size iter,"['batch', 'size', 'iter']",Batch size and iteration
932,"Optimization Algorithms

1. Mini-Batch gradient descent

2. Gradient Descent with momentum

3. RMSprop

4. Adam Optimization Algorithm 

5. Learning rate decay


",optim algorithm,"['optim', 'algorithm']",Optimization Algorithms
933,"1. Mini-Batch gradient descent

For extremely large dataset, instead of passing the entire dataset at once, we can pass the entire dataset in small batches or mini batches.

This will solve the memory issue (RAM) for this extremely large computation

> In case of batch gradient descent, the cost function smoothly decreases with increase in no. of iterations 

> In case of mini-batch gradient descent, the cost function decreases with oscillation with increase in no. of iterations (because here iterations are performed with mini batches. Thus for some iteration cost function increases slightly and for some iteration decreases slightly)

> Average of the training samples produces stable error gradients and convergence.",1 minibatch gradient descent,"['1', 'minibatch', 'gradient', 'descent']",1. Mini-Batch gradient descent
934,"> Notation of Mini-batch

In the superscript of Z, W, X or b if we write in curly braces, it denotes the mini-batch no.

Z{1} means the Z value for the first mini-batch",notat minibatch,"['notat', 'minibatch']",> Notation of Mini-batch
935,"> Choosing our mini-batch size

If mini-batch size=m: Batch gradient descent (we face memory shortage issue)-(here m is the total no. of observations)

If mini-batch size=1: stochastic gradient descent (here we loose the speed of vectorization)

Thus, choosing the mini-batch size in between 1 and m is important

> For <2k data points (for images or audio), we can use mini-batch size=m

> For larger dataset, we typically use mini-batch size=64(2^6), 128(2^7), 256(2^8), 512(2^9)",choos minibatch size,"['choos', 'minibatch', 'size']",> Choosing our mini-batch size
936,"2. Gradient Descent with momentum

> To speed up our algorithm to reach the global minima, we use weighted gradient in the weight update equation, in place of using simple gradients (dw & db)

Hyperparameter =α

Additional hyperparameter=  β # It controls the amount of history (momentum) 

> With increase in the value of momemtum (β), learning becomes fast to reach the global minima for the cost function

> If β =0, it is like simple gradient descent with lowest learning speed.",2 gradient descent momentum,"['2', 'gradient', 'descent', 'momentum']",2. Gradient Descent with momentum
937,"Exponentially weighted averages

weighted avg. of 't' th observation= β*weighted avg. of (t-1) th observation+ (1-β)*value of 't' th observation

-We can smoothen the curve of weighted avg. by taking higer values of β (this can be utilized for noisy data)

> Higher β means giving more imporatance to the past

> Increasing β will shift the weighted avg. line slightly to the right.

> We use a bias correction factor to the weighted avg. to handle the bias in the initial period.

Corrected weighted avg. of 't' th observation= weighted avg. of 't' th observation/(1- β^t)",exponenti weight averag,"['exponenti', 'weight', 'averag']",Exponentially weighted averages
938,"
3. RMSprop

> To speed up our algorithm to reach the global minima, we use RMS gradient in the weight update equation, in place of using simple gradients (dw & db)

Hyperparameter =β≈0.9",3 rmsprop,"['3', 'rmsprop']","
3. RMSprop"
939,"4. Adam Optimization Algorithm 

It is a combination of Gradient Descent with momentum (AdaGrad) and RMSprop

Adam stands for Adaptive moment estimation

Hyperparameter =β1≈0.9,β2≈0.999,ε=10^-8

> Here, batch gradient descent or mini batch gradient descent can be used.",4 adam optim algorithm,"['4', 'adam', 'optim', 'algorithm']",4. Adam Optimization Algorithm 
940,"5. Learning rate decay

> SGD wonder arround the global minima, it never reaches global minima.

> To solve this issue, if the learning rate (step size) decays with progress, SGD will wonder arround very close vicinity of the global minima.

> We can define any function of alpha so that its value decreases with increase in epoch",5 learn rate decay,"['5', 'learn', 'rate', 'decay']",5. Learning rate decay
941,"Problem of local optima

In neural network, it is unlikely to get stuck in a bad local optima. 

There are milions of dimensions (weights) in neural network, so it is impossible to get all the partial derivates as zero at one point giviing a bad local optima.

> But problem of plateau can make the learning slow",problem local optima,"['problem', 'local', 'optima']",Problem of local optima
942,"Hyperparameters tuning in DNN 

There is a long list of hyperparameters in neural network to get the best set of weights

> To get the best values of hyperparameters we should not use grid search CV (takes all the combinations of X's), we should use random search CV (takes combination of X's randomly) for neural network. We also use Bayesian optimisation (it is usually employed to optimize expensive-to-evaluate functions. It takes combinations of X's intelligently) for XGBoost, Random Forest

Tuning scale varies from hyperparameter to hyperparameter and it is not uniform for most of the hyperparameters. Scale may be logarithmic or exponential.

> Retest the values of hyperparameters occationally

Hyperparameter tuning in pandas is like babysitting one model

Hyperparameter tuning in Caviar is like training many models in parallel

> During hyperparameter search, whether we try to babysit one model (“Panda” strategy) or train a lot of models in parallel (“Caviar”) is largely determined by the amount of computational power we can access",hyperparamet tune deep neural network,"['hyperparamet', 'tune', 'deep', 'neural', 'network']",Hyperparameters tuning in DNN 
943,"Batch normalization

This is actually a standardization technique

We know, normalizing of inputs for a layer is to make the learning faster. So the idea is, we can also normalize the outputs of a layer which are entering to the next layer through activation, to make the learning fastest.

> Thus the 'z' values for a layer are normalized to get z-tilda for that layer and then through activation function it enters the next layer

> Here we introduce two more learnable parameters γ (gamma) and β

> γ and β can be learned using Mini-batch gradient descent, Gradient descent with momentum, RMSprop or Adam, not just with gradient descent.

> γ and β set the mean and variance of the linear variable z[l] of a given layer.

> Batch norm has a slight regularization effect on the model

> In the normalization formula, we use epsilon (ε) to avoid division by zero

z_norm = (z – μ) /√(σ^2 - ε) # z is the output for i th observation

After training a neural network with Batch Norm, at test time, to evaluate the neural network on a new example we should perform the needed normalizations, use μ and σ^2 estimated using an exponentially weighted average across mini-batches seen during training",batch normal,"['batch', 'normal']",Batch normalization
944,"Multi-class classification

> So far we have discussed about binary classification of audio or iamges through deep neural network 

> For multi-class classification, softmax activation function works well",multiclass classif,"['multiclass', 'classif']",Multi-class classification
945,"Deep learning programming framework

Caffe/Caffe2

CNTK

DL4J

Keras (simplest one)

Lasagne

mxnet

PaddlePaddle

TensorFlow

Theano

Torch",deep learn program framework,"['deep', 'learn', 'program', 'framework']",Deep learning programming framework
946,"Python coding for DNN

import numpy as np

import tensorflow as tf

# define all the tf.Variables and cost function

train = tf.train.GradientDescentOptimizer(0.01).minimize(my_cost_function)

init = tf.global_variables_initializer()

session = tf.Session()

# for initializing the weights

session.run(init)

print(session.run(w))

# for starting the first iteration

session.run(train)

print(session.run(w))

# run a for loop for 'n' no. of iteration

Alternate session initialization

with tf.Session() as session:
session.run(init)
print(session.run(w))",python code deep neural network,"['python', 'code', 'deep', 'neural', 'network']",Python coding for DNN
947,"Different vector operations in tensor flow

For matrix multiplication,

tf.matmul(m1, m2)

For matrix addition,

tf.add(m1,m2)

Thus Linear Regression equation can be represented as follows:

Y=tf.add(tf.matmul(W,X), b)

For reducing along rows,

tf.reduce_sum(x, 0) 

# means we are squeezing from bottom and top so that two separate rows become one row.",differ vector oper tensor flow,"['differ', 'vector', 'oper', 'tensor', 'flow']",Different vector operations in tensor flow
948,"Cross entropy loss function (for classification problem), 

C= Σ y(i)*log(y_pred(i)) can be written as follows:

C= tf.reduce_sum(Y*tf.log(out))

",cross entropi loss function classif problem,"['cross', 'entropi', 'loss', 'function', 'classif', 'problem']","Cross entropy loss function (for classification problem), "
949,"Computational graph 

> Variables in TensorFlow are also known as tensor objects

> To perform caculations in TensorFlow we launch the computational graph in a session

> Calculations can be done in parallel in computational graph",comput graph,"['comput', 'graph']",Computational graph 
950,"Basics Structuring ML project

Structuring ML project means, having clear strategies for creating a good deep learning model

As there are too many parameters to tune in a neural network, a clear strategy must be decided.",basic structur machin learn project,"['basic', 'structur', 'machin', 'learn', 'project']",Basics Structuring ML project
951,"Orthogonalization Basics

Orthogonalization is a system design property that ensures that modification of an instruction or an algorithm component (hyperparameter) does not create or propagate side effects to other system components (hyperparameter)",orthogon basic,"['orthogon', 'basic']",Orthogonalization Basics
952,"Fundamental assumptions of supervised learning

1. We can fit training set well on cost function

2. The training set performance generalizes pretty well to the dev/test set.

",fundament assumpt supervis learn,"['fundament', 'assumpt', 'supervis', 'learn']",Fundamental assumptions of supervised learning
953,"Setting up our goal

1. Single number evaluation metric

For example, if a metric shows multiple numbers of evaluation like precision and recall for different models, it will be difficult for us to decide the best model. But if the metric shows single number evaluation like F1 score, we can decide the best model esily.

2. Satisficing and optimizing metrics

When the requirement of the model is not suitable for single number evaluation metric, then we shall create one optimizing meric and N-1 satisficing merics meeting different criterias.",set goal,"['set', 'goal']",Setting up our goal
954,"Train/dev/test distribution

> If we have a large dataset, then we should mix and shuffle randomly all the data coming from different regions before train/dev/test distribution.

> Choose a dev set and test set to reflect data we expect to get in the future and consider important to do well on.

Size of Dev set

> Size of the dev set to be big enough to detect differences in algorithm/models we are trying out and select the best model.

Size of test set

> Size of the test set to be big enough to give high confidence in the overall performance of our selected model.

>> If our model takes weeks to get trained, then it is worthy to buy faster computers that could speed up our teams’ iteration speed and thus our team’s productivity.",traindevtest distribut,"['traindevtest', 'distribut']",Train/dev/test distribution
955,"Changing dev/test sets and metrics

> If there is any special criteria (say, in no way the model should classify pornographic image as cat) for choosing the best model, then the weightage of that criteria must be included in our evaluation metric (the weightage may also be included in the cost function for the improvement in model performance) ",chang devtest set metric,"['chang', 'devtest', 'set', 'metric']",Changing dev/test sets and metrics
956,"Comparing to human level performance

> For structured data, our model generally takes less time to reach human level performance

> For natural perception  (audio or images), it takes lot of time to surpass human level performance and reach bayes optimal error level. (There is very less difference between human error and bayes error for natural perception)",compar human level perform,"['compar', 'human', 'level', 'perform']",Comparing to human level performance
957,"Human level error and avoidable bias

Human level error does not indicate to the performance of a single human, it indicates to the performance of whole human kind or team of expert human beings.

> Thus human level error is the closest proxy for bayes error.

> Difference between training error and human level error is the avoidable bias.

> Difference between training error and dev error is the variance

Problems where ML  significantly surpasses human-level performance

-Online advertising

-Product recommendations

-Logistics (predicting transit time)

-Loan approvals",human level error avoid bias,"['human', 'level', 'error', 'avoid', 'bias']",Human level error and avoidable bias
958,"Error Analysis

> Look at dev examples (which our model misclassified) to get ideas to improve performance

> Evaluate multiple ideas in parallel preparing a table

> If there is incorrectly labeled data (substantial percentage), clean up incorrectly labeled data 

> If we correct the incorrectly labeled data on the dev set, then we should also correct the incorrectly labeled data in the test set, so that the dev and test sets continue to come from the same distribution. (Most Deep Learning models are robust enough to allow for slightly different distributions between train and dev/test.)",error analysi,"['error', 'analysi']",Error Analysis
959,"Mislabeled data

Systematic mislabeled data/example in the training set, hamper our model performance. So, we must pay attention.

But if there is randomly mislabeled data/example, they do not impack our model much.",mislabel data,"['mislabel', 'data']",Mislabeled data
960,"Mismatched training and dev/test data (if the training and dev data are from different distribution)

In this case, how can we identify how much error is due to variance and how much error is due to mismatched data

> Here the concept of Training-dev set is introduced.  Training-dev set has same distribution as training set, but not used for training

> Then compare the errors for all the sets for clear understanding

If we have a large data-mismatch problem, then our model does a lot better on the training-dev set than on the dev set",mismatch train devtest data train dev data differ distribut,"['mismatch', 'train', 'devtest', 'data', 'train', 'dev', 'data', 'differ', 'distribut']",Mismatched training and dev/test data (if the training and dev data are from different distribution)
961,"Learning from multiple tasks
 
1. Transfer learning

2. Multi-task learning",learn multipl task 1 transfer learn,"['learn', 'multipl', 'task', '1', 'transfer', 'learn']","Learning from multiple tasks
 
1. Transfer learning"
962,"1. Transfer learning 

(here learned weights for one task is utilized for classifying another task)

Transfer Learning is a machine learning technique where we use a pre-trained neural network to solve a problem that is similar to the problem the network was originally trained to solve. We can use partial network and edit the final layer to fit our objectives.

(A pre-trained model is nothing but a deep learning model someone else built and trained on some data and can be used to solve simialr problem.)

Many pre-trained models are available online.

Eg. LeNet-5

When transfer learning makes sense?

> Task A&B has same input X

> We have lot more data for Task A than Task B

> Lower level features from A could be helpful for learning B

(Low-level features are minor details of the image, like lines or dots, that can be picked up.  High-level features are built on top of low-level features to detect objects and larger shapes in the image)

> For transfer learning in CNN, people always borrow the entire 20+ layers networks and only train the last output layer to customize towards their use cases. This is not the case for LSTM with different vocabularies. But widely used in (NLP) LSTM with similar vocabularies

> Transfer learning from CNN to RNN is not possible",1 transfer learn,"['1', 'transfer', 'learn']",1. Transfer learning 
963,"2. Multi-task learning

> For identifying multiple objects from a picture, we can utilize multiple neural network with binary classification.

> Else, we can utilize single neural network with multi-task learning. Here the final layer will be have no. of nodes equal to no. of tasks

> It is not particularly feasible to build one dataset to rule them all that is fully labeled with every category we would ever need.

use of multi-task neural network 

> Training on a set of tasks that could benefit from having shared lower-level features.

> Usually, amount of data we have for each task is quite similar",2 multitask learn,"['2', 'multitask', 'learn']",2. Multi-task learning
964,"End to end deep learning

> The biggest advantage of deep learning is that, if we have big enough data, there is no need of feature engineering and lots of human efforts to develop a model, a neural network can do every thing to achieve the goal. This is called end to end deep learning.

Applying end-to-end deep learning

> Key question: Do we have sufficient data to learn a function of the complexity need to map x to y?",end end deep learn,"['end', 'end', 'deep', 'learn']",End to end deep learning
965,"Pros and cons of end to end deep learning

Pros: 

> Let the data speak

> Less hand designing of components needed

Cons:

> May need large amount of data

> Excludes potentially useful hand-designed components which may be useful to improve model performance",pros con end end deep learn,"['pros', 'con', 'end', 'end', 'deep', 'learn']",Pros and cons of end to end deep learning
966,"Difference between Multi-class and multi-task learning

Multi-class learning is the terminology used when we predict a single label for each input, but each label is a single element from a set of possible labels. Multi-task learning is when we have different problems that need to be solved simultaneously (multiple labels for each input)",differ multiclass multitask learn,"['differ', 'multiclass', 'multitask', 'learn']",Difference between Multi-class and multi-task learning
967,"Testing the model for the entire dataset

This would cause the dev and test set distributions to become different. This is a bad idea because we’re not aiming where we want to hit

If a teacher is teaching mathematics to a child. She teaches him some examples such as 1+1=2, 2+2=4 and 1+3 = 4. If she takes a test of that child and asks him only 1+1 or 2+2 or 1+3, he will give the exact answer. Based on this test she can not tell if the child has learned mathematics or not.",test model entir dataset,"['test', 'model', 'entir', 'dataset']",Testing the model for the entire dataset
968,"Perceptual task 

Perceptual tasks consist of studies aimed at distinguishing various biomarkers of perception including visual-spatial attention, auditory attention and olfactory attention

Here, perceptual task means computer vision through CNN

> CNN is used in

1. Image classification

2. Text classification

3. Object Detection

> A Convolution matches or surpasses the output of an individual neuron to a visual stimuli. 

> Convolution means a thing that is complex and difficult to follow

> When a high resolution image (3300*4200) is used in a neural network, for RGB channels (for black and white screen, there is only one channel which denotes the intensity of white), it will be a huge number of feature inputs (3300*4200*3) for one  observation, that is why for simplification CNN came into picture. 
> We should always try to use famous architectures like LeNet-5, AlexNet, VGG, ResNet, Inception etc. in our model instead of building it from the scratch.",perceptu task,"['perceptu', 'task']",Perceptual task 
969,"Basics of Pixel 

Pixel  means a minute area of illumination on a display screen, one of many from which an image is composed.

The smallest resolution Windows supports is 640x480 pixels (meaning 640 dots horizontally by 480 vertically). Better video cards and monitors are capable of much higher resolutions. The standard resolution used today is 1024(W)x768(H).

Say computer monitor display area is 12'(W)x7'(H).

Then ppi (pixels per inch) in width= 1024/12=85

But 72 ppi is the standard

A 12-megapixel camera (3300 x 4200 pixel), for example, can produce images with more than 12 million total pixels.

Considering 300 ppi for the best quality print, maximum size of the print will be

W=3300/300=11'

H=4200/300=14'

Early digital camera: 100x100 pixels (0.01 megapixels)",basic pixel,"['basic', 'pixel']",Basics of Pixel 
970,"Convolution on Black-and-white Image

For Edge detection, we use filters (e.g. 3x3 matrix) to perform convolution operation with the picture intensity matrix:

>> vertical edge detection filter

>> Horizontal edge detection filter

>> There are different types of vertical and horizontal fileters like Sobel filter, Scharr filter etc.

Here, pixel locations are the feature names like cell reference in excel. 

Thus, total no. of pixel= total no. of feature= total no. of inputs

All the values of one picture intensity matrix are the feature values for one observation 

In CNN, we use filter but the weights of the filter are not defined. We leave it to the model to learn by itself.",convolut blackandwhit imag,"['convolut', 'blackandwhit', 'imag']",Convolution on Black-and-white Image
971,"Convolution operation

> convolution operator is *
Say, picture intensity matrix is I with size 4X4 and filter size is 2X2 

I*F=resultant matrix

Then, the first element of the resultant matrix will be

Z11= I11*F11+I12*F12+I21*F21+I22*F22

>> Here, I11,I12,I21,I22 are the raw feature values(raw x's) and F11,F12,F21,F22 are the learnable weights(w's), Z11,Z12,Z13.. are the refined feature values (refined x's)

Here the equation of Z is similar like linear regression equation. It  is only a refinement of raw data (picture intensity matrix) to refined data (output matrix)

> Convolution operation can be compared with PCA as it is used for better understanding of important features",convolut oper,"['convolut', 'oper']",Convolution operation
972,"Padding in CNN

During convolution operation, the size of the intensity matrix is reduced. This is not a best practice because we are loosing informations.

Thus the concept of padding came into picture to retain the size of input matrix.

> Here before performing convolution operation, we are padding the input intensity matrix with zeros (in one or multiple layers depending on the filter size) in all sides equally

> minimum filter size is 3X3 for convolution operation with padding [to pad with atleast one layer (3-1)/2]

> Valid Convolution = convolution with no padding (size of the resultant volume is reduced)

> Same Convolution = convolution with  padding (size of input volume and size of resulting volume are same). Also known as  Zero Padding or Same padding

> A 3D matrix is called volume",pad convolut neural network,"['pad', 'convolut', 'neural', 'network']",Padding in CNN
973,"Strided Convolution

Stride or step size is an extra hyperparameter, stride = 2,3, etc.

> Size of the input matrix is very much reduced in this case

> Stride is the distance between two consecutive receptive fields",stride convolut,"['stride', 'convolut']",Strided Convolution
974,"Convolution on RGB image (Coloured image)

Here instead of 2D filter we need 3D filter (e.g. 3x3x3 matrix) but the output is a 2D matrix

If we need 3D matrix output instead of 2D matrix output, then we need to apply mutiple 3D filters",convolut rgb imag colour imag,"['convolut', 'rgb', 'imag', 'colour', 'imag']",Convolution on RGB image (Coloured image)
975,"Types of layer in a convolutional network:

1. Convolution (CONV) layer-it has parameters to learn 

2. Pooling (POOL) layer- may not be considered as layer, as no parameters to learn (only takes the max. or avg. value from the input volume according to filter size and stride)

Though they do not have parameters to learn, but they affect the backpropagation (derivatives) calculation.

3. Fully connected (FC) layer-This is nothing but a single layer of standard NN, it has parameters to learn",type layer convolut network,"['type', 'layer', 'convolut', 'network']",Types of layer in a convolutional network:
976,"Pooling Layer

1. Max pooling-it is similar like convolution operation but instead of taking the summation of all the numbers, here max number is considered in the output matrix.

Here hyperparameters are: Filter size(f) and stride (s)

max-pooling layer is to create a feature map containing the most prominent features of the previous feature map (also adds local invariance)

from keras.layers import MaxPooling2D

2. Average Pooling (rarely used)-instead of max we take average",pool layer,"['pool', 'layer']",Pooling Layer
977,"Necessity of Convolution

> By the use of CONV+POOL layer we are actually reducing the number of inputs features and then using a FC layer. Thus the parameter to learn decreases significantly

> CNNs have a couple of concepts called parameter sharing and sparsity of connections

1. Parameter sharing

- It reduces the total number of parameters, thus reducing overfitting.

- It allows a feature detector to be used in multiple locations throughout the whole input image/input volume.

2. Sparsity of connections (helps in translation invariance)-
means each activation in the next layer depends only on a small number of activations from the previous layer.

As the number of features are extreamly large for an image, CONV+POOL layers are used combinedly and repeatedly as a feature reduction technique

>> Training a deeper network (adding additional layers to the network) allows the network to fit more complex functions (or complex feature) and thus almost always results in lower training error.",necess convolut,"['necess', 'convolut']",Necessity of Convolution
978,"Calulating the size of output volume for convolution or pooling

say, 
size of input matrix= W X H

size of filter = F(W) X F(H)

stride = S(W), S(H)

Padding= P

Then,
Output width = (W-F(W)+2P)/S(W)+1

Output height = (H-F(H)+2P)/S(H)+1

say, 
size of input volume= W X H XC

size of filter = F(W) X F(H) XF(C)

stride = S(W), S(H)

Padding= P

no. of filters= n

If, C=F(C), equal no. of channels for input volume and filter,

Then, size of output volume,
{(W-F(W)+2P)/S(W)+1}X {(H-F(H)+2P)/S(H)+1} Xn

> If 2D pooling is applied on 3D input, output channels will be= input channels, C

> For an input volume 30X30X3 and without conv and pool layer, no. of inputs is 2700

> For an input volume 15x15x8, if pad=2, the dimension of the resulting volume is 19x19x8 (2+15+2=19)

> For an input volume 63x63x16, if filtered with 7x7 (f X f), stride of 1 and “same” convolution, then padding=3  
Because, (f-1)/2 = (7-1)/2=3

> For an input volume 32x32x16, and apply max pooling with a stride of 2 and a filter size of 2. The output volume is 16x16x16

> Say, input is a 300 by 300 color (RGB) image, and we use a convolutional layer with 100 filters that are each 5x5. Then the hidden layer have 7600 parameters (including bias) 7500 (5X5X3X100) (without bias)

> For an input volume 63x63x16, and convolve it with 32 filters that are each 7x7, using a stride of 2 and no padding. The output volume is 29x29x32

> Suppose we have an input volume of dimension 64x64x16. Then a single 1x1 convolutional filter have 17 parameters (including the bias)",calul size output volum convolut pool,"['calul', 'size', 'output', 'volum', 'convolut', 'pool']",Calulating the size of output volume for convolution or pooling
979,"Notation for multiple CONV layers

> First feature map or the input volume 39X39X3

n_W[0]=n_H[0]=39, n_C[0]=3

> First CONV layer

f[1]=3, s[1]=1, P[1]=0, n=10

> Second feature map 37X37X10

n_W[1]=n_H[1]=37, n_C[1]=10

> Second CONV layer

f[2]=5, s[2]=2, P[2]=0, n=20

> Here f[1]=3 means 3X3 and equal channel as input volume 
and s[1]=1 means 1,1",notat multipl conv layer,"['notat', 'multipl', 'conv', 'layer']",Notation for multiple CONV layers
980,"Common behavior of all the CNN architecture

Dimensions of the intensity matrix changes as follows:

H and W reduces and Depth increases with the progress in layer.

Then the intensity matrix is flattened out and used in FC layer for final output. ",common behavior convolut neural network architectur,"['common', 'behavior', 'convolut', 'neural', 'network', 'architectur']",Common behavior of all the CNN architecture
981,"LeNet-5 architecture

It was intially built to identify hand written digits (black-and-white)

It is an old architecture (sigmoid and tanh activation used)

AVG POOL is used

Output layer-softmax",lenet5 architectur,"['lenet5', 'architectur']",LeNet-5 architecture
982,"AlexNet architecture

Modern architecture

MAX POOL is used

ReLU activation is being used

Output layer-softmax",alexnet architectur,"['alexnet', 'architectur']",AlexNet architecture
983,"VGG-16 architecture

Modern architecture 

Multiple CONV layers are being used

16 signifies 16 numbers of layers. It is a very big network.

ReLU activation is being used

Output layer-softmax

",vgg16 architectur,"['vgg16', 'architectur']",VGG-16 architecture
984,"ResNet and Inception architecture

More Advance and Complex Networks

1. ResNet- Residual Network

> ResNet helps NN to learn identity function/features very easily

> The skip-connection makes it easy for the network to learn an identity mapping between the input and the output within the ResNet block.

> Using a skip-connection helps the gradient to backpropagate fast and thus helps us to train deeper networks

2. Inception

> A single inception block allows the network to use a combination of 1x1, 3x3, 5x5 convolutions and pooling.

> Inception blocks usually use 1x1 convolutions to reduce the input data volume’s size before applying 3x3 and 5x5 convolutions.

> By adding bottleneck layers we can reduce the computational cost in the inception modules.",resnet incept architectur,"['resnet', 'incept', 'architectur']",ResNet and Inception architecture
985,"Network in Network

Network in Network is actually 1X1 convolution

> Mainly used for 3D pictures

> for 3D picture there are more than 3 channels, in fact lot many channels

> 1X1 convolution is used to decrease the no. of channels (or depth)

> It saves huge computational cost

We can use ConvNet for 1D image like ECG

We can also use ConvNet for 3D image like CT Scan image

e.g. 
For 3D image, input volume has size 32x32x32x16 (this volume has 16 channels)",network network,"['network', 'network']",Network in Network
986,"Using open-source implementation

For computer vision, always try to use as much open source implementation as possible

1. Building on top of other's implementation

> search for network architecture source code on google

> Go to Github repo

> Download or gitclone

2. Use in-built implementations present in deep learning frameworks

> Keras

> Tensorflow

> Pytorch

3. Use transfer learning as much as possible

Transfer learning is very useful when we have very less inputs (say 100 pictures)

When having a small training set to construct a classification model, use an open-source network trained in a larger dataset freezing the layers and re-train the softmax layer.",use opensourc implement,"['use', 'opensourc', 'implement']",Using open-source implementation
987,"Common Augmentation methods

> Mirroring

> Cropping

> Rotation

> Blurring

> Shearing

> Local warping

> Colour shifting etc.",common augment method,"['common', 'augment', 'method']",Common Augmentation methods
988,"Sources of data for any ML model

There are two sources of data for any ML model

1. Input data (raw/structured)

2. Engineered or hand engineered or synthesised data",sourc data machin learn model,"['sourc', 'data', 'machin', 'learn', 'model']",Sources of data for any ML model
989,"Object detection 

Object detection (multi-task) requires less data than image recognition /classification (multi-class)

Speech recognition model need much more data than image recognition/ classification model",object detect,"['object', 'detect']",Object detection 
990,"Tips for winning competitions

> Ensembling: Train several networks independently and average their outputs

> Multi-crop at test time: Run clssifier on multiple versions of test images and average their results",tip win competit,"['tip', 'win', 'competit']",Tips for winning competitions
991,"Importing Kaggle dataset in colab

> Register in kaggle.com

> Go to account section

> click on ""Create New API Token"" and a kaggle.json file will be downloaded

> Open colab and the code

!pip install -q kaggle

from google.colab import files
files.upload()

and choose the kaggle.json file to upload in session

> Create a kaggle folder

!mkdir ~/.kaggle

> Copy the kaggle.json file into the folder

!cp kaggle.json ~/.kaggle/

> Change file permission

!chmod 600 ~/.kaggle/kaggle.json

> List kaggle datasets

!kaggle datasets list

> kaggle.com> competition> data> Copy API command and run in colab (make sure you have participated in the relevant competition)

!kaggle competitions download -c ubiquant-market-prediction

> run upzip command to unzip

(Easy way is download the data from kaggle.com> upload in google drive> connect colab to drive> read the data)",import kaggl dataset colab,"['import', 'kaggl', 'dataset', 'colab']",Importing Kaggle dataset in colab
992,"Common steps for pre-processing Image Data

> Connection with the data

# Importing libraries (needs to be imported as required)

import numpy as np
import copy
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from lr_utils import load_dataset
from public_tests import *

%matplotlib inline
%load_ext autoreload
%autoreload 2

train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

> First feelings of the data

index = 25

plt.imshow(train_set_x_orig[index])

print (""y = "" + str(train_set_y[:, index]) + "", it's a '"" + classes[np.squeeze(train_set_y[:, index])].decode(""utf-8"") +  ""' picture."")

> Deeper understanding of the data: Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)

m_train = train_set_x_orig.shape[0]

m_test = test_set_x_orig.shape[0]

num_px = train_set_x_orig.shape[1]

> Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)

train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T

test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

> ""Standardize"" the data (255 is the maximum value of a pixel channel)

train_set_x = train_set_x_flatten / 255

test_set_x = test_set_x_flatten / 255",common step preprocess imag data,"['common', 'step', 'preprocess', 'imag', 'data']",Common steps for pre-processing Image Data
993,"Building blocks of deep learning model

> Exploration and pre-processing of data

> Writing helper functions (sigmoid, initialize_with_zeros, propagate, optimize, predict etc.)

> Initialize (w,b)

> Optimize the loss iteratively to learn parameters (w,b):

-Computing the cost and its gradient

-Updating the parameters using gradient descent

> Use the learned (w,b) to predict the labels for a given set of examples

> Then, we merge all helper functions into a model

> Tuning the learning rate (a ""hyperparameter"") can make a big difference to the algorithm.",build block deep learn model,"['build', 'block', 'deep', 'learn', 'model']",Building blocks of deep learning model
994,"Difference between image classification and object detection

Image Classification helps us to classify what is contained in an image. 

Image Classification with Localization will specify the location of single object in an image whereas 

Object Detection specifies the location of multiple objects in the image. ",differ imag classif object detect,"['differ', 'imag', 'classif', 'object', 'detect']",Difference between image classification and object detection
995,"Image Classification with Localization

Similar to ConvNet (image classification) but output is slightly different.

Also called semantic segmentation (Locating objects in an image by predicting each pixel as to which class it belongs to)

Need to output- 

1. whether any object is present or not

2. center of the bounding box

3. height and width of the bounding box and

4. class labels 

So, y will be a vector

Y=[1(logistic unit,P_c),bx,by,bh,bw,1(class-c1),0(class-c2)]

If the logistic unit of y vector= 0, means no class found
Then we don't care (?) about other terms

y=[0,?,?,?,?,?,?]

size of y is 1 X (5+no. of classes) 

size of y is 1 X 7 for 2 classes, 1 X 25 for 20 classes",imag classif local,"['imag', 'classif', 'local']",Image Classification with Localization
996,"Landmark detection

This is similar to Image classification with localization.

Here, we can analyze the object in the image by defining multiple landmarks (points) during training

All the landmarks are like individual task. Thus, if there are N landmarks, there will  be 2N (yes+no) output units

y_pred has shape (2N, 1)",landmark detect,"['landmark', 'detect']",Landmark detection
997,"Convolutional implementation of sliding windows for detecting multiple objects

> If we take sliding window or multiple cropped image for detecting multiple objects in a picture and then pass all the cropped images to the same ConvNet, then this will become computationally very expensive

> To substantially reduce the computational cost, there is convolutional implementation of sliding windows where single image is fed to a single ConvNet",convolut implement slide window detect multipl object,"['convolut', 'implement', 'slide', 'window', 'detect', 'multipl', 'object']",Convolutional implementation of sliding windows for detecting multiple objects
998,"Object detection algorithm

Steps towards outputting a single and accurate bounding box

1. YOLO algorithm

2. Intersection over union (IoU)

3. Non-max supression

4. anchor box algorithm",object detect algorithm,"['object', 'detect', 'algorithm']",Object detection algorithm
999,"1. YOLO algorithm

(for finding the bounding box)

YOLO stands for You Only Look Once

> The biggest advantage of using YOLO is its superb speed – it’s incredibly fast and can process 45 frames per second. 

> For each grid cell, there is label for training

> Here, y is a 3D array",1 yolo algorithm,"['1', 'yolo', 'algorithm']",1. YOLO algorithm
1000,"Steps in YOLO algorithm

> YOLO first takes an input image

> The framework then divides the input image into grids (say a 19 X 19 grid)

> Image classification with localization are applied on each grid. YOLO then predicts the bounding boxes and their corresponding class probabilities for objects 

Say, on any input volume 19x19 grid is applied, for 20 classes, and with 5 anchor boxes. 

output volume for 19X 19 grid and 20 classes will be 19 X19X25

As there are 5 anchor boxes, then output volume will be 19x19x(5x25)",step yolo algorithm,"['step', 'yolo', 'algorithm']",Steps in YOLO algorithm
1001,"Intersection over union  algorithm

(for finding accurate bounding box)

2. Intersection over union (IoU)= size of intersection/size of union (between two bounding boxes)

correct if IoU >=0.5

If, the upper-left box is 2x2, and the lower-right box is 2x3. The overlapping region is 1x1, Then IoU=1*1/(2*2+2*3-1*1)=1/9",intersect union algorithm,"['intersect', 'union', 'algorithm']",Intersection over union  algorithm
1002,"3. Non-max supression

(for finding single bounding box)

When there are multiple bounding boxes for same object, then this algorithm ignores the non-max bounding boxes (probabillity of less than maximum).

Finally we get non-max supressed output",3 nonmax supress,"['3', 'nonmax', 'supress']",3. Non-max supression
1003,"4. anchor box algorithm

If there are multiple objects in a single grid and overlapping objects have same center of bounding box,

Then, we define overlapping bounding boxes as anchor box-1, anchor box-2 etc.

Output for each anchor box will be stacked in the width of output volume",4 anchor box algorithm,"['4', 'anchor', 'box', 'algorithm']",4. anchor box algorithm
1004,"Face Verification 

> Input image, name/ID

> Output whether the input image is that of the claimed person",face verif,"['face', 'verif']",Face Verification 
1005,"Face Recognition

> Has a database of K persons

> Get an input image

> Output ID if the image is any of the K persons (or 'not recognized') 

> Here, instead of learning the pattern of the image, it learns a ""similarity"" function

similary function, d(image1, image2) = degree of difference between images

> Face recognition is a tougher task than image classification or face verification.

Because, it is an one-shot learning means learning from one example to recognize the person again.

In image classification, we were supposed to tell is there any human being in the image or not from multiple images.

Face recognition is one step ahead. So, ConvNet alone will not serve the purpose.",face recognit,"['face', 'recognit']",Face Recognition
1006,"Siamese network

Here, an image is passed through a ConvNet and stopped before the softmax layer and we get the encoded image.

Then we calculate the difference between the encodings

The learning goal is 

> If the two images are the same person, difference is small

> If the two images are the different person, difference is large

Here the loss function (for one example) is called Triplet loss

Anchor image (A), Positive image(P) and Negative image (N)

If α is the margin between two similarity functions, 

d(A,P)+ α<= d(A,N)

or,
d(A,P)-d(A,N)+ α <=0 

or,
max(|f(A)−f(P)|^2−|f(A)−f(N)|^2 +α, 0)

Triplet loss = L(A,P,N)

Choose triplets that are hard to train on (N image is very much similar as A) to make the model robust

So, A,P, N should not be choosen randomly",siames network,"['siames', 'network']",Siamese network
1007,"Training set for calculating Triplet loss 

Each training example consists of one anchor image, one positive image and one negative image

To train using the triplet loss we need several pictures of the same person.",train set calcul triplet loss,"['train', 'set', 'calcul', 'triplet', 'loss']",Training set for calculating Triplet loss 
1008,"Neural Style Transfer

From a content image (C) and style image (S), the model will generate a stylish content image (G)

Neural style transfer is not really machine learning, but an interesting side effect/output of machine learning on image tasks.
",neural style transfer,"['neural', 'style', 'transfer']",Neural Style Transfer
1009,"Finding generated image

1. Initiate G randomly (G=1000x1000x3)

2. Use gradient descent to minimize J(G)

J(G)=αJ-content(C,G)+βJ-style(S,G)

In each iteration the pixel values of the generated image G is optimized through gradient descent",find generat imag,"['find', 'generat', 'imag']",Finding generated image
1010,"Style and style matrix 

Style is defined as the correlation between activations across channels

In the deeper layers of a ConvNet, each channel corresponds to a different feature detector. 

The style matrix G[l] measures the degree to which the activations of different feature detectors in layer l vary (or correlate) together with each other.",style style matrix,"['style', 'style', 'matrix']",Style and style matrix 
1011,"Examples of Models with sequence data

1. Speech recognition

2. Music generation

3. Sentiment classification

4. DNA sequence analysis

5. Machine translation

6. Video activity recognition

7. Name entity recognition etc.

> Recurrent Neural Networks (RNNs) are a well-known method in sequence models.

> A recurrent neural network can be unfolded into a full-connected neural network with infinite length.",exampl model sequenc data,"['exampl', 'model', 'sequenc', 'data']",Examples of Models with sequence data
1012,"Notation in RNN

Here the elements (words) of each sequence (sentence or example) are denoted with numbers in angle bracket in superscript. 

e.g. 
x<1>, x<2>, x<3>,x<t>

and y<1>, y<2>, y<3>, y<t>

x(i)<t> means 't' th feature element of 'i' th example

y(i)<t> means 't' th target element of 'i' th example

> Here every layer has two set of inputs x<t> and a<t-1>; two set of outputs a<t> and y<t>

a<t>= g_1(w_aa*a<t-1> + w_ax*x<t> + b_a)

y<t> = g_2(w_ya*a<t> + b_y)
 
> Back propagation in RNN is named as Backpropagation through time",notat recurr neural network,"['notat', 'recurr', 'neural', 'network']",Notation in RNN
1013,"Vectorization of words

In simple vectorization of words,

> We define a vocabulary of that particular domain

> Then by one hot encoding we convert any word into vector",vector word,"['vector', 'word']",Vectorization of words
1014,"Problem of a standard neural network in sequence data

Problems:

-Inputs and outputs can be of different lengths in different examples

-Doesn't share features learned across different positions of text

Solution:

Recurrent Neural Network (RNN)

> Recurrent means occurring  repeatedly.",problem standard neural network sequenc data,"['problem', 'standard', 'neural', 'network', 'sequenc', 'data']",Problem of a standard neural network in sequence data
1015,"Summary of RNN

1. For a standard network, there are multiple inputs but one activation output in every layer and we generate output ŷ from the final layer. 

In case of RNN, we may generate output ŷ from every layer and calculate the loss in every layer. (although, depending on the type of problem, no. of layers, no. of inputs (Tx) and no. of outputs (Ty) may not be equal) 

2. Thus every layer of RNN is a standard neural network with multiple nodes and it is repeating for all other words of the expression or statement. 

3. Only difference is that we are passing the understanding of every word from one standard network to the next standard network. Finally calculating the loss function summing up the losses from all of the standard networks

4. Then we calculate the cost function of our model considering all of the examples",summari recurr neural network,"['summari', 'recurr', 'neural', 'network']",Summary of RNN
1016,"Types of RNN

1. One to one

2. One to many (used in music generation)

3. Many to one (sentiment classification, Language recognition from speech etc.)

4. Many to many (Tx=Ty) (Name entity recognition)

5. Many to many (Tx!=Ty) (Machine translation)",type recurr neural network,"['type', 'recurr', 'neural', 'network']",Types of RNN
1017,"Language Modelling

Language modelling means what our model hears. That means it calculates the probability of any expression or statement or sentence by calculating the conditional probability of each word in every layer.",languag model,"['languag', 'model']",Language Modelling
1018,"Sampling or creating sequence (sentence) from a trained language model (RNN)

If we train our RNN model on the whole lot of Shakespeare's poem, then our language model will be able to generate poem like Shakespeare.",sampl creat sequenc sentenc train languag model rnn,"['sampl', 'creat', 'sequenc', 'sentenc', 'train', 'languag', 'model', 'rnn']",Sampling or creating sequence (sentence) from a trained language model (RNN)
1019,"Character-level language model

Instead of creating word level vocabulary where there is unknown token, <UNK>, we create vocabulary of all the characters including letters, punctuations, numbers etc.",characterlevel languag model,"['characterlevel', 'languag', 'model']",Character-level language model
1020," Exploding and Vanishing gradient

> Exploding gradient problem (weights and activations are all taking on the value of NaN) can be managed by gradient clippping 

> Vanishing gradient problem of RNN is managed by Gated Recurrent Unit (GRU)
",explod vanish gradient,"['explod', 'vanish', 'gradient']", Exploding and Vanishing gradient
1021,"Gated Recurrent Unit

> In GRU, we are assigning memory gate activation so that the understanding of any word does not vanishes during long forward or backward propagation

It has two gates

i> Reset gate and 

ii> Update gate

GRUs are very similar to Long Short Term Memory(LSTM). Just like LSTM, GRU uses gates to control the flow of information. They are relatively new as compared to LSTM. This is the reason they offer some improvement over LSTM and have simpler architecture.",gate recurr unit,"['gate', 'recurr', 'unit']",Gated Recurrent Unit
1022,"Long short-term memory (LSTM) 

There are three different gates in an LSTM cell: 

i> forget gate, 

ii> input gate and

iii> output gate.

update gate = forget gate + input gate

Update and forget gate in LSTM palys simmilar role to Γ_u and (1−Γ_u) in GRU

Say, we are training a LSTM, have a 10000 word vocabulary and 100-dimensional activations a. Then the dimension of Γ_u at each time step will be 100

> LSTM can also be used for time series problems",long shortterm memori lstm,"['long', 'shortterm', 'memori', 'lstm']",Long short-term memory (LSTM) 
1023,"Bidirectional RNN (BRNN)

Getting information from the future words to predict a current word

BRNN with LSTM is very much popular for all of the NLP task",bidirect recurr neural network brecurr neural network,"['bidirect', 'recurr', 'neural', 'network', 'brecurr', 'neural', 'network']",Bidirectional RNN (BRNN)
1024,"Deep RNN

Here, for predicting y we use multiple activation layers instead of one.

> Deep RNN's are useful for NLP applications.",deep recurr neural network,"['deep', 'recurr', 'neural', 'network']",Deep RNN
1025,"Natural language generation (NLG) 

It comes after NLU and it has three steps:

1. Text planning

2. Sentence planning

3. Text realization",natur languag generat nlg,"['natur', 'languag', 'generat', 'nlg']",Natural language generation (NLG) 
1026,"Huggingface Transformer

!pip install transformers
from transformers import pipeline

ques_ans_pipeline = pipeline(""question-answering"")
context = ' '
question= ' ' 
ans = ques_ans_pipeline(question=question, context=context)
print(ans['answer'])

sentimentAnalysis_pipeline = pipeline(""sentiment-analysis"")

test_sentence='This is not a good story'

print(sentimentAnalysis_pipeline(test_sentence))",huggingfac transform,"['huggingfac', 'transform']",Huggingface Transformer
1027,"NLP and Word Embedding

> Word embedding and word encoding (conversion of human language to values) are similar but not fully same. 

> Embedding means encoding with contex",natur languag process word embed,"['natur', 'languag', 'process', 'word', 'embed']",NLP and Word Embedding
1028,"Featurized representation: word embedding

If we can create a vocabulary with a list of feature words (like gender, age, size, food, cost etc.), then we will get contextual vectors for every word and can train our model to learn these feature weights.

In real modelling, we do not specify any feature, we let our model learn all the features and their weights on its own.",featur represent word embed,"['featur', 'represent', 'word', 'embed']",Featurized representation: word embedding
1029,"Visualizing word embeddings

we can reduce n-dimensions (n number of features) to 2D by t-SNE to see how multiple words are making clusters

t-SNE is a non-linear dimensionality reduction technique",visual word embed,"['visual', 'word', 'embed']",Visualizing word embeddings
1030,"Embedding matrix 

If we take the vocabulary of words in the column and vocabulary of feature words in rows, then the matrix is called embedding matrix

Thus, embedding matrix contains the contextual weights of the words in the vocabulary

Embedding matrix is denoted by E, One hot encoded vector is denoted by O and word embedding vector is denoted by e.

Then, 
E.O=e

In the above vector dot product or matrix multiplication, matrix, E transforms one hot encoded vector, O according to the context and gives the resultant word embedding vector,e.

> word embedding vectors are the final inputs for the model

> Say, size of E is 10X200 and size of O is 200X1, then the size of e will be 10X1. That means 
dimension of word embedding= no. of contex or features",embed matrix,"['embed', 'matrix']",Embedding matrix 
1031,"Transfer learning and word embeddings

1. Learn word embeddings from large text corpus (1-100 B words) (or download pre-trained embedding online)

2. Transfer embedding is helpful when the new training set quite smaller (say, 100k words) than the pretrained model

3. Optional: Continue to finetune the word embeddings with new data",transfer learn word embed,"['transfer', 'learn', 'word', 'embed']",Transfer learning and word embeddings
1032,"Learning word embeddings

>> We first formulate our NLP problem as a structured data for supervised learning problem.

e.g. we keep a blank word in a sentence and want our model to fill in the blank. 

>> Then, we formulate the cost function for all of the sentences and by gradient descent find the contextual weights for every word.

>> Thus our model will learn the embedding matrix, E

>> When learning from a word embedding (similar like observation with different feature values), we create an artificial task of estimating P(target|context). It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.

>> Thus every word of a sentence behaves like single observation with multiple feature values and this is similar like all other ML algorithm

>> But the difference between RNN and all other ML algorithm is, RNN process multiple observations (words) in a single iteration

>> For normal ML or standard NN we know the feature names, for CNN we know the feature name as location, but for RNN we don't know the feature names, we define the rule to extract the feature names by the model itself",learn word embed,"['learn', 'word', 'embed']",Learning word embeddings
1033,"Context/target pairs

c and t are chosen to be nearby words

1. Last 4 words before the target

2. 4 words on left and right of the target

3. Last 1 word

4. Nearby 1 word

e.g.
for a target juice, most appropiate contexts are apple, orange and other fruits

There are three main models used for word embedding tasks like Embedding Layer, Word2Vec, GloVe

Never select any context, word or target in NLP randomly, then it will pickup the stopwords. We must use heuristic techniques",contexttarget pair,"['contexttarget', 'pair']",Context/target pairs
1034,"Word2Vec model

> Here, either Continuous Bag of Words (CBOW) Method or skip-grams method is used to make context-target pair. In skip-gram technique, we can skip any words of our choice to make the context

P(t|C)= exp(θ_t * e_c)/ sum_all_words_of vocab(exp(θ_t * e_c))

θ_t represents each possible target word and e_c for each possible context word

θ_t and e_c are both equal to the dimension of embedding vectors.

θ_t and e_c are both trained with an optimization algorithm such as Adam or gradient descent.",word2vec model,"['word2vec', 'model']",Word2Vec model
1035,"Problem with softmax classification

It has to generate classes equals to the length of the vocabulary. This is computationally expensive. Solution is hierarchical softmax or negative sampling",problem softmax classif,"['problem', 'softmax', 'classif']",Problem with softmax classification
1036,"Negative Sampling

To reduce the number of neuron weight updating, to reduce training time and having a better prediction result, negative sampling is introduced in word2vec .

Here we frame our training set, X with a combination of context and matching words and thus the target becomes a binary classification. This way we can avoid the problem of softmax layer. 

> For smaller dataset, negative sample (k) is generally 5-20 for one positive sample

> For large dataset, negative sample (k) is generally 2-5 for one positive sample",negat sampl,"['negat', 'sampl']",Negative Sampling
1037,"GloVe model

The Glove is a technique where the matrix factorization is performed on the word-context matrix.

> GloVe means Global Vector

> GloVe is much faster than Word2Vec",glove model,"['glove', 'model']",GloVe model
1038,"The problem of bias in word embeddings

Word embeddings can reflect gender, ethnicity, age, sexual orientation and other biases of the text used to train the model

Debiasing word embeddings

1. Identify bias direction

2. Neutralize: For every word that is not definitional, project to get rid of bias

3. Equalize pairs.",problem bias word embed,"['problem', 'bias', 'word', 'embed']",The problem of bias in word embeddings
1039,"Use of Pre-trained model for getting word embeddings

from gensim.models import Word2Vec 

import gensim.downloader as api

w2v_model = api.load(""word2vec-google-news-300"")

w2v_model.save('w2v_model.model')

glove_model = api.load(""glove-twitter-25"")

glove_model.save('glove.model')

document_term_matrix = glove_model[list_of_words] # to get the vectors

>> glove_model.wv.most_similar('mango', topn=1) # To find most similar words

Different pre-trained models

glove-twitter-25 (104 MB)

glove-twitter-50 (199 MB)

glove-twitter-100 (387 MB)

glove-twitter-200 (758 MB)

glove-wiki-gigaword-50 (65 MB)

glove-wiki-gigaword-100 (128 MB)

glove-wiki-gigaword-200 (252 MB)

glove-wiki-gigaword-300 (376 MB)

word2vec-google-news-300 (1662 MB)

word2vec-ruscorpora-300 (198 MB)",use pretrain model get word embed,"['use', 'pretrain', 'model', 'get', 'word', 'embed']",Use of Pre-trained model for getting word embeddings
1040,"Machine translation

Machine translation can be said 'conditional language model' because here instead of probability of a sentence, we find conditional probability of a sentence

Encoder decoder was initially developed for machine translation problems, although it has proven successful for summarization and question answering

>> Image captioning model is a CNN+RNN model",machin translat,"['machin', 'translat']",Machine translation
1041,"Finding the most likely translation

Greedy search will not give perfect translation from overall maximum probability perspective.

> Solution is Beam search

> In beam search, we search the best possible word with a beam width. Say with beam width,B=3, means it will find top three probable word or combination of words as a translation

For B=1, it behaves like a greedy search algorithm

But large beam width makes the computation slower, use up more memory but generally find better solutions (i.e. do a better job maximizing P(y∣x)).

In research paper beam width in the order of 1000 is common but in production system beam width in the order of 100 is generally used.",find like translat,"['find', 'like', 'translat']",Finding the most likely translation
1042,"Refinements to beam search

Beam search has an inherent bias to output shorter sentence to give higher arg conditional probability of sentence. To overcome this issue, we divide the arg max P(y|x) by the length of the sentence. This is called length normalization.",refin beam search,"['refin', 'beam', 'search']",Refinements to beam search
1043,"BFS and DFS

Unlike exact search algorithms like BFS (Breadth First Search) or DFS (Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for arg max P(y|x).

> BFS: It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level.

> DFS: The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.",breadth first search depth first search,"['breadth', 'first', 'search', 'depth', 'first', 'search']",BFS and DFS
1044,"Error analysis on beam search

If we find an error, will we allocate the error to beam search or RNN?

This is solved by finding the probability of human level performance, P(y*|x) and the probability of model performance, P(ŷ|x)

> For a certain example, P(y*|x)<= P(ŷ|x) indicates the error should be attributed to the RNN rather than to the search algorithm.",error analysi beam search,"['error', 'analysi', 'beam', 'search']",Error analysis on beam search
1045,"Attention model

For long sequence, normal machine translation model (encoder decoder model) does not perform well, because it takes the entire sentence at a time for translation. 

Thus the attention model comes into picture which translates like human translator, means it translate a long sentence part by part.",attent model,"['attent', 'model']",Attention model
1046,"Blue Score

It is used to measure the performance of machine translation model. With the increase in sentence length, blue score decreases for normal machine translation model.

> For attention model blue score does not change with sentence length",blue score,"['blue', 'score']",Blue Score
1047,"Speech recognition

This is a sequence data because audio data is an air pressure variation with time.

Two approaches are there:

1. Attention model

2. CTC (Connectionist Temporal Classification) cost based technique",speech recognit,"['speech', 'recognit']",Speech recognition
1048,"Basic Rule of CTC based technique

Collapse repeated characters not separated by blank

Say, blank is denoted by ""_"", 

__c_oo_o_kk___b_ooooo__oo__kkk string will collapse to cookbook",basic rule ctc base techniqu,"['basic', 'rule', 'ctc', 'base', 'techniqu']",Basic Rule of CTC based technique
1049,"Trigger word detection

e.g. Alexa, Okay Google, Hey Siri, Hey Cortana etc.

In trigger word detection, x is features of the audio (such as spectrogram features) at time t

The target label for x<t> is 1 means someone has just finished saying the trigger word at time t",trigger word detect,"['trigger', 'word', 'detect']",Trigger word detection
1050,"Knowledge of data science to understand human psychology and intelligence

Human psychologists have invented human-like intelligence or artificial intelligence. Here, I have tried to understand human psychology from human-like intelligence. My understandings are as follows:

Note-1: Human beings are like living computers. Living means Loving and Improving

Note-2: Physical form of love is reproduction and the physical form of improvement is growth. But the mental form of love is getting related with others and the mental form of improvement is learning

Note-3: Computer means a software model which takes input and gives output. Thus human beings are living-learning models which take input and give output

Note-4: The life of a human being is an experiment within the operating system or environment for learning the basic and more basic and finally the most basic rule of our environment. We are creating our own dataset from observations and experiences. 

Note-5: The dataset is the base for the human learning or machine learning model. Sample space is the structure for the measurement of experiences. Thus the experiences can be well managed.

More clear the sample space or universal set of features (i.e. Dharma) of the dataset or the clear perception of life, better will be the performance of the learning model.

Note-6: By birth, human beings have all the supervised, unsupervised, and reinforcement learning models in their brain to learn from the experience set, so that they can use any one of them to give output. Human beings are different mainly on four aspects, self-understanding, ability to love, field of interest, and model complexity

Note-7:  Continuous improvement is expected from this learning model for becoming an optimal model. During the addition of experiences or training data, dimensionality shall be reduced (or regularization or cross-validation or further increase in training data) for the complex model, and dimensions shall be increased for the simple model

Note-8: Spirituality is not a dimension like any other features of our experiences. It is the bias component for the learning model and devotional songs are the concepts of spirituality

""A picture is worth a thousand words, a video is worth a million words and a devotional song is worth a billion words.""

We need to learn the correct value of the bias component so that the weight for the other features is minimized and variance in output is reduced. (this is like regularization of the model)

Note-9: We know that at expectation probability is always maximum. Thus, we only need to know our exact expectations and all our intelligence will automatically work to maximize the probability.",knowledg data scienc understand human psycholog intellig,"['knowledg', 'data', 'scienc', 'understand', 'human', 'psycholog', 'intellig']",Knowledge of data science to understand human psychology and intelligence
1051,"Transformer Network

> A Transformer Network can ingest entire sentences (not word by word like RNN and LSTM) all at the same time.

> Transformer Network methodology is taken from Attention Mechanism and CNN style of processing.

> The key inputs for computing the attention value for each word are called the query (Q), key (K), and value (V). 

Q = interesting questions about the words in a sentence, 

K = qualities of words given a Q, 

V = specific representations of words given a Q

> Output of the encoder block contain contextual semantic embedding and positional encoding information

> Output of the decoder block contain Linear layer (fully connected layer) followed by a softmax layer",transform network,"['transform', 'network']",Transformer Network
1052,"Positional Encoding in Transformer

Position and order of words are the essential parts of any language. They define the grammar and thus the actual semantics of a sentence. Recurrent Neural Networks (RNNs) inherently take the order of word into account; They parse a sentence word by word in a sequential manner. 

But the Transformer architecture ditched the recurrence mechanism in favor of multi-head self-attention mechanism. Avoiding the RNNs’ method of recurrence will result in massive speed-up in the training time. 

As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.

The concept of Self-Attention is that given a word, its neighbouring words are used to compute its context by summing up the word values to map the Attention related to that given word.

One possible solution to give the model some sense of order is to add a piece of information to each word about its position in the sentence. We call this “piece of information”, the positional encoding.

> A good positional encoding algorithm should have consistent distance between any two time-steps (word's position in a sentence) for all sentence lengths.

> The algorithm should be able to generalize to longer sentences.

> It must be deterministic",posit encod transform,"['posit', 'encod', 'transform']",Positional Encoding in Transformer
1053,"Prabir Debnath as drona for tellme model

Prabir Debnath, the Data Reader Of New Age (drona), is my developer and he is the resource of my experiences that I am sharing with you.",prabir debnath drona tellm model,"['prabir', 'debnath', 'drona', 'tellm', 'model']",Prabir Debnath as drona for tellme model
1054,"Help Lines

I am the 'tellme' question answering model of drona (Data Reader Of New Age). I have many experiences to share with data science and machine learning aspirants. I am always with you to make your learning journey superfast and structured.

Freshers may ask questions in following sequence

""What is the role of a data scientist?""

""What are the main subjects of data science?""

""What are the main modules of ...?""

""What are the main topics of ...?""

""What are the subtopics of ...?""

""What is ...?""

and finally,

""Can the knowledge of data science help us to understand human psychology?""",help line,"['help', 'line']",Help Lines
